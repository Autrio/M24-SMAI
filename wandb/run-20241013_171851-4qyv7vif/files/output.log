(506,)
(506, 13)
Data split into training (406 samples), validation (50 samples), and testing (50 samples) sets.
Number of classes: 1
Feature data normalized using z-score normalization.
Layer: [in:13] [out:8] [activation:Tanh]
Layer: [in:8] [out:16] [activation:Tanh]
Layer: [in:16] [out:16] [activation:Tanh]
Layer: [in:16] [out:8] [activation:Tanh]
Layer: [in:8] [out:1] [activation:Linear]
                                                                                                                                                                                                           

Epoch: 0 	Train:[Loss: 617.6202, Acc: -6.0538] 	Val:[Loss: 595.1842, Acc: -5.7264]
Epoch: 100 	Train:[Loss: 298.0583, Acc: -2.4044] 	Val:[Loss: 281.1147, Acc: -2.1770]
Epoch: 200 	Train:[Loss: 162.2117, Acc: -0.8523] 	Val:[Loss: 152.5405, Acc: -0.7239]
Epoch: 300 	Train:[Loss: 105.9524, Acc: -0.2095] 	Val:[Loss: 100.5717, Acc: -0.1366]
Epoch: 400 	Train:[Loss: 80.4178, Acc: 0.0824] 	Val:[Loss: 76.5338, Acc: 0.1351]
Epoch: 500 	Train:[Loss: 67.5158, Acc: 0.2298] 	Val:[Loss: 63.0493, Acc: 0.2875]
Epoch: 600 	Train:[Loss: 59.3784, Acc: 0.3228] 	Val:[Loss: 54.3219, Acc: 0.3861]
Epoch: 700 	Train:[Loss: 53.9750, Acc: 0.3846] 	Val:[Loss: 48.9783, Acc: 0.4465]
Epoch: 800 	Train:[Loss: 50.2227, Acc: 0.4275] 	Val:[Loss: 45.8084, Acc: 0.4823]
Epoch: 900 	Train:[Loss: 46.7874, Acc: 0.4669] 	Val:[Loss: 42.6568, Acc: 0.5179]
Epoch: 1000 	Train:[Loss: 43.0008, Acc: 0.5104] 	Val:[Loss: 37.7672, Acc: 0.5732]
Epoch: 1100 	Train:[Loss: 39.7247, Acc: 0.5479] 	Val:[Loss: 34.4864, Acc: 0.6103]
Epoch: 1200 	Train:[Loss: 36.7304, Acc: 0.5821] 	Val:[Loss: 32.1194, Acc: 0.6370]
Epoch: 1300 	Train:[Loss: 33.5785, Acc: 0.6179] 	Val:[Loss: 29.7420, Acc: 0.6639]
Epoch: 1400 	Train:[Loss: 30.6038, Acc: 0.6518] 	Val:[Loss: 27.3128, Acc: 0.6913]
Epoch: 1500 	Train:[Loss: 28.1105, Acc: 0.6802] 	Val:[Loss: 24.9730, Acc: 0.7178]
Epoch: 1600 	Train:[Loss: 25.9409, Acc: 0.7049] 	Val:[Loss: 22.8570, Acc: 0.7417]
Epoch: 1700 	Train:[Loss: 24.0314, Acc: 0.7266] 	Val:[Loss: 21.0260, Acc: 0.7624]
Epoch: 1800 	Train:[Loss: 22.3442, Acc: 0.7458] 	Val:[Loss: 19.4594, Acc: 0.7801]
Epoch: 1900 	Train:[Loss: 20.8626, Acc: 0.7627] 	Val:[Loss: 18.1167, Acc: 0.7953]
Epoch: 2000 	Train:[Loss: 19.5660, Acc: 0.7775] 	Val:[Loss: 16.9741, Acc: 0.8082]
Epoch: 2100 	Train:[Loss: 18.4270, Acc: 0.7904] 	Val:[Loss: 16.0110, Acc: 0.8191]
Epoch: 2200 	Train:[Loss: 17.4214, Acc: 0.8019] 	Val:[Loss: 15.1992, Acc: 0.8282]
Epoch: 2300 	Train:[Loss: 16.5304, Acc: 0.8120] 	Val:[Loss: 14.5110, Acc: 0.8360]
Epoch: 2400 	Train:[Loss: 15.7387, Acc: 0.8209] 	Val:[Loss: 13.9219, Acc: 0.8427]
Epoch: 2500 	Train:[Loss: 15.0336, Acc: 0.8289] 	Val:[Loss: 13.4111, Acc: 0.8484]
Epoch: 2600 	Train:[Loss: 14.4035, Acc: 0.8361] 	Val:[Loss: 12.9618, Acc: 0.8535]
Epoch: 2700 	Train:[Loss: 13.8376, Acc: 0.8425] 	Val:[Loss: 12.5608, Acc: 0.8580]
Epoch: 2800 	Train:[Loss: 13.3261, Acc: 0.8482] 	Val:[Loss: 12.1984, Acc: 0.8621]
Epoch: 2900 	Train:[Loss: 12.8599, Acc: 0.8535] 	Val:[Loss: 11.8667, Acc: 0.8659]
Epoch: 3000 	Train:[Loss: 12.4316, Acc: 0.8584] 	Val:[Loss: 11.5569, Acc: 0.8694]
Epoch: 3100 	Train:[Loss: 12.0364, Acc: 0.8628] 	Val:[Loss: 11.2584, Acc: 0.8728]
Epoch: 3200 	Train:[Loss: 11.6718, Acc: 0.8669] 	Val:[Loss: 10.9627, Acc: 0.8761]
Epoch: 3300 	Train:[Loss: 11.3350, Acc: 0.8708] 	Val:[Loss: 10.6671, Acc: 0.8794]
Epoch: 3400 	Train:[Loss: 11.0214, Acc: 0.8743] 	Val:[Loss: 10.3730, Acc: 0.8828]
Epoch: 3500 	Train:[Loss: 10.7259, Acc: 0.8777] 	Val:[Loss: 10.0832, Acc: 0.8860]
Epoch: 3600 	Train:[Loss: 10.4438, Acc: 0.8809] 	Val:[Loss: 9.8035, Acc: 0.8892]
Epoch: 3700 	Train:[Loss: 10.1714, Acc: 0.8840] 	Val:[Loss: 9.5418, Acc: 0.8922]
Epoch: 3800 	Train:[Loss: 9.9077, Acc: 0.8870] 	Val:[Loss: 9.3043, Acc: 0.8948]
Epoch: 3900 	Train:[Loss: 9.6547, Acc: 0.8899] 	Val:[Loss: 9.0904, Acc: 0.8973]
Epoch: 3999 	Train:[Loss: 9.4170, Acc: 0.8926] 	Val:[Loss: 8.8974, Acc: 0.8994]
