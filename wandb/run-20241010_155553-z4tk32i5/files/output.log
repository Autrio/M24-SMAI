Data split into training (915 samples), validation (114 samples), and testing (114 samples) sets.
Number of classes: 6
Feature data normalized using z-score normalization.
Layer: [in:11] [out:16] [activation:Tanh]
Layer: [in:16] [out:64] [activation:Tanh]
Layer: [in:64] [out:128] [activation:Tanh]
Layer: [in:128] [out:256] [activation:Tanh]
Layer: [in:256] [out:6] [activation:Softmax]
                                                                                                                                                                                                            

Epoch: 0 	Train:[Loss: 7.6331, Acc: 0.3372] 	Val:[Loss: 8.1982, Acc: 0.3542]
Epoch: 128 	Train:[Loss: 4.5699, Acc: 0.4297] 	Val:[Loss: 3.4805, Acc: 0.3854]
Epoch: 256 	Train:[Loss: 4.5665, Acc: 0.3477] 	Val:[Loss: 5.3679, Acc: 0.4583]
Epoch: 384 	Train:[Loss: 3.4744, Acc: 0.4049] 	Val:[Loss: 2.8747, Acc: 0.4479]
Epoch: 512 	Train:[Loss: 3.2935, Acc: 0.4232] 	Val:[Loss: 5.8724, Acc: 0.4479]
Epoch: 640 	Train:[Loss: 3.6113, Acc: 0.4388] 	Val:[Loss: 2.5411, Acc: 0.3958]
Epoch: 768 	Train:[Loss: 2.8294, Acc: 0.4284] 	Val:[Loss: 4.7559, Acc: 0.4375]
Epoch: 896 	Train:[Loss: 3.1650, Acc: 0.4180] 	Val:[Loss: 2.0371, Acc: 0.4167]
Epoch: 1024 	Train:[Loss: 2.9173, Acc: 0.4688] 	Val:[Loss: 3.0842, Acc: 0.4271]
Epoch: 1152 	Train:[Loss: 2.4576, Acc: 0.4688] 	Val:[Loss: 2.9656, Acc: 0.3958]
Epoch: 1280 	Train:[Loss: 2.7270, Acc: 0.4375] 	Val:[Loss: 3.2228, Acc: 0.3958]
Epoch: 1408 	Train:[Loss: 2.6011, Acc: 0.5013] 	Val:[Loss: 2.0241, Acc: 0.6042]
Epoch: 1536 	Train:[Loss: 2.8375, Acc: 0.4701] 	Val:[Loss: 2.1433, Acc: 0.6250]
Epoch: 1664 	Train:[Loss: 2.4815, Acc: 0.5000] 	Val:[Loss: 2.0663, Acc: 0.6250]
Epoch: 1792 	Train:[Loss: 2.3823, Acc: 0.4961] 	Val:[Loss: 2.0191, Acc: 0.4479]
Epoch: 1920 	Train:[Loss: 2.3907, Acc: 0.5143] 	Val:[Loss: 2.0293, Acc: 0.5833]
Epoch: 2048 	Train:[Loss: 1.9417, Acc: 0.5638] 	Val:[Loss: 1.9835, Acc: 0.6042]
Epoch: 2176 	Train:[Loss: 2.4353, Acc: 0.5286] 	Val:[Loss: 2.1734, Acc: 0.4479]
Epoch: 2304 	Train:[Loss: 2.3841, Acc: 0.5234] 	Val:[Loss: 2.1032, Acc: 0.5938]
Epoch: 2432 	Train:[Loss: 1.8311, Acc: 0.6263] 	Val:[Loss: 2.0941, Acc: 0.4375]
Epoch: 2560 	Train:[Loss: 3.0810, Acc: 0.4753] 	Val:[Loss: 1.9121, Acc: 0.5521]
Epoch: 2688 	Train:[Loss: 1.7086, Acc: 0.5625] 	Val:[Loss: 3.3274, Acc: 0.5000]
Epoch: 2816 	Train:[Loss: 1.4837, Acc: 0.5742] 	Val:[Loss: 3.6158, Acc: 0.5104]
Epoch: 2944 	Train:[Loss: 1.5400, Acc: 0.5807] 	Val:[Loss: 3.7119, Acc: 0.4792]
Epoch: 3072 	Train:[Loss: 1.4102, Acc: 0.6016] 	Val:[Loss: 3.5361, Acc: 0.5312]
Epoch: 3200 	Train:[Loss: 1.4712, Acc: 0.5872] 	Val:[Loss: 3.6705, Acc: 0.5104]
Epoch: 3328 	Train:[Loss: 1.1755, Acc: 0.6458] 	Val:[Loss: 3.5635, Acc: 0.5208]
Epoch: 3456 	Train:[Loss: 1.4286, Acc: 0.6198] 	Val:[Loss: 3.3608, Acc: 0.5208]
Epoch: 3584 	Train:[Loss: 1.1497, Acc: 0.6367] 	Val:[Loss: 3.2652, Acc: 0.5521]
Epoch: 3712 	Train:[Loss: 2.1565, Acc: 0.5065] 	Val:[Loss: 3.5823, Acc: 0.4792]
Epoch: 3840 	Train:[Loss: 1.1140, Acc: 0.6315] 	Val:[Loss: 3.4185, Acc: 0.5104]
Epoch: 3968 	Train:[Loss: 2.1101, Acc: 0.5182] 	Val:[Loss: 4.3232, Acc: 0.4792]
Epoch: 4096 	Train:[Loss: 1.6011, Acc: 0.5703] 	Val:[Loss: 3.6911, Acc: 0.4896]
Epoch: 4224 	Train:[Loss: 1.1055, Acc: 0.6654] 	Val:[Loss: 3.4727, Acc: 0.5104]
Epoch: 4352 	Train:[Loss: 1.0130, Acc: 0.6562] 	Val:[Loss: 3.7648, Acc: 0.5000]
Epoch: 4480 	Train:[Loss: 1.5157, Acc: 0.6029] 	Val:[Loss: 3.6879, Acc: 0.5000]
Epoch: 4608 	Train:[Loss: 1.0606, Acc: 0.6628] 	Val:[Loss: 3.6958, Acc: 0.5000]
Epoch: 4736 	Train:[Loss: 1.3257, Acc: 0.6263] 	Val:[Loss: 3.5262, Acc: 0.5312]
Epoch: 4864 	Train:[Loss: 1.0989, Acc: 0.6615] 	Val:[Loss: 3.6017, Acc: 0.5208]
Epoch: 4992 	Train:[Loss: 0.9649, Acc: 0.6745] 	Val:[Loss: 3.5842, Acc: 0.5208]
