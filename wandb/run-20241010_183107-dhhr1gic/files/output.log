Data split into training (915 samples), validation (114 samples), and testing (114 samples) sets.
Number of classes: 6
Feature data normalized using z-score normalization.
Layer: [in:11] [out:32] [activation:Tanh]
Layer: [in:32] [out:128] [activation:Tanh]
Layer: [in:128] [out:128] [activation:Tanh]
Layer: [in:128] [out:32] [activation:Tanh]
Layer: [in:32] [out:6] [activation:Softmax]
                                                                                                                                                                                                            

Epoch: 0 	Train:[Loss: 3.2472, Acc: 0.3806] 	Val:[Loss: 1.5900, Acc: 0.4688]
Epoch: 32 	Train:[Loss: 1.1383, Acc: 0.5112] 	Val:[Loss: 1.0478, Acc: 0.5521]
Epoch: 64 	Train:[Loss: 1.0323, Acc: 0.5536] 	Val:[Loss: 1.0064, Acc: 0.5104]
Epoch: 96 	Train:[Loss: 1.0041, Acc: 0.5446] 	Val:[Loss: 0.9876, Acc: 0.5208]
Epoch: 128 	Train:[Loss: 0.9462, Acc: 0.6027] 	Val:[Loss: 0.9214, Acc: 0.6250]
Epoch: 160 	Train:[Loss: 0.9542, Acc: 0.5748] 	Val:[Loss: 0.9985, Acc: 0.6042]
Epoch: 192 	Train:[Loss: 0.9538, Acc: 0.5960] 	Val:[Loss: 0.9891, Acc: 0.5938]
Epoch: 224 	Train:[Loss: 0.9138, Acc: 0.5904] 	Val:[Loss: 1.0616, Acc: 0.6458]
Epoch: 256 	Train:[Loss: 0.8401, Acc: 0.6283] 	Val:[Loss: 1.0335, Acc: 0.6146]
Epoch: 288 	Train:[Loss: 0.8483, Acc: 0.6406] 	Val:[Loss: 1.0005, Acc: 0.5729]
Epoch: 320 	Train:[Loss: 0.8110, Acc: 0.6663] 	Val:[Loss: 1.0752, Acc: 0.6146]
Epoch: 352 	Train:[Loss: 0.8250, Acc: 0.6362] 	Val:[Loss: 1.0589, Acc: 0.5938]
Epoch: 384 	Train:[Loss: 0.8867, Acc: 0.6261] 	Val:[Loss: 1.0285, Acc: 0.5833]
Epoch: 416 	Train:[Loss: 0.7745, Acc: 0.6607] 	Val:[Loss: 1.1129, Acc: 0.6146]
Epoch: 448 	Train:[Loss: 0.7647, Acc: 0.6775] 	Val:[Loss: 1.1435, Acc: 0.5833]
Epoch: 480 	Train:[Loss: 0.9479, Acc: 0.6116] 	Val:[Loss: 1.1967, Acc: 0.6146]
Epoch: 512 	Train:[Loss: 0.8720, Acc: 0.6172] 	Val:[Loss: 1.1011, Acc: 0.6250]
Epoch: 544 	Train:[Loss: 0.7374, Acc: 0.6830] 	Val:[Loss: 1.2146, Acc: 0.5625]
Epoch: 576 	Train:[Loss: 0.7516, Acc: 0.6685] 	Val:[Loss: 1.2473, Acc: 0.5833]
Epoch: 608 	Train:[Loss: 0.7271, Acc: 0.6607] 	Val:[Loss: 1.1882, Acc: 0.6250]
Epoch: 640 	Train:[Loss: 0.7092, Acc: 0.6775] 	Val:[Loss: 1.2591, Acc: 0.6250]
Epoch: 672 	Train:[Loss: 0.7558, Acc: 0.6596] 	Val:[Loss: 1.0044, Acc: 0.5729]
Epoch: 704 	Train:[Loss: 0.6842, Acc: 0.6931] 	Val:[Loss: 1.2785, Acc: 0.6562]
Epoch: 736 	Train:[Loss: 0.7181, Acc: 0.6819] 	Val:[Loss: 1.2324, Acc: 0.5833]
Epoch: 768 	Train:[Loss: 0.7386, Acc: 0.6853] 	Val:[Loss: 1.2537, Acc: 0.5938]
Epoch: 800 	Train:[Loss: 0.7214, Acc: 0.6741] 	Val:[Loss: 1.2411, Acc: 0.6146]
Epoch: 832 	Train:[Loss: 0.6341, Acc: 0.7355] 	Val:[Loss: 1.3132, Acc: 0.6250]
Epoch: 864 	Train:[Loss: 0.6526, Acc: 0.7087] 	Val:[Loss: 1.2869, Acc: 0.5938]
Epoch: 896 	Train:[Loss: 0.6263, Acc: 0.7266] 	Val:[Loss: 1.2862, Acc: 0.5417]
Epoch: 928 	Train:[Loss: 0.6579, Acc: 0.7232] 	Val:[Loss: 1.3402, Acc: 0.5729]
Epoch: 960 	Train:[Loss: 0.7164, Acc: 0.6942] 	Val:[Loss: 1.2450, Acc: 0.6354]
Epoch: 992 	Train:[Loss: 0.6583, Acc: 0.7165] 	Val:[Loss: 1.4151, Acc: 0.5729]
Epoch: 1024 	Train:[Loss: 0.6136, Acc: 0.7299] 	Val:[Loss: 1.3769, Acc: 0.5729]
Epoch: 1056 	Train:[Loss: 0.5889, Acc: 0.7455] 	Val:[Loss: 1.3727, Acc: 0.6250]
Epoch: 1088 	Train:[Loss: 0.6902, Acc: 0.7254] 	Val:[Loss: 1.4738, Acc: 0.5833]
Epoch: 1120 	Train:[Loss: 0.7666, Acc: 0.6942] 	Val:[Loss: 1.2865, Acc: 0.6562]
Epoch: 1152 	Train:[Loss: 0.5585, Acc: 0.7467] 	Val:[Loss: 1.3494, Acc: 0.6042]
Epoch: 1184 	Train:[Loss: 0.5117, Acc: 0.7812] 	Val:[Loss: 1.4393, Acc: 0.6354]
