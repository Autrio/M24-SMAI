Data split into training (915 samples), validation (114 samples), and testing (114 samples) sets.
Number of classes: 6
Feature data normalized using z-score normalization.
Layer: [in:11] [out:16] [activation:Tanh]
Layer: [in:16] [out:32] [activation:Tanh]
Layer: [in:32] [out:32] [activation:Tanh]
Layer: [in:32] [out:16] [activation:Tanh]
Layer: [in:16] [out:6] [activation:Softmax]
                                                                                                                                                                                                            

Epoch: 0 	Train:[Loss: 5.4667, Acc: 0.2630] 	Val:[Loss: 3.8990, Acc: 0.2500]
Epoch: 128 	Train:[Loss: 1.1422, Acc: 0.5195] 	Val:[Loss: 1.1671, Acc: 0.5104]
Epoch: 256 	Train:[Loss: 1.0878, Acc: 0.5456] 	Val:[Loss: 1.0834, Acc: 0.5312]
Epoch: 384 	Train:[Loss: 1.0630, Acc: 0.5482] 	Val:[Loss: 1.0716, Acc: 0.5417]
Epoch: 512 	Train:[Loss: 1.0409, Acc: 0.5703] 	Val:[Loss: 1.0754, Acc: 0.5208]
Epoch: 640 	Train:[Loss: 1.0206, Acc: 0.5807] 	Val:[Loss: 1.0698, Acc: 0.5312]
Epoch: 768 	Train:[Loss: 1.0029, Acc: 0.5885] 	Val:[Loss: 1.0597, Acc: 0.5417]
Epoch: 896 	Train:[Loss: 0.9860, Acc: 0.5938] 	Val:[Loss: 1.0498, Acc: 0.5312]
Epoch: 1024 	Train:[Loss: 0.9701, Acc: 0.6068] 	Val:[Loss: 1.0374, Acc: 0.5417]
Epoch: 1152 	Train:[Loss: 0.9555, Acc: 0.6120] 	Val:[Loss: 1.0304, Acc: 0.5208]
Epoch: 1280 	Train:[Loss: 0.9414, Acc: 0.6159] 	Val:[Loss: 1.0269, Acc: 0.5417]
Epoch: 1408 	Train:[Loss: 0.9375, Acc: 0.6172] 	Val:[Loss: 1.0126, Acc: 0.5833]
Epoch: 1536 	Train:[Loss: 0.9265, Acc: 0.6172] 	Val:[Loss: 1.0107, Acc: 0.5729]
Epoch: 1664 	Train:[Loss: 0.9180, Acc: 0.6276] 	Val:[Loss: 1.0104, Acc: 0.5938]
Epoch: 1792 	Train:[Loss: 0.9097, Acc: 0.6341] 	Val:[Loss: 1.0109, Acc: 0.6042]
Epoch: 1920 	Train:[Loss: 0.9011, Acc: 0.6367] 	Val:[Loss: 1.0118, Acc: 0.6354]
Epoch: 2048 	Train:[Loss: 0.8923, Acc: 0.6393] 	Val:[Loss: 1.0121, Acc: 0.6354]
Epoch: 2176 	Train:[Loss: 0.8839, Acc: 0.6419] 	Val:[Loss: 1.0105, Acc: 0.6458]
Epoch: 2304 	Train:[Loss: 0.8753, Acc: 0.6419] 	Val:[Loss: 1.0074, Acc: 0.6562]
Epoch: 2432 	Train:[Loss: 0.8669, Acc: 0.6458] 	Val:[Loss: 1.0041, Acc: 0.6562]
Epoch: 2560 	Train:[Loss: 0.8591, Acc: 0.6497] 	Val:[Loss: 1.0006, Acc: 0.6562]
Epoch: 2688 	Train:[Loss: 0.8518, Acc: 0.6510] 	Val:[Loss: 0.9969, Acc: 0.6458]
Epoch: 2816 	Train:[Loss: 0.8447, Acc: 0.6523] 	Val:[Loss: 0.9935, Acc: 0.6458]
Epoch: 2944 	Train:[Loss: 0.8377, Acc: 0.6536] 	Val:[Loss: 0.9909, Acc: 0.6458]
Epoch: 3072 	Train:[Loss: 0.8309, Acc: 0.6549] 	Val:[Loss: 0.9894, Acc: 0.6562]
Epoch: 3200 	Train:[Loss: 0.8243, Acc: 0.6549] 	Val:[Loss: 0.9882, Acc: 0.6667]
Epoch: 3328 	Train:[Loss: 0.8180, Acc: 0.6576] 	Val:[Loss: 0.9874, Acc: 0.6562]
Epoch: 3456 	Train:[Loss: 0.8117, Acc: 0.6654] 	Val:[Loss: 0.9875, Acc: 0.6458]
Epoch: 3584 	Train:[Loss: 0.8051, Acc: 0.6680] 	Val:[Loss: 0.9880, Acc: 0.6458]
Epoch: 3712 	Train:[Loss: 0.7984, Acc: 0.6706] 	Val:[Loss: 0.9884, Acc: 0.6458]
Epoch: 3840 	Train:[Loss: 0.7920, Acc: 0.6693] 	Val:[Loss: 0.9891, Acc: 0.6458]
Epoch: 3968 	Train:[Loss: 0.7857, Acc: 0.6706] 	Val:[Loss: 0.9903, Acc: 0.6458]
