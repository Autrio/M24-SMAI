(506,)
(506, 13)
Data split into training (406 samples), validation (50 samples), and testing (50 samples) sets.
Number of classes: 1
Feature data normalized using z-score normalization.
Layer: [in:13] [out:8] [activation:Tanh]
Layer: [in:8] [out:16] [activation:Tanh]
Layer: [in:16] [out:16] [activation:Tanh]
Layer: [in:16] [out:8] [activation:Tanh]
Layer: [in:8] [out:1] [activation:Linear]
                                                                                                                                                                                                           

Epoch: 0 	Train:[Loss: 611.5561, Acc: -5.9866] 	Val:[Loss: 586.4229, Acc: -5.6274]
Epoch: 100 	Train:[Loss: 403.9897, Acc: -3.6147] 	Val:[Loss: 382.3677, Acc: -3.3213]
Epoch: 200 	Train:[Loss: 220.9288, Acc: -1.5232] 	Val:[Loss: 206.7562, Acc: -1.3366]
Epoch: 300 	Train:[Loss: 135.6196, Acc: -0.5482] 	Val:[Loss: 127.1688, Acc: -0.4372]
Epoch: 400 	Train:[Loss: 95.6608, Acc: -0.0917] 	Val:[Loss: 86.8454, Acc: 0.0185]
Epoch: 500 	Train:[Loss: 74.3550, Acc: 0.1517] 	Val:[Loss: 66.8735, Acc: 0.2442]
Epoch: 600 	Train:[Loss: 63.8036, Acc: 0.2721] 	Val:[Loss: 57.1992, Acc: 0.3536]
Epoch: 700 	Train:[Loss: 57.7198, Acc: 0.3415] 	Val:[Loss: 51.6396, Acc: 0.4164]
Epoch: 800 	Train:[Loss: 53.0367, Acc: 0.3951] 	Val:[Loss: 46.8946, Acc: 0.4700]
Epoch: 900 	Train:[Loss: 47.9753, Acc: 0.4531] 	Val:[Loss: 42.3234, Acc: 0.5217]
Epoch: 1000 	Train:[Loss: 42.9332, Acc: 0.5107] 	Val:[Loss: 38.0894, Acc: 0.5695]
Epoch: 1100 	Train:[Loss: 38.9816, Acc: 0.5558] 	Val:[Loss: 34.2685, Acc: 0.6127]
Epoch: 1200 	Train:[Loss: 35.6381, Acc: 0.5940] 	Val:[Loss: 31.0303, Acc: 0.6493]
Epoch: 1300 	Train:[Loss: 32.7231, Acc: 0.6272] 	Val:[Loss: 28.3219, Acc: 0.6799]
Epoch: 1400 	Train:[Loss: 30.1491, Acc: 0.6566] 	Val:[Loss: 26.0641, Acc: 0.7054]
Epoch: 1500 	Train:[Loss: 27.8630, Acc: 0.6827] 	Val:[Loss: 24.1575, Acc: 0.7270]
Epoch: 1600 	Train:[Loss: 25.8451, Acc: 0.7057] 	Val:[Loss: 22.5244, Acc: 0.7454]
Epoch: 1700 	Train:[Loss: 24.0843, Acc: 0.7257] 	Val:[Loss: 21.1211, Acc: 0.7613]
Epoch: 1800 	Train:[Loss: 22.5606, Acc: 0.7431] 	Val:[Loss: 19.9211, Acc: 0.7749]
Epoch: 1900 	Train:[Loss: 21.2443, Acc: 0.7581] 	Val:[Loss: 18.9009, Acc: 0.7864]
Epoch: 2000 	Train:[Loss: 20.1015, Acc: 0.7711] 	Val:[Loss: 18.0357, Acc: 0.7962]
Epoch: 2100 	Train:[Loss: 19.0997, Acc: 0.7825] 	Val:[Loss: 17.3006, Acc: 0.8045]
Epoch: 2200 	Train:[Loss: 18.2111, Acc: 0.7926] 	Val:[Loss: 16.6720, Acc: 0.8116]
Epoch: 2300 	Train:[Loss: 17.4125, Acc: 0.8017] 	Val:[Loss: 16.1275, Acc: 0.8177]
Epoch: 2400 	Train:[Loss: 16.6831, Acc: 0.8100] 	Val:[Loss: 15.6471, Acc: 0.8232]
Epoch: 2500 	Train:[Loss: 15.9970, Acc: 0.8178] 	Val:[Loss: 15.2151, Acc: 0.8280]
Epoch: 2600 	Train:[Loss: 15.3165, Acc: 0.8255] 	Val:[Loss: 14.8177, Acc: 0.8325]
Epoch: 2700 	Train:[Loss: 14.6566, Acc: 0.8330] 	Val:[Loss: 14.4326, Acc: 0.8369]
Epoch: 2800 	Train:[Loss: 14.0663, Acc: 0.8397] 	Val:[Loss: 14.0604, Acc: 0.8411]
Epoch: 2900 	Train:[Loss: 13.5299, Acc: 0.8457] 	Val:[Loss: 13.7079, Acc: 0.8451]
Epoch: 3000 	Train:[Loss: 13.0326, Acc: 0.8514] 	Val:[Loss: 13.3651, Acc: 0.8490]
Epoch: 3100 	Train:[Loss: 12.5672, Acc: 0.8567] 	Val:[Loss: 13.0245, Acc: 0.8528]
Epoch: 3200 	Train:[Loss: 12.1295, Acc: 0.8617] 	Val:[Loss: 12.6824, Acc: 0.8567]
Epoch: 3300 	Train:[Loss: 11.7168, Acc: 0.8664] 	Val:[Loss: 12.3374, Acc: 0.8606]
Epoch: 3400 	Train:[Loss: 11.3269, Acc: 0.8708] 	Val:[Loss: 11.9899, Acc: 0.8645]
Epoch: 3500 	Train:[Loss: 10.9583, Acc: 0.8750] 	Val:[Loss: 11.6416, Acc: 0.8684]
Epoch: 3600 	Train:[Loss: 10.6095, Acc: 0.8790] 	Val:[Loss: 11.2948, Acc: 0.8724]
Epoch: 3700 	Train:[Loss: 10.2793, Acc: 0.8828] 	Val:[Loss: 10.9519, Acc: 0.8762]
Epoch: 3800 	Train:[Loss: 9.9665, Acc: 0.8864] 	Val:[Loss: 10.6151, Acc: 0.8800]
Epoch: 3900 	Train:[Loss: 9.6696, Acc: 0.8898] 	Val:[Loss: 10.2864, Acc: 0.8838]
Epoch: 3999 	Train:[Loss: 9.3903, Acc: 0.8929] 	Val:[Loss: 9.9710, Acc: 0.8873]
================Test set metrics======================

R2 Score ::  0.8400485317610378
MSE ::  10.170521145839013
RMSE ::  3.1891254515680334
MAE ::  2.397826731639425

======================================================
