(506,)
(506, 13)
Data split into training (406 samples), validation (50 samples), and testing (50 samples) sets.
Number of classes: 1
Feature data normalized using z-score normalization.
Layer: [in:13] [out:8] [activation:Tanh]
Layer: [in:8] [out:16] [activation:Tanh]
Layer: [in:16] [out:16] [activation:Tanh]
Layer: [in:16] [out:8] [activation:Tanh]
Layer: [in:8] [out:1] [activation:Linear]
                                                                                                                                                                                                           

Epoch: 0 	Train:[Loss: 623.8763, Acc: -6.1268] 	Val:[Loss: 585.8277, Acc: -5.6206]
Epoch: 100 	Train:[Loss: 379.1247, Acc: -3.3305] 	Val:[Loss: 352.3577, Acc: -2.9821]
Epoch: 200 	Train:[Loss: 188.1166, Acc: -1.1483] 	Val:[Loss: 175.7033, Acc: -0.9857]
Epoch: 300 	Train:[Loss: 118.3429, Acc: -0.3509] 	Val:[Loss: 111.8631, Acc: -0.2642]
Epoch: 400 	Train:[Loss: 87.5373, Acc: 0.0011] 	Val:[Loss: 81.5842, Acc: 0.0780]
Epoch: 500 	Train:[Loss: 71.0719, Acc: 0.1892] 	Val:[Loss: 64.8522, Acc: 0.2671]
Epoch: 600 	Train:[Loss: 62.1814, Acc: 0.2907] 	Val:[Loss: 55.8105, Acc: 0.3693]
Epoch: 700 	Train:[Loss: 56.0083, Acc: 0.3612] 	Val:[Loss: 49.2017, Acc: 0.4440]
Epoch: 800 	Train:[Loss: 51.7420, Acc: 0.4100] 	Val:[Loss: 45.2832, Acc: 0.4882]
Epoch: 900 	Train:[Loss: 47.6334, Acc: 0.4571] 	Val:[Loss: 42.1671, Acc: 0.5235]
Epoch: 1000 	Train:[Loss: 43.6222, Acc: 0.5030] 	Val:[Loss: 39.5048, Acc: 0.5535]
Epoch: 1100 	Train:[Loss: 40.0240, Acc: 0.5441] 	Val:[Loss: 35.9464, Acc: 0.5938]
Epoch: 1200 	Train:[Loss: 36.7793, Acc: 0.5811] 	Val:[Loss: 32.9991, Acc: 0.6271]
Epoch: 1300 	Train:[Loss: 33.9341, Acc: 0.6136] 	Val:[Loss: 30.4069, Acc: 0.6564]
Epoch: 1400 	Train:[Loss: 31.3438, Acc: 0.6432] 	Val:[Loss: 28.0756, Acc: 0.6827]
Epoch: 1500 	Train:[Loss: 28.9784, Acc: 0.6702] 	Val:[Loss: 25.9983, Acc: 0.7062]
Epoch: 1600 	Train:[Loss: 26.8236, Acc: 0.6947] 	Val:[Loss: 24.0979, Acc: 0.7277]
Epoch: 1700 	Train:[Loss: 24.8941, Acc: 0.7167] 	Val:[Loss: 22.3396, Acc: 0.7475]
Epoch: 1800 	Train:[Loss: 23.2314, Acc: 0.7356] 	Val:[Loss: 20.7912, Acc: 0.7650]
Epoch: 1900 	Train:[Loss: 21.8208, Acc: 0.7517] 	Val:[Loss: 19.5062, Acc: 0.7796]
Epoch: 2000 	Train:[Loss: 20.6206, Acc: 0.7653] 	Val:[Loss: 18.4708, Acc: 0.7913]
Epoch: 2100 	Train:[Loss: 19.5861, Acc: 0.7771] 	Val:[Loss: 17.6411, Acc: 0.8006]
Epoch: 2200 	Train:[Loss: 18.6838, Acc: 0.7874] 	Val:[Loss: 16.9678, Acc: 0.8082]
Epoch: 2300 	Train:[Loss: 17.8913, Acc: 0.7964] 	Val:[Loss: 16.4069, Acc: 0.8146]
Epoch: 2400 	Train:[Loss: 17.1903, Acc: 0.8044] 	Val:[Loss: 15.9257, Acc: 0.8200]
Epoch: 2500 	Train:[Loss: 16.5651, Acc: 0.8115] 	Val:[Loss: 15.5038, Acc: 0.8248]
Epoch: 2600 	Train:[Loss: 16.0033, Acc: 0.8179] 	Val:[Loss: 15.1275, Acc: 0.8290]
Epoch: 2700 	Train:[Loss: 15.4951, Acc: 0.8236] 	Val:[Loss: 14.7872, Acc: 0.8329]
Epoch: 2800 	Train:[Loss: 15.0321, Acc: 0.8289] 	Val:[Loss: 14.4760, Acc: 0.8364]
Epoch: 2900 	Train:[Loss: 14.6068, Acc: 0.8337] 	Val:[Loss: 14.1893, Acc: 0.8396]
Epoch: 3000 	Train:[Loss: 14.2125, Acc: 0.8382] 	Val:[Loss: 13.9222, Acc: 0.8427]
Epoch: 3100 	Train:[Loss: 13.8432, Acc: 0.8424] 	Val:[Loss: 13.6661, Acc: 0.8456]
Epoch: 3200 	Train:[Loss: 13.4942, Acc: 0.8463] 	Val:[Loss: 13.4054, Acc: 0.8485]
Epoch: 3300 	Train:[Loss: 13.1620, Acc: 0.8501] 	Val:[Loss: 13.1214, Acc: 0.8517]
Epoch: 3400 	Train:[Loss: 12.8435, Acc: 0.8537] 	Val:[Loss: 12.7994, Acc: 0.8553]
Epoch: 3500 	Train:[Loss: 12.5360, Acc: 0.8572] 	Val:[Loss: 12.4320, Acc: 0.8595]
Epoch: 3600 	Train:[Loss: 12.2374, Acc: 0.8606] 	Val:[Loss: 12.0165, Acc: 0.8642]
Epoch: 3700 	Train:[Loss: 11.9451, Acc: 0.8640] 	Val:[Loss: 11.5530, Acc: 0.8694]
Epoch: 3800 	Train:[Loss: 11.6558, Acc: 0.8673] 	Val:[Loss: 11.0586, Acc: 0.8750]
Epoch: 3900 	Train:[Loss: 11.3683, Acc: 0.8706] 	Val:[Loss: 10.5922, Acc: 0.8803]
Epoch: 3999 	Train:[Loss: 11.0894, Acc: 0.8738] 	Val:[Loss: 10.2228, Acc: 0.8845]
