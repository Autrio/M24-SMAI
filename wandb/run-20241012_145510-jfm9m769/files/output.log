Index(['age', 'gender', 'income', 'education', 'married', 'children', 'city',
       'occupation', 'purchase_amount', 'most bought item', 'labels'],
      dtype='object')
(1000, 8)
(1000, 10)
Data split into training (800 samples), validation (100 samples), and testing (100 samples) sets.
Number of classes: 8
Feature data normalized using z-score normalization.
                                                                                                                                                                                                            
Epoch: 0 	Train:[Loss: 0.7277, Acc: 0.5148] 	Val:[Loss: 0.7341, Acc: 0.4974]
Epoch: 64 	Train:[Loss: 0.6275, Acc: 0.6616] 	Val:[Loss: 0.6396, Acc: 0.6745]
Epoch: 128 	Train:[Loss: 0.6176, Acc: 0.6688] 	Val:[Loss: 0.6419, Acc: 0.6576]
Epoch: 192 	Train:[Loss: 0.6069, Acc: 0.6756] 	Val:[Loss: 0.6463, Acc: 0.6393]
Epoch: 256 	Train:[Loss: 0.5954, Acc: 0.6882] 	Val:[Loss: 0.6547, Acc: 0.6315]
Epoch: 320 	Train:[Loss: 0.5854, Acc: 0.6943] 	Val:[Loss: 0.6642, Acc: 0.6328]
Epoch: 384 	Train:[Loss: 0.5764, Acc: 0.7005] 	Val:[Loss: 0.6761, Acc: 0.6289]
Epoch: 448 	Train:[Loss: 0.5682, Acc: 0.7077] 	Val:[Loss: 0.6903, Acc: 0.6172]
Epoch: 512 	Train:[Loss: 0.5613, Acc: 0.7132] 	Val:[Loss: 0.7038, Acc: 0.6159]
Epoch: 576 	Train:[Loss: 0.5550, Acc: 0.7150] 	Val:[Loss: 0.7165, Acc: 0.6107]
Epoch: 640 	Train:[Loss: 0.5489, Acc: 0.7215] 	Val:[Loss: 0.7300, Acc: 0.6042]
Epoch: 704 	Train:[Loss: 0.5431, Acc: 0.7256] 	Val:[Loss: 0.7427, Acc: 0.5924]
Epoch: 768 	Train:[Loss: 0.5376, Acc: 0.7288] 	Val:[Loss: 0.7541, Acc: 0.5911]
Epoch: 832 	Train:[Loss: 0.5321, Acc: 0.7345] 	Val:[Loss: 0.7661, Acc: 0.5872]
Epoch: 896 	Train:[Loss: 0.5269, Acc: 0.7360] 	Val:[Loss: 0.7780, Acc: 0.5807]
Epoch: 960 	Train:[Loss: 0.5221, Acc: 0.7380] 	Val:[Loss: 0.7893, Acc: 0.5820]
Epoch: 1024 	Train:[Loss: 0.5178, Acc: 0.7437] 	Val:[Loss: 0.7997, Acc: 0.5846]
Epoch: 1088 	Train:[Loss: 0.5141, Acc: 0.7469] 	Val:[Loss: 0.8092, Acc: 0.5820]
Epoch: 1152 	Train:[Loss: 0.5109, Acc: 0.7469] 	Val:[Loss: 0.8175, Acc: 0.5872]
Epoch: 1216 	Train:[Loss: 0.5081, Acc: 0.7500] 	Val:[Loss: 0.8255, Acc: 0.5846]
Epoch: 1280 	Train:[Loss: 0.5055, Acc: 0.7536] 	Val:[Loss: 0.8339, Acc: 0.5833]
Epoch: 1344 	Train:[Loss: 0.5033, Acc: 0.7531] 	Val:[Loss: 0.8429, Acc: 0.5781]
Epoch: 1408 	Train:[Loss: 0.5013, Acc: 0.7552] 	Val:[Loss: 0.8520, Acc: 0.5755]
Epoch: 1472 	Train:[Loss: 0.5004, Acc: 0.7547] 	Val:[Loss: 0.8643, Acc: 0.5794]
Epoch: 1536 	Train:[Loss: 0.4987, Acc: 0.7541] 	Val:[Loss: 0.8731, Acc: 0.5794]
Epoch: 1600 	Train:[Loss: 0.4973, Acc: 0.7570] 	Val:[Loss: 0.8806, Acc: 0.5807]
Epoch: 1664 	Train:[Loss: 0.4959, Acc: 0.7554] 	Val:[Loss: 0.8864, Acc: 0.5768]
Epoch: 1728 	Train:[Loss: 0.4944, Acc: 0.7562] 	Val:[Loss: 0.8912, Acc: 0.5807]
Epoch: 1792 	Train:[Loss: 0.4927, Acc: 0.7567] 	Val:[Loss: 0.8962, Acc: 0.5729]
Epoch: 1856 	Train:[Loss: 0.4907, Acc: 0.7580] 	Val:[Loss: 0.9022, Acc: 0.5664]
Epoch: 1920 	Train:[Loss: 0.4881, Acc: 0.7601] 	Val:[Loss: 0.9100, Acc: 0.5729]
Epoch: 1984 	Train:[Loss: 0.4860, Acc: 0.7645] 	Val:[Loss: 0.9176, Acc: 0.5612]
Epoch: 2048 	Train:[Loss: 0.4842, Acc: 0.7642] 	Val:[Loss: 0.9253, Acc: 0.5573]
Epoch: 2112 	Train:[Loss: 0.4825, Acc: 0.7629] 	Val:[Loss: 0.9320, Acc: 0.5625]
Epoch: 2176 	Train:[Loss: 0.4809, Acc: 0.7650] 	Val:[Loss: 0.9373, Acc: 0.5573]
Epoch: 2240 	Train:[Loss: 0.4794, Acc: 0.7684] 	Val:[Loss: 0.9416, Acc: 0.5521]
Epoch: 2304 	Train:[Loss: 0.4805, Acc: 0.7686] 	Val:[Loss: 0.9340, Acc: 0.5482]
Epoch: 2368 	Train:[Loss: 0.4821, Acc: 0.7681] 	Val:[Loss: 0.9509, Acc: 0.5547]
Epoch: 2432 	Train:[Loss: 0.4780, Acc: 0.7712] 	Val:[Loss: 0.9426, Acc: 0.5508]
Epoch: 2496 	Train:[Loss: 0.4804, Acc: 0.7661] 	Val:[Loss: 0.9453, Acc: 0.5625]
Epoch: 2560 	Train:[Loss: 0.4773, Acc: 0.7674] 	Val:[Loss: 0.9541, Acc: 0.5482]
Epoch: 2624 	Train:[Loss: 0.4833, Acc: 0.7638] 	Val:[Loss: 0.9481, Acc: 0.5534]
Epoch: 2688 	Train:[Loss: 0.4774, Acc: 0.7695] 	Val:[Loss: 0.9598, Acc: 0.5482]
Epoch: 2752 	Train:[Loss: 0.4810, Acc: 0.7666] 	Val:[Loss: 0.9657, Acc: 0.5599]
Epoch: 2816 	Train:[Loss: 0.4739, Acc: 0.7682] 	Val:[Loss: 0.9632, Acc: 0.5547]
Epoch: 2880 	Train:[Loss: 0.4759, Acc: 0.7668] 	Val:[Loss: 0.9553, Acc: 0.5560]
Epoch: 2944 	Train:[Loss: 0.4794, Acc: 0.7625] 	Val:[Loss: 0.9660, Acc: 0.5508]
Epoch: 3008 	Train:[Loss: 0.4772, Acc: 0.7697] 	Val:[Loss: 0.9656, Acc: 0.5508]
Epoch: 3072 	Train:[Loss: 0.4719, Acc: 0.7705] 	Val:[Loss: 0.9622, Acc: 0.5560]
Epoch: 3136 	Train:[Loss: 0.4772, Acc: 0.7686] 	Val:[Loss: 0.9814, Acc: 0.5560]
Epoch: 3200 	Train:[Loss: 0.4914, Acc: 0.7598] 	Val:[Loss: 0.9662, Acc: 0.5625]
Epoch: 3264 	Train:[Loss: 0.4775, Acc: 0.7666] 	Val:[Loss: 0.9710, Acc: 0.5495]
Epoch: 3328 	Train:[Loss: 0.4762, Acc: 0.7655] 	Val:[Loss: 0.9764, Acc: 0.5599]
Epoch: 3392 	Train:[Loss: 0.4734, Acc: 0.7690] 	Val:[Loss: 0.9814, Acc: 0.5612]
Epoch: 3456 	Train:[Loss: 0.4723, Acc: 0.7710] 	Val:[Loss: 0.9761, Acc: 0.5586]
Epoch: 3520 	Train:[Loss: 0.4660, Acc: 0.7757] 	Val:[Loss: 0.9866, Acc: 0.5586]
Epoch: 3584 	Train:[Loss: 0.4677, Acc: 0.7733] 	Val:[Loss: 0.9864, Acc: 0.5534]
Epoch: 3648 	Train:[Loss: 0.4679, Acc: 0.7764] 	Val:[Loss: 0.9865, Acc: 0.5573]
Epoch: 3712 	Train:[Loss: 0.4662, Acc: 0.7790] 	Val:[Loss: 0.9827, Acc: 0.5599]
Epoch: 3776 	Train:[Loss: 0.4777, Acc: 0.7663] 	Val:[Loss: 0.9837, Acc: 0.5573]
Epoch: 3840 	Train:[Loss: 0.4683, Acc: 0.7754] 	Val:[Loss: 0.9901, Acc: 0.5560]
Epoch: 3904 	Train:[Loss: 0.4870, Acc: 0.7601] 	Val:[Loss: 0.9916, Acc: 0.5482]
Epoch: 3968 	Train:[Loss: 0.4995, Acc: 0.7562] 	Val:[Loss: 0.9776, Acc: 0.5586]
Epoch: 4032 	Train:[Loss: 0.4703, Acc: 0.7759] 	Val:[Loss: 0.9886, Acc: 0.5547]
Epoch: 4096 	Train:[Loss: 0.4740, Acc: 0.7720] 	Val:[Loss: 0.9832, Acc: 0.5651]
Epoch: 4160 	Train:[Loss: 0.4671, Acc: 0.7747] 	Val:[Loss: 0.9912, Acc: 0.5521]
Epoch: 4224 	Train:[Loss: 0.4651, Acc: 0.7775] 	Val:[Loss: 0.9806, Acc: 0.5599]
Epoch: 4288 	Train:[Loss: 0.4678, Acc: 0.7756] 	Val:[Loss: 1.0004, Acc: 0.5495]
Epoch: 4352 	Train:[Loss: 0.4723, Acc: 0.7726] 	Val:[Loss: 0.9852, Acc: 0.5495]
Epoch: 4416 	Train:[Loss: 0.4651, Acc: 0.7760] 	Val:[Loss: 0.9865, Acc: 0.5534]
Epoch: 4480 	Train:[Loss: 0.4751, Acc: 0.7710] 	Val:[Loss: 0.9753, Acc: 0.5638]
Epoch: 4544 	Train:[Loss: 0.4671, Acc: 0.7743] 	Val:[Loss: 0.9665, Acc: 0.5560]
Epoch: 4608 	Train:[Loss: 0.4755, Acc: 0.7703] 	Val:[Loss: 0.9908, Acc: 0.5612]
Epoch: 4672 	Train:[Loss: 0.4711, Acc: 0.7728] 	Val:[Loss: 0.9836, Acc: 0.5625]
Epoch: 4736 	Train:[Loss: 0.4828, Acc: 0.7658] 	Val:[Loss: 0.9874, Acc: 0.5625]
Epoch: 4800 	Train:[Loss: 0.4672, Acc: 0.7725] 	Val:[Loss: 0.9760, Acc: 0.5599]
Epoch: 4864 	Train:[Loss: 0.4815, Acc: 0.7640] 	Val:[Loss: 1.0006, Acc: 0.5651]
Epoch: 4928 	Train:[Loss: 0.4893, Acc: 0.7640] 	Val:[Loss: 0.9851, Acc: 0.5729]
Epoch: 4992 	Train:[Loss: 0.4688, Acc: 0.7729] 	Val:[Loss: 0.9824, Acc: 0.5690]
['beauty', 'books', 'clothing', 'electronics', 'food', 'furniture', 'home', 'sports']
[0. 1. 0. 0. 0. 0. 0. 0.]
[[0.38704251 0.57289151 0.0757844  0.44224171 0.00996558 0.36842654
  0.40464057 0.15656745]]
[[0. 1. 0. 0. 0. 0. 0. 0.]]
