(506,)
(506, 13)
Data split into training (406 samples), validation (50 samples), and testing (50 samples) sets.
Number of classes: 1
Feature data normalized using z-score normalization.
Layer: [in:13] [out:32] [activation:ReLU]
Layer: [in:32] [out:128] [activation:ReLU]
Layer: [in:128] [out:128] [activation:ReLU]
Layer: [in:128] [out:32] [activation:ReLU]
Layer: [in:32] [out:1] [activation:Linear]
                                                                                                                                                                                                            

Epoch: 0 	Train:[Loss: 570.0761, Acc: -5.3232] 	Val:[Loss: 372.5674, Acc: -3.2105]
Epoch: 128 	Train:[Loss: 18.2017, Acc: 0.7981] 	Val:[Loss: 15.8206, Acc: 0.8212]
Epoch: 256 	Train:[Loss: 10.1101, Acc: 0.8879] 	Val:[Loss: 13.8490, Acc: 0.8435]
Epoch: 384 	Train:[Loss: 7.0583, Acc: 0.9217] 	Val:[Loss: 12.6745, Acc: 0.8568]
Epoch: 512 	Train:[Loss: 5.6155, Acc: 0.9377] 	Val:[Loss: 11.9646, Acc: 0.8648]
Epoch: 640 	Train:[Loss: 4.6927, Acc: 0.9479] 	Val:[Loss: 11.4562, Acc: 0.8705]
Epoch: 768 	Train:[Loss: 3.9650, Acc: 0.9560] 	Val:[Loss: 11.0879, Acc: 0.8747]
Epoch: 896 	Train:[Loss: 3.6070, Acc: 0.9600] 	Val:[Loss: 11.0764, Acc: 0.8748]
Epoch: 1024 	Train:[Loss: 3.2123, Acc: 0.9644] 	Val:[Loss: 10.9400, Acc: 0.8764]
Epoch: 1152 	Train:[Loss: 2.9170, Acc: 0.9676] 	Val:[Loss: 10.8972, Acc: 0.8768]
Epoch: 1280 	Train:[Loss: 2.7484, Acc: 0.9695] 	Val:[Loss: 11.0806, Acc: 0.8748]
Epoch: 1408 	Train:[Loss: 2.4831, Acc: 0.9725] 	Val:[Loss: 11.1572, Acc: 0.8739]
Epoch: 1536 	Train:[Loss: 2.1165, Acc: 0.9765] 	Val:[Loss: 11.0366, Acc: 0.8753]
Epoch: 1664 	Train:[Loss: 2.2202, Acc: 0.9754] 	Val:[Loss: 11.6634, Acc: 0.8682]
Epoch: 1792 	Train:[Loss: 1.8665, Acc: 0.9793] 	Val:[Loss: 11.4029, Acc: 0.8711]
Epoch: 1920 	Train:[Loss: 2.1196, Acc: 0.9765] 	Val:[Loss: 12.2097, Acc: 0.8620]
Epoch: 2048 	Train:[Loss: 1.7470, Acc: 0.9806] 	Val:[Loss: 11.8519, Acc: 0.8661]
Epoch: 2176 	Train:[Loss: 1.5701, Acc: 0.9826] 	Val:[Loss: 11.8979, Acc: 0.8655]
Epoch: 2304 	Train:[Loss: 1.8257, Acc: 0.9797] 	Val:[Loss: 12.6224, Acc: 0.8574]
Epoch: 2432 	Train:[Loss: 1.6863, Acc: 0.9813] 	Val:[Loss: 12.5992, Acc: 0.8576]
Epoch: 2560 	Train:[Loss: 1.2968, Acc: 0.9856] 	Val:[Loss: 12.1489, Acc: 0.8627]
Epoch: 2688 	Train:[Loss: 1.1035, Acc: 0.9878] 	Val:[Loss: 12.0193, Acc: 0.8642]
Epoch: 2816 	Train:[Loss: 0.9788, Acc: 0.9891] 	Val:[Loss: 11.9656, Acc: 0.8648]
Epoch: 2944 	Train:[Loss: 0.8575, Acc: 0.9905] 	Val:[Loss: 11.8569, Acc: 0.8660]
Epoch: 3072 	Train:[Loss: 0.8085, Acc: 0.9910] 	Val:[Loss: 11.8930, Acc: 0.8656]
Epoch: 3200 	Train:[Loss: 1.0099, Acc: 0.9888] 	Val:[Loss: 12.5098, Acc: 0.8586]
Epoch: 3328 	Train:[Loss: 1.6879, Acc: 0.9813] 	Val:[Loss: 13.9858, Acc: 0.8419]
Epoch: 3456 	Train:[Loss: 0.8526, Acc: 0.9905] 	Val:[Loss: 12.6257, Acc: 0.8573]
Epoch: 3584 	Train:[Loss: 0.5445, Acc: 0.9940] 	Val:[Loss: 11.8805, Acc: 0.8657]
Epoch: 3712 	Train:[Loss: 1.0912, Acc: 0.9879] 	Val:[Loss: 13.2264, Acc: 0.8505]
Epoch: 3840 	Train:[Loss: 0.9643, Acc: 0.9893] 	Val:[Loss: 13.2396, Acc: 0.8504]
Epoch: 3968 	Train:[Loss: 0.4469, Acc: 0.9950] 	Val:[Loss: 11.9591, Acc: 0.8648]
Epoch: 4096 	Train:[Loss: 1.0659, Acc: 0.9882] 	Val:[Loss: 13.4992, Acc: 0.8474]
Epoch: 4224 	Train:[Loss: 0.6327, Acc: 0.9930] 	Val:[Loss: 12.7665, Acc: 0.8557]
Epoch: 4352 	Train:[Loss: 0.3795, Acc: 0.9958] 	Val:[Loss: 11.9964, Acc: 0.8644]
Epoch: 4480 	Train:[Loss: 1.5217, Acc: 0.9831] 	Val:[Loss: 14.6759, Acc: 0.8341]
Epoch: 4608 	Train:[Loss: 0.3166, Acc: 0.9965] 	Val:[Loss: 11.9508, Acc: 0.8649]
Epoch: 4736 	Train:[Loss: 1.1921, Acc: 0.9868] 	Val:[Loss: 14.1152, Acc: 0.8405]
Epoch: 4864 	Train:[Loss: 0.3933, Acc: 0.9956] 	Val:[Loss: 12.4489, Acc: 0.8593]
Epoch: 4992 	Train:[Loss: 0.3592, Acc: 0.9960] 	Val:[Loss: 12.3207, Acc: 0.8608]
