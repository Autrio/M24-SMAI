Data split into training (915 samples), validation (114 samples), and testing (114 samples) sets.
Number of classes: 6
Feature data normalized using z-score normalization.
Layer: [in:11] [out:8] [activation:Sigmoid]
Layer: [in:8] [out:16] [activation:Sigmoid]
Layer: [in:16] [out:16] [activation:Sigmoid]
Layer: [in:16] [out:8] [activation:Sigmoid]
Layer: [in:8] [out:6] [activation:Softmax]
                                                                                                                                                                                                            

Epoch: 0 	Train:[Loss: 2.2518, Acc: 0.0169] 	Val:[Loss: 2.0831, Acc: 0.0104]
Epoch: 128 	Train:[Loss: 1.1970, Acc: 0.4362] 	Val:[Loss: 1.1490, Acc: 0.4375]
Epoch: 256 	Train:[Loss: 1.1930, Acc: 0.4336] 	Val:[Loss: 1.1398, Acc: 0.4375]
Epoch: 384 	Train:[Loss: 1.1881, Acc: 0.4505] 	Val:[Loss: 1.1303, Acc: 0.4479]
Epoch: 512 	Train:[Loss: 1.1810, Acc: 0.4831] 	Val:[Loss: 1.1172, Acc: 0.5312]
Epoch: 640 	Train:[Loss: 1.1723, Acc: 0.4974] 	Val:[Loss: 1.1019, Acc: 0.5625]
Epoch: 768 	Train:[Loss: 1.1636, Acc: 0.5039] 	Val:[Loss: 1.0876, Acc: 0.6042]
Epoch: 896 	Train:[Loss: 1.1559, Acc: 0.5208] 	Val:[Loss: 1.0759, Acc: 0.5833]
Epoch: 1024 	Train:[Loss: 1.1495, Acc: 0.5026] 	Val:[Loss: 1.0669, Acc: 0.5833]
Epoch: 1152 	Train:[Loss: 1.1440, Acc: 0.5000] 	Val:[Loss: 1.0596, Acc: 0.5938]
Epoch: 1280 	Train:[Loss: 1.1390, Acc: 0.5065] 	Val:[Loss: 1.0532, Acc: 0.6042]
Epoch: 1408 	Train:[Loss: 1.1340, Acc: 0.5117] 	Val:[Loss: 1.0471, Acc: 0.6042]
Epoch: 1536 	Train:[Loss: 1.1289, Acc: 0.5169] 	Val:[Loss: 1.0410, Acc: 0.6042]
Epoch: 1664 	Train:[Loss: 1.1235, Acc: 0.5221] 	Val:[Loss: 1.0345, Acc: 0.5938]
Epoch: 1792 	Train:[Loss: 1.1176, Acc: 0.5208] 	Val:[Loss: 1.0276, Acc: 0.5729]
Epoch: 1920 	Train:[Loss: 1.1110, Acc: 0.5273] 	Val:[Loss: 1.0202, Acc: 0.5625]
Epoch: 2048 	Train:[Loss: 1.1035, Acc: 0.5326] 	Val:[Loss: 1.0122, Acc: 0.5625]
Epoch: 2176 	Train:[Loss: 1.0951, Acc: 0.5352] 	Val:[Loss: 1.0040, Acc: 0.5938]
Epoch: 2304 	Train:[Loss: 1.0857, Acc: 0.5417] 	Val:[Loss: 0.9952, Acc: 0.6042]
Epoch: 2432 	Train:[Loss: 1.0741, Acc: 0.5508] 	Val:[Loss: 0.9789, Acc: 0.6250]
Epoch: 2560 	Train:[Loss: 1.0623, Acc: 0.5560] 	Val:[Loss: 0.9649, Acc: 0.6250]
Epoch: 2688 	Train:[Loss: 1.0527, Acc: 0.5599] 	Val:[Loss: 0.9540, Acc: 0.6354]
Epoch: 2816 	Train:[Loss: 1.0445, Acc: 0.5612] 	Val:[Loss: 0.9438, Acc: 0.6354]
Epoch: 2944 	Train:[Loss: 1.0376, Acc: 0.5677] 	Val:[Loss: 0.9350, Acc: 0.6354]
Epoch: 3072 	Train:[Loss: 1.0320, Acc: 0.5729] 	Val:[Loss: 0.9277, Acc: 0.6458]
Epoch: 3200 	Train:[Loss: 1.0274, Acc: 0.5716] 	Val:[Loss: 0.9218, Acc: 0.6562]
Epoch: 3328 	Train:[Loss: 1.0235, Acc: 0.5781] 	Val:[Loss: 0.9170, Acc: 0.6667]
Epoch: 3456 	Train:[Loss: 1.0202, Acc: 0.5742] 	Val:[Loss: 0.9132, Acc: 0.6875]
Epoch: 3584 	Train:[Loss: 1.0174, Acc: 0.5742] 	Val:[Loss: 0.9102, Acc: 0.6771]
Epoch: 3712 	Train:[Loss: 1.0149, Acc: 0.5729] 	Val:[Loss: 0.9078, Acc: 0.6875]
Epoch: 3840 	Train:[Loss: 1.0128, Acc: 0.5729] 	Val:[Loss: 0.9058, Acc: 0.6875]
Epoch: 3968 	Train:[Loss: 1.0108, Acc: 0.5742] 	Val:[Loss: 0.9041, Acc: 0.6875]
Epoch: 4096 	Train:[Loss: 1.0091, Acc: 0.5768] 	Val:[Loss: 0.9027, Acc: 0.6667]
Epoch: 4224 	Train:[Loss: 1.0075, Acc: 0.5794] 	Val:[Loss: 0.9014, Acc: 0.6667]
Epoch: 4352 	Train:[Loss: 1.0061, Acc: 0.5781] 	Val:[Loss: 0.9003, Acc: 0.6562]
Epoch: 4480 	Train:[Loss: 1.0047, Acc: 0.5768] 	Val:[Loss: 0.8993, Acc: 0.6562]
Epoch: 4608 	Train:[Loss: 1.0035, Acc: 0.5768] 	Val:[Loss: 0.8984, Acc: 0.6562]
Epoch: 4736 	Train:[Loss: 1.0022, Acc: 0.5768] 	Val:[Loss: 0.8976, Acc: 0.6562]
Epoch: 4864 	Train:[Loss: 1.0011, Acc: 0.5755] 	Val:[Loss: 0.8969, Acc: 0.6562]
Epoch: 4992 	Train:[Loss: 1.0000, Acc: 0.5716] 	Val:[Loss: 0.8961, Acc: 0.6667]
