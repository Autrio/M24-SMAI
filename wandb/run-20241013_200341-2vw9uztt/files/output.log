(506,)
(506, 13)
Data split into training (406 samples), validation (50 samples), and testing (50 samples) sets.
Number of classes: 1
Feature data normalized using z-score normalization.
{'lr': 0.05563583082068074, 'batch_size': 64, 'epoch': 5000, 'optimizer': 'mini-batch', 'loss_fn': 'MSELoss', 'activation': 'Tanh', 'type': 'regression', 'early_stopping': True, 'activations': 'Tanh', 'model_architecture': 'arch5'}
Layer: [in:13] [out:16] [activation:Tanh]
Layer: [in:16] [out:64] [activation:Tanh]
Layer: [in:64] [out:128] [activation:Tanh]
Layer: [in:128] [out:256] [activation:Tanh]
Layer: [in:256] [out:1] [activation:Linear]
Training:  28%|████████████████████████████████▎                                                                                    | 1379/5000 [02:03<05:23, 11.18epoch/s, Train Acc=0.979, Val Acc=0.907]

Epoch: 0 	Train:[Loss: 618.2447, Acc: -5.8575] 	Val:[Loss: 560.9643, Acc: -5.3396]
Epoch: 100 	Train:[Loss: 18.0580, Acc: 0.7997] 	Val:[Loss: 14.9428, Acc: 0.8311]
Epoch: 200 	Train:[Loss: 8.9913, Acc: 0.9003] 	Val:[Loss: 11.3835, Acc: 0.8714]
Epoch: 300 	Train:[Loss: 5.8576, Acc: 0.9350] 	Val:[Loss: 10.0134, Acc: 0.8868]
Epoch: 400 	Train:[Loss: 4.5539, Acc: 0.9495] 	Val:[Loss: 9.1034, Acc: 0.8971]
Epoch: 500 	Train:[Loss: 3.8381, Acc: 0.9574] 	Val:[Loss: 8.4956, Acc: 0.9040]
Epoch: 600 	Train:[Loss: 3.3661, Acc: 0.9627] 	Val:[Loss: 8.1083, Acc: 0.9084]
Epoch: 700 	Train:[Loss: 3.0219, Acc: 0.9665] 	Val:[Loss: 7.8934, Acc: 0.9108]
Epoch: 800 	Train:[Loss: 2.7554, Acc: 0.9694] 	Val:[Loss: 7.8030, Acc: 0.9118]
Epoch: 900 	Train:[Loss: 2.5400, Acc: 0.9718] 	Val:[Loss: 7.7922, Acc: 0.9119]
Epoch: 1000 	Train:[Loss: 2.3600, Acc: 0.9738] 	Val:[Loss: 7.8301, Acc: 0.9115]
Epoch: 1100 	Train:[Loss: 2.2055, Acc: 0.9755] 	Val:[Loss: 7.8998, Acc: 0.9107]
Epoch: 1200 	Train:[Loss: 2.0702, Acc: 0.9770] 	Val:[Loss: 7.9930, Acc: 0.9097]
Epoch: 1300 	Train:[Loss: 1.9513, Acc: 0.9784] 	Val:[Loss: 8.0919, Acc: 0.9086]
Early stopping triggered at epoch 1379.
Model weights restored to epoch 867.
best validation loss::7.789160529427033
