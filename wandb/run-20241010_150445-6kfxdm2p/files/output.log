Data split into training (915 samples), validation (114 samples), and testing (114 samples) sets.
Number of classes: 6
Feature data normalized using z-score normalization.
Layer: [in:11] [out:16] [activation:Tanh]
Layer: [in:16] [out:32] [activation:Tanh]
Layer: [in:32] [out:32] [activation:Tanh]
Layer: [in:32] [out:16] [activation:Tanh]
Layer: [in:16] [out:6] [activation:Softmax]
                                                                                                                                                                                                            

Epoch: 0 	Train:[Loss: 3.4198, Acc: 0.1641] 	Val:[Loss: 1.7036, Acc: 0.3646]
Epoch: 128 	Train:[Loss: 1.0468, Acc: 0.5690] 	Val:[Loss: 0.9826, Acc: 0.5833]
Epoch: 256 	Train:[Loss: 1.0102, Acc: 0.5664] 	Val:[Loss: 0.9495, Acc: 0.6250]
Epoch: 384 	Train:[Loss: 0.9666, Acc: 0.5872] 	Val:[Loss: 0.9435, Acc: 0.6146]
Epoch: 512 	Train:[Loss: 0.9383, Acc: 0.5951] 	Val:[Loss: 0.8643, Acc: 0.6146]
Epoch: 640 	Train:[Loss: 0.9492, Acc: 0.5807] 	Val:[Loss: 0.9540, Acc: 0.5729]
Epoch: 768 	Train:[Loss: 0.9185, Acc: 0.5951] 	Val:[Loss: 0.8924, Acc: 0.6042]
Epoch: 896 	Train:[Loss: 0.8859, Acc: 0.6081] 	Val:[Loss: 0.9010, Acc: 0.6250]
Epoch: 1024 	Train:[Loss: 0.8714, Acc: 0.5951] 	Val:[Loss: 0.9365, Acc: 0.6042]
Epoch: 1152 	Train:[Loss: 0.8724, Acc: 0.6042] 	Val:[Loss: 0.8895, Acc: 0.6042]
Epoch: 1280 	Train:[Loss: 0.7962, Acc: 0.6484] 	Val:[Loss: 0.9324, Acc: 0.5729]
Epoch: 1408 	Train:[Loss: 0.7802, Acc: 0.6536] 	Val:[Loss: 0.9291, Acc: 0.5729]
Epoch: 1536 	Train:[Loss: 0.7566, Acc: 0.6641] 	Val:[Loss: 0.9469, Acc: 0.6146]
Epoch: 1664 	Train:[Loss: 0.7484, Acc: 0.6654] 	Val:[Loss: 0.9591, Acc: 0.5833]
Epoch: 1792 	Train:[Loss: 0.7447, Acc: 0.6445] 	Val:[Loss: 1.0120, Acc: 0.5938]
Epoch: 1920 	Train:[Loss: 0.7175, Acc: 0.6719] 	Val:[Loss: 1.0113, Acc: 0.5938]
Epoch: 2048 	Train:[Loss: 0.7033, Acc: 0.6953] 	Val:[Loss: 0.9886, Acc: 0.5625]
Epoch: 2176 	Train:[Loss: 0.7465, Acc: 0.6628] 	Val:[Loss: 1.1354, Acc: 0.5417]
Epoch: 2304 	Train:[Loss: 0.6736, Acc: 0.6849] 	Val:[Loss: 1.1118, Acc: 0.6250]
Epoch: 2432 	Train:[Loss: 0.7288, Acc: 0.6745] 	Val:[Loss: 1.0584, Acc: 0.6146]
Epoch: 2560 	Train:[Loss: 0.6389, Acc: 0.7240] 	Val:[Loss: 1.1918, Acc: 0.5938]
Epoch: 2688 	Train:[Loss: 0.6048, Acc: 0.7279] 	Val:[Loss: 1.0241, Acc: 0.6042]
Epoch: 2816 	Train:[Loss: 0.7600, Acc: 0.6654] 	Val:[Loss: 1.0730, Acc: 0.5521]
Epoch: 2944 	Train:[Loss: 0.6786, Acc: 0.6888] 	Val:[Loss: 1.2346, Acc: 0.5729]
Epoch: 3072 	Train:[Loss: 0.7410, Acc: 0.6732] 	Val:[Loss: 1.3419, Acc: 0.5938]
Epoch: 3200 	Train:[Loss: 0.5378, Acc: 0.7526] 	Val:[Loss: 1.2917, Acc: 0.5833]
Epoch: 3328 	Train:[Loss: 0.5370, Acc: 0.7721] 	Val:[Loss: 1.2688, Acc: 0.5417]
Epoch: 3456 	Train:[Loss: 0.5924, Acc: 0.7617] 	Val:[Loss: 1.2440, Acc: 0.5312]
Epoch: 3584 	Train:[Loss: 0.5993, Acc: 0.7357] 	Val:[Loss: 1.2741, Acc: 0.5312]
Epoch: 3712 	Train:[Loss: 0.5151, Acc: 0.7786] 	Val:[Loss: 1.4089, Acc: 0.5729]
Epoch: 3840 	Train:[Loss: 0.5293, Acc: 0.7630] 	Val:[Loss: 1.4614, Acc: 0.5938]
Epoch: 3968 	Train:[Loss: 0.6581, Acc: 0.6979] 	Val:[Loss: 1.2961, Acc: 0.5729]
