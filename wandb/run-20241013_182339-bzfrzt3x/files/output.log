(1143, 6)
(1143, 11)
Data split into training (915 samples), validation (114 samples), and testing (114 samples) sets.
Number of classes: 6
Feature data normalized using z-score normalization.
Layer: [in:11] [out:8] [activation:Sigmoid]
Layer: [in:8] [out:16] [activation:Sigmoid]
Layer: [in:16] [out:16] [activation:Sigmoid]
Layer: [in:16] [out:8] [activation:Sigmoid]
Layer: [in:8] [out:6] [activation:Softmax]
                                                                                                                                                                                                           

Epoch: 0 	Train:[Loss: 0.5863, Acc: 0.0078] 	Val:[Loss: 0.5554, Acc: 0.0000]
Epoch: 100 	Train:[Loss: 0.3326, Acc: 0.4336] 	Val:[Loss: 0.3240, Acc: 0.4375]
Epoch: 200 	Train:[Loss: 0.3325, Acc: 0.4336] 	Val:[Loss: 0.3236, Acc: 0.4375]
Epoch: 300 	Train:[Loss: 0.3325, Acc: 0.4336] 	Val:[Loss: 0.3236, Acc: 0.4375]
Epoch: 400 	Train:[Loss: 0.3324, Acc: 0.4336] 	Val:[Loss: 0.3236, Acc: 0.4375]
Epoch: 500 	Train:[Loss: 0.3324, Acc: 0.4336] 	Val:[Loss: 0.3235, Acc: 0.4375]
Epoch: 600 	Train:[Loss: 0.3324, Acc: 0.4336] 	Val:[Loss: 0.3235, Acc: 0.4375]
Epoch: 700 	Train:[Loss: 0.3323, Acc: 0.4336] 	Val:[Loss: 0.3235, Acc: 0.4375]
Epoch: 800 	Train:[Loss: 0.3323, Acc: 0.4336] 	Val:[Loss: 0.3235, Acc: 0.4375]
Epoch: 900 	Train:[Loss: 0.3322, Acc: 0.4336] 	Val:[Loss: 0.3235, Acc: 0.4375]
Epoch: 1000 	Train:[Loss: 0.3322, Acc: 0.4336] 	Val:[Loss: 0.3234, Acc: 0.4375]
Epoch: 1100 	Train:[Loss: 0.3321, Acc: 0.4336] 	Val:[Loss: 0.3234, Acc: 0.4375]
Epoch: 1200 	Train:[Loss: 0.3319, Acc: 0.4336] 	Val:[Loss: 0.3233, Acc: 0.4375]
Epoch: 1300 	Train:[Loss: 0.3318, Acc: 0.4336] 	Val:[Loss: 0.3233, Acc: 0.4375]
Epoch: 1400 	Train:[Loss: 0.3315, Acc: 0.4336] 	Val:[Loss: 0.3231, Acc: 0.4375]
Epoch: 1500 	Train:[Loss: 0.3311, Acc: 0.4336] 	Val:[Loss: 0.3229, Acc: 0.4375]
Epoch: 1600 	Train:[Loss: 0.3304, Acc: 0.4388] 	Val:[Loss: 0.3225, Acc: 0.4375]
Epoch: 1700 	Train:[Loss: 0.3292, Acc: 0.4609] 	Val:[Loss: 0.3217, Acc: 0.4583]
Epoch: 1800 	Train:[Loss: 0.3268, Acc: 0.4857] 	Val:[Loss: 0.3199, Acc: 0.5000]
Epoch: 1900 	Train:[Loss: 0.3222, Acc: 0.5117] 	Val:[Loss: 0.3162, Acc: 0.5521]
Epoch: 2000 	Train:[Loss: 0.3147, Acc: 0.5260] 	Val:[Loss: 0.3095, Acc: 0.5521]
Epoch: 2100 	Train:[Loss: 0.3069, Acc: 0.5312] 	Val:[Loss: 0.3011, Acc: 0.5625]
Epoch: 2200 	Train:[Loss: 0.3019, Acc: 0.5378] 	Val:[Loss: 0.2941, Acc: 0.5625]
Epoch: 2300 	Train:[Loss: 0.2989, Acc: 0.5391] 	Val:[Loss: 0.2889, Acc: 0.5729]
Epoch: 2400 	Train:[Loss: 0.2972, Acc: 0.5404] 	Val:[Loss: 0.2857, Acc: 0.5729]
Epoch: 2500 	Train:[Loss: 0.2964, Acc: 0.5417] 	Val:[Loss: 0.2843, Acc: 0.5833]
Epoch: 2600 	Train:[Loss: 0.2958, Acc: 0.5495] 	Val:[Loss: 0.2825, Acc: 0.6042]
Epoch: 2700 	Train:[Loss: 0.2944, Acc: 0.5430] 	Val:[Loss: 0.2782, Acc: 0.6146]
Epoch: 2800 	Train:[Loss: 0.2926, Acc: 0.5534] 	Val:[Loss: 0.2747, Acc: 0.6458]
Epoch: 2900 	Train:[Loss: 0.2911, Acc: 0.5586] 	Val:[Loss: 0.2721, Acc: 0.6458]
Epoch: 2999 	Train:[Loss: 0.2905, Acc: 0.5547] 	Val:[Loss: 0.2710, Acc: 0.6250]
================Test set metrics======================

accuracy ::  0.8581871345029239
precision ::  0.2467505241090147
recall ::  0.1835627868168544
F1-score ::  0.21051737282546243

======================================================
