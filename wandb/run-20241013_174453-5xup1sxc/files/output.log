(506,)
(506, 13)
Data split into training (406 samples), validation (50 samples), and testing (50 samples) sets.
Number of classes: 1
Feature data normalized using z-score normalization.
Layer: [in:13] [out:8] [activation:Tanh]
Layer: [in:8] [out:16] [activation:Tanh]
Layer: [in:16] [out:16] [activation:Tanh]
Layer: [in:16] [out:8] [activation:Tanh]
Layer: [in:8] [out:1] [activation:Linear]
                                                                                                                                                                                                           

Epoch: 0 	Train:[Loss: 603.3072, Acc: -5.8924] 	Val:[Loss: 582.2706, Acc: -5.5804]
Epoch: 100 	Train:[Loss: 301.7824, Acc: -2.4470] 	Val:[Loss: 284.6751, Acc: -2.2172]
Epoch: 200 	Train:[Loss: 165.1064, Acc: -0.8854] 	Val:[Loss: 154.2975, Acc: -0.7438]
Epoch: 300 	Train:[Loss: 108.6671, Acc: -0.2403] 	Val:[Loss: 98.7370, Acc: -0.1159]
Epoch: 400 	Train:[Loss: 80.1987, Acc: 0.0846] 	Val:[Loss: 70.1456, Acc: 0.2073]
Epoch: 500 	Train:[Loss: 67.2268, Acc: 0.2327] 	Val:[Loss: 58.5932, Acc: 0.3378]
Epoch: 600 	Train:[Loss: 60.6504, Acc: 0.3078] 	Val:[Loss: 53.2809, Acc: 0.3979]
Epoch: 700 	Train:[Loss: 56.5120, Acc: 0.3551] 	Val:[Loss: 49.6023, Acc: 0.4394]
Epoch: 800 	Train:[Loss: 52.4021, Acc: 0.4022] 	Val:[Loss: 45.0822, Acc: 0.4905]
Epoch: 900 	Train:[Loss: 48.1323, Acc: 0.4511] 	Val:[Loss: 42.9943, Acc: 0.5141]
Epoch: 1000 	Train:[Loss: 44.2509, Acc: 0.4957] 	Val:[Loss: 40.4625, Acc: 0.5427]
Epoch: 1100 	Train:[Loss: 40.8948, Acc: 0.5341] 	Val:[Loss: 37.0705, Acc: 0.5811]
Epoch: 1200 	Train:[Loss: 37.8965, Acc: 0.5683] 	Val:[Loss: 33.5122, Acc: 0.6213]
Epoch: 1300 	Train:[Loss: 35.0391, Acc: 0.6009] 	Val:[Loss: 30.1533, Acc: 0.6592]
Epoch: 1400 	Train:[Loss: 32.3696, Acc: 0.6314] 	Val:[Loss: 27.2845, Acc: 0.6916]
Epoch: 1500 	Train:[Loss: 29.9708, Acc: 0.6588] 	Val:[Loss: 24.9009, Acc: 0.7186]
Epoch: 1600 	Train:[Loss: 27.8441, Acc: 0.6830] 	Val:[Loss: 22.8926, Acc: 0.7413]
Epoch: 1700 	Train:[Loss: 25.9664, Acc: 0.7044] 	Val:[Loss: 21.1703, Acc: 0.7607]
Epoch: 1800 	Train:[Loss: 24.3169, Acc: 0.7232] 	Val:[Loss: 19.6780, Acc: 0.7776]
Epoch: 1900 	Train:[Loss: 22.8715, Acc: 0.7397] 	Val:[Loss: 18.3808, Acc: 0.7923]
Epoch: 2000 	Train:[Loss: 21.6015, Acc: 0.7541] 	Val:[Loss: 17.2510, Acc: 0.8050]
Epoch: 2100 	Train:[Loss: 20.4774, Acc: 0.7669] 	Val:[Loss: 16.2632, Acc: 0.8162]
Epoch: 2200 	Train:[Loss: 19.4730, Acc: 0.7784] 	Val:[Loss: 15.3932, Acc: 0.8260]
Epoch: 2300 	Train:[Loss: 18.5665, Acc: 0.7887] 	Val:[Loss: 14.6188, Acc: 0.8348]
Epoch: 2400 	Train:[Loss: 17.7406, Acc: 0.7981] 	Val:[Loss: 13.9210, Acc: 0.8427]
Epoch: 2500 	Train:[Loss: 16.9820, Acc: 0.8067] 	Val:[Loss: 13.2842, Acc: 0.8499]
Epoch: 2600 	Train:[Loss: 16.2812, Acc: 0.8147] 	Val:[Loss: 12.6964, Acc: 0.8565]
Epoch: 2700 	Train:[Loss: 15.6314, Acc: 0.8220] 	Val:[Loss: 12.1492, Acc: 0.8627]
Epoch: 2800 	Train:[Loss: 15.0280, Acc: 0.8289] 	Val:[Loss: 11.6369, Acc: 0.8685]
Epoch: 2900 	Train:[Loss: 14.4676, Acc: 0.8353] 	Val:[Loss: 11.1560, Acc: 0.8739]
Epoch: 3000 	Train:[Loss: 13.9470, Acc: 0.8412] 	Val:[Loss: 10.7047, Acc: 0.8790]
Epoch: 3100 	Train:[Loss: 13.4635, Acc: 0.8467] 	Val:[Loss: 10.2822, Acc: 0.8838]
Epoch: 3200 	Train:[Loss: 13.0141, Acc: 0.8518] 	Val:[Loss: 9.8879, Acc: 0.8883]
Epoch: 3300 	Train:[Loss: 12.5960, Acc: 0.8565] 	Val:[Loss: 9.5212, Acc: 0.8924]
Epoch: 3400 	Train:[Loss: 12.2064, Acc: 0.8610] 	Val:[Loss: 9.1812, Acc: 0.8962]
Epoch: 3500 	Train:[Loss: 11.8429, Acc: 0.8651] 	Val:[Loss: 8.8668, Acc: 0.8998]
Epoch: 3600 	Train:[Loss: 11.5033, Acc: 0.8689] 	Val:[Loss: 8.5765, Acc: 0.9031]
Epoch: 3700 	Train:[Loss: 11.1856, Acc: 0.8725] 	Val:[Loss: 8.3089, Acc: 0.9061]
Epoch: 3800 	Train:[Loss: 10.8880, Acc: 0.8759] 	Val:[Loss: 8.0624, Acc: 0.9089]
Epoch: 3900 	Train:[Loss: 10.6086, Acc: 0.8791] 	Val:[Loss: 7.8355, Acc: 0.9114]
Epoch: 3999 	Train:[Loss: 10.3485, Acc: 0.8820] 	Val:[Loss: 7.6285, Acc: 0.9138]
[38;5;247mic[39m[38;5;245m|[39m[38;5;245m [39m[38;5;247mpredictions[39m[38;5;245m.[39m[38;5;247mshape[39m[38;5;245m:[39m[38;5;245m [39m[38;5;245m([39m[38;5;36m50[39m[38;5;245m,[39m[38;5;245m [39m[38;5;36m1[39m[38;5;245m)[39m
[38;5;247mic[39m[38;5;245m|[39m[38;5;245m [39m[38;5;247mground_truth[39m[38;5;245m.[39m[38;5;247mshape[39m[38;5;245m:[39m[38;5;245m [39m[38;5;245m([39m[38;5;36m50[39m[38;5;245m,[39m[38;5;245m)[39m
[0m
