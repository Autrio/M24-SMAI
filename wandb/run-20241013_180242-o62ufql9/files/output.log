(506,)
(506, 13)
Data split into training (406 samples), validation (50 samples), and testing (50 samples) sets.
Number of classes: 1
Feature data normalized using z-score normalization.
Layer: [in:13] [out:16] [activation:Sigmoid]
Layer: [in:16] [out:32] [activation:Sigmoid]
Layer: [in:32] [out:32] [activation:Sigmoid]
Layer: [in:32] [out:16] [activation:Sigmoid]
Layer: [in:16] [out:1] [activation:Linear]
                                                                                                                                                                                                           

Epoch: 0 	Train:[Loss: 640.7974, Acc: -6.1076] 	Val:[Loss: 602.2831, Acc: -5.8066]
Epoch: 100 	Train:[Loss: 95.3176, Acc: -0.0573] 	Val:[Loss: 91.5861, Acc: -0.0350]
Epoch: 200 	Train:[Loss: 89.9001, Acc: 0.0028] 	Val:[Loss: 88.3099, Acc: 0.0020]
Epoch: 300 	Train:[Loss: 89.8044, Acc: 0.0039] 	Val:[Loss: 88.3002, Acc: 0.0021]
Epoch: 400 	Train:[Loss: 89.7136, Acc: 0.0049] 	Val:[Loss: 88.2008, Acc: 0.0032]
Epoch: 500 	Train:[Loss: 89.6085, Acc: 0.0061] 	Val:[Loss: 88.0798, Acc: 0.0046]
Epoch: 600 	Train:[Loss: 89.4828, Acc: 0.0075] 	Val:[Loss: 87.9341, Acc: 0.0062]
Epoch: 700 	Train:[Loss: 89.3274, Acc: 0.0092] 	Val:[Loss: 87.7530, Acc: 0.0083]
Epoch: 800 	Train:[Loss: 89.1283, Acc: 0.0114] 	Val:[Loss: 87.5201, Acc: 0.0109]
Epoch: 900 	Train:[Loss: 88.8634, Acc: 0.0143] 	Val:[Loss: 87.2088, Acc: 0.0144]
Epoch: 1000 	Train:[Loss: 88.4956, Acc: 0.0184] 	Val:[Loss: 86.7745, Acc: 0.0193]
Epoch: 1100 	Train:[Loss: 87.9585, Acc: 0.0244] 	Val:[Loss: 86.1378, Acc: 0.0265]
Epoch: 1200 	Train:[Loss: 87.1270, Acc: 0.0336] 	Val:[Loss: 85.1490, Acc: 0.0377]
Epoch: 1300 	Train:[Loss: 85.7514, Acc: 0.0489] 	Val:[Loss: 83.5096, Acc: 0.0562]
Epoch: 1400 	Train:[Loss: 83.3210, Acc: 0.0758] 	Val:[Loss: 80.6134, Acc: 0.0890]
Epoch: 1500 	Train:[Loss: 78.8889, Acc: 0.1250] 	Val:[Loss: 75.3556, Acc: 0.1484]
Epoch: 1600 	Train:[Loss: 71.2762, Acc: 0.2094] 	Val:[Loss: 66.3867, Acc: 0.2497]
Epoch: 1700 	Train:[Loss: 60.8056, Acc: 0.3256] 	Val:[Loss: 54.0987, Acc: 0.3886]
Epoch: 1800 	Train:[Loss: 50.7903, Acc: 0.4366] 	Val:[Loss: 42.3166, Acc: 0.5218]
Epoch: 1900 	Train:[Loss: 42.9112, Acc: 0.5240] 	Val:[Loss: 33.2177, Acc: 0.6246]
Epoch: 2000 	Train:[Loss: 36.6884, Acc: 0.5931] 	Val:[Loss: 26.5460, Acc: 0.7000]
Epoch: 2100 	Train:[Loss: 32.2146, Acc: 0.6427] 	Val:[Loss: 22.0638, Acc: 0.7506]
Epoch: 2200 	Train:[Loss: 29.1483, Acc: 0.6767] 	Val:[Loss: 19.1788, Acc: 0.7833]
Epoch: 2300 	Train:[Loss: 26.9312, Acc: 0.7013] 	Val:[Loss: 17.3064, Acc: 0.8044]
Epoch: 2400 	Train:[Loss: 25.2169, Acc: 0.7203] 	Val:[Loss: 16.0523, Acc: 0.8186]
Epoch: 2500 	Train:[Loss: 23.8324, Acc: 0.7357] 	Val:[Loss: 15.1848, Acc: 0.8284]
Epoch: 2600 	Train:[Loss: 22.6845, Acc: 0.7484] 	Val:[Loss: 14.5689, Acc: 0.8354]
Epoch: 2700 	Train:[Loss: 21.7091, Acc: 0.7592] 	Val:[Loss: 14.1158, Acc: 0.8405]
Epoch: 2800 	Train:[Loss: 20.8527, Acc: 0.7687] 	Val:[Loss: 13.7645, Acc: 0.8444]
Epoch: 2900 	Train:[Loss: 20.0776, Acc: 0.7773] 	Val:[Loss: 13.4774, Acc: 0.8477]
Epoch: 3000 	Train:[Loss: 19.3616, Acc: 0.7852] 	Val:[Loss: 13.2339, Acc: 0.8504]
Epoch: 3100 	Train:[Loss: 18.6918, Acc: 0.7927] 	Val:[Loss: 13.0211, Acc: 0.8528]
Epoch: 3200 	Train:[Loss: 18.0595, Acc: 0.7997] 	Val:[Loss: 12.8277, Acc: 0.8550]
Epoch: 3300 	Train:[Loss: 17.4584, Acc: 0.8064] 	Val:[Loss: 12.6423, Acc: 0.8571]
Epoch: 3400 	Train:[Loss: 16.8840, Acc: 0.8127] 	Val:[Loss: 12.4545, Acc: 0.8592]
Epoch: 3500 	Train:[Loss: 16.3328, Acc: 0.8188] 	Val:[Loss: 12.2566, Acc: 0.8615]
Epoch: 3600 	Train:[Loss: 15.8029, Acc: 0.8247] 	Val:[Loss: 12.0459, Acc: 0.8639]
Epoch: 3700 	Train:[Loss: 15.2921, Acc: 0.8304] 	Val:[Loss: 11.8245, Acc: 0.8664]
Epoch: 3800 	Train:[Loss: 14.7982, Acc: 0.8359] 	Val:[Loss: 11.5983, Acc: 0.8689]
Epoch: 3900 	Train:[Loss: 14.3174, Acc: 0.8412] 	Val:[Loss: 11.3746, Acc: 0.8715]
Epoch: 4000 	Train:[Loss: 13.8442, Acc: 0.8464] 	Val:[Loss: 11.1601, Acc: 0.8739]
Epoch: 4100 	Train:[Loss: 13.3707, Acc: 0.8517] 	Val:[Loss: 10.9608, Acc: 0.8761]
Epoch: 4200 	Train:[Loss: 12.8892, Acc: 0.8570] 	Val:[Loss: 10.7820, Acc: 0.8781]
Epoch: 4300 	Train:[Loss: 12.3967, Acc: 0.8625] 	Val:[Loss: 10.6272, Acc: 0.8799]
Epoch: 4400 	Train:[Loss: 11.8951, Acc: 0.8681] 	Val:[Loss: 10.4933, Acc: 0.8814]
Epoch: 4500 	Train:[Loss: 11.3901, Acc: 0.8737] 	Val:[Loss: 10.3707, Acc: 0.8828]
Epoch: 4600 	Train:[Loss: 10.8979, Acc: 0.8791] 	Val:[Loss: 10.2478, Acc: 0.8842]
Epoch: 4700 	Train:[Loss: 10.4337, Acc: 0.8843] 	Val:[Loss: 10.1138, Acc: 0.8857]
Epoch: 4800 	Train:[Loss: 9.9988, Acc: 0.8891] 	Val:[Loss: 9.9665, Acc: 0.8874]
Epoch: 4900 	Train:[Loss: 9.5888, Acc: 0.8936] 	Val:[Loss: 9.8112, Acc: 0.8891]
Epoch: 4999 	Train:[Loss: 9.2047, Acc: 0.8979] 	Val:[Loss: 9.6552, Acc: 0.8909]
