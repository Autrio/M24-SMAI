Index(['age', 'gender', 'income', 'education', 'married', 'children', 'city',
       'occupation', 'purchase_amount', 'most bought item', 'labels'],
      dtype='object')
Data split into training (900 samples), validation (50 samples), and testing (50 samples) sets.
Number of classes: 8
Feature data normalized using z-score normalization.
                                                                                                                                                                                                            
Epoch: 0 	Train:[Loss: 0.3884, Acc: 0.4796] 	Val:[Loss: 0.4001, Acc: 0.4336]
Epoch: 64 	Train:[Loss: 0.2421, Acc: 0.6452] 	Val:[Loss: 0.2611, Acc: 0.6055]
Epoch: 128 	Train:[Loss: 0.2258, Acc: 0.6604] 	Val:[Loss: 0.2592, Acc: 0.5977]
Epoch: 192 	Train:[Loss: 0.2206, Acc: 0.6681] 	Val:[Loss: 0.2584, Acc: 0.5898]
Epoch: 256 	Train:[Loss: 0.2177, Acc: 0.6702] 	Val:[Loss: 0.2544, Acc: 0.5938]
Epoch: 320 	Train:[Loss: 0.2157, Acc: 0.6748] 	Val:[Loss: 0.2525, Acc: 0.6055]
Epoch: 384 	Train:[Loss: 0.2138, Acc: 0.6759] 	Val:[Loss: 0.2504, Acc: 0.5977]
Epoch: 448 	Train:[Loss: 0.2123, Acc: 0.6800] 	Val:[Loss: 0.2502, Acc: 0.5898]
Epoch: 512 	Train:[Loss: 0.2112, Acc: 0.6818] 	Val:[Loss: 0.2510, Acc: 0.5742]
Epoch: 576 	Train:[Loss: 0.2100, Acc: 0.6843] 	Val:[Loss: 0.2518, Acc: 0.5742]
Epoch: 640 	Train:[Loss: 0.2088, Acc: 0.6853] 	Val:[Loss: 0.2535, Acc: 0.5820]
Epoch: 704 	Train:[Loss: 0.2076, Acc: 0.6871] 	Val:[Loss: 0.2541, Acc: 0.5703]
Epoch: 768 	Train:[Loss: 0.2067, Acc: 0.6886] 	Val:[Loss: 0.2522, Acc: 0.5703]
Epoch: 832 	Train:[Loss: 0.2059, Acc: 0.6899] 	Val:[Loss: 0.2512, Acc: 0.5781]
Epoch: 896 	Train:[Loss: 0.2051, Acc: 0.6914] 	Val:[Loss: 0.2520, Acc: 0.5859]
Epoch: 960 	Train:[Loss: 0.2043, Acc: 0.6915] 	Val:[Loss: 0.2527, Acc: 0.5859]
Epoch: 1024 	Train:[Loss: 0.2036, Acc: 0.6936] 	Val:[Loss: 0.2538, Acc: 0.5859]
Epoch: 1088 	Train:[Loss: 0.2029, Acc: 0.6960] 	Val:[Loss: 0.2546, Acc: 0.5742]
Epoch: 1152 	Train:[Loss: 0.2022, Acc: 0.6973] 	Val:[Loss: 0.2550, Acc: 0.5742]
Epoch: 1216 	Train:[Loss: 0.2016, Acc: 0.6991] 	Val:[Loss: 0.2556, Acc: 0.5664]
Epoch: 1280 	Train:[Loss: 0.2011, Acc: 0.7012] 	Val:[Loss: 0.2564, Acc: 0.5703]
Epoch: 1344 	Train:[Loss: 0.2006, Acc: 0.7015] 	Val:[Loss: 0.2571, Acc: 0.5703]
Epoch: 1408 	Train:[Loss: 0.2002, Acc: 0.7027] 	Val:[Loss: 0.2579, Acc: 0.5742]
Epoch: 1472 	Train:[Loss: 0.1998, Acc: 0.7040] 	Val:[Loss: 0.2589, Acc: 0.5664]
Epoch: 1536 	Train:[Loss: 0.1994, Acc: 0.7037] 	Val:[Loss: 0.2598, Acc: 0.5625]
Epoch: 1600 	Train:[Loss: 0.1991, Acc: 0.7045] 	Val:[Loss: 0.2606, Acc: 0.5625]
Epoch: 1664 	Train:[Loss: 0.1988, Acc: 0.7041] 	Val:[Loss: 0.2613, Acc: 0.5625]
Epoch: 1728 	Train:[Loss: 0.1984, Acc: 0.7052] 	Val:[Loss: 0.2619, Acc: 0.5664]
Epoch: 1792 	Train:[Loss: 0.1981, Acc: 0.7044] 	Val:[Loss: 0.2624, Acc: 0.5703]
Epoch: 1856 	Train:[Loss: 0.1978, Acc: 0.7054] 	Val:[Loss: 0.2629, Acc: 0.5703]
Epoch: 1920 	Train:[Loss: 0.1975, Acc: 0.7058] 	Val:[Loss: 0.2634, Acc: 0.5703]
Epoch: 1984 	Train:[Loss: 0.1972, Acc: 0.7073] 	Val:[Loss: 0.2640, Acc: 0.5703]
Epoch: 2048 	Train:[Loss: 0.1969, Acc: 0.7079] 	Val:[Loss: 0.2646, Acc: 0.5703]
Epoch: 2112 	Train:[Loss: 0.1967, Acc: 0.7084] 	Val:[Loss: 0.2654, Acc: 0.5625]
Epoch: 2176 	Train:[Loss: 0.1964, Acc: 0.7084] 	Val:[Loss: 0.2662, Acc: 0.5625]
Epoch: 2240 	Train:[Loss: 0.1961, Acc: 0.7086] 	Val:[Loss: 0.2672, Acc: 0.5664]
Epoch: 2304 	Train:[Loss: 0.1959, Acc: 0.7088] 	Val:[Loss: 0.2683, Acc: 0.5664]
Epoch: 2368 	Train:[Loss: 0.1956, Acc: 0.7095] 	Val:[Loss: 0.2695, Acc: 0.5664]
Epoch: 2432 	Train:[Loss: 0.1954, Acc: 0.7097] 	Val:[Loss: 0.2708, Acc: 0.5664]
Epoch: 2496 	Train:[Loss: 0.1951, Acc: 0.7109] 	Val:[Loss: 0.2720, Acc: 0.5664]
Epoch: 2560 	Train:[Loss: 0.1949, Acc: 0.7114] 	Val:[Loss: 0.2730, Acc: 0.5625]
Epoch: 2624 	Train:[Loss: 0.1947, Acc: 0.7111] 	Val:[Loss: 0.2738, Acc: 0.5625]
Epoch: 2688 	Train:[Loss: 0.1944, Acc: 0.7130] 	Val:[Loss: 0.2748, Acc: 0.5547]
Epoch: 2752 	Train:[Loss: 0.1941, Acc: 0.7133] 	Val:[Loss: 0.2759, Acc: 0.5469]
Epoch: 2816 	Train:[Loss: 0.1938, Acc: 0.7147] 	Val:[Loss: 0.2769, Acc: 0.5508]
Epoch: 2880 	Train:[Loss: 0.1936, Acc: 0.7155] 	Val:[Loss: 0.2777, Acc: 0.5508]
Epoch: 2944 	Train:[Loss: 0.1934, Acc: 0.7160] 	Val:[Loss: 0.2785, Acc: 0.5547]
Epoch: 3008 	Train:[Loss: 0.1932, Acc: 0.7160] 	Val:[Loss: 0.2792, Acc: 0.5547]
Epoch: 3072 	Train:[Loss: 0.1929, Acc: 0.7164] 	Val:[Loss: 0.2799, Acc: 0.5547]
Epoch: 3136 	Train:[Loss: 0.1927, Acc: 0.7179] 	Val:[Loss: 0.2807, Acc: 0.5547]
Epoch: 3200 	Train:[Loss: 0.1925, Acc: 0.7182] 	Val:[Loss: 0.2813, Acc: 0.5625]
Epoch: 3264 	Train:[Loss: 0.1923, Acc: 0.7188] 	Val:[Loss: 0.2818, Acc: 0.5625]
Epoch: 3328 	Train:[Loss: 0.1921, Acc: 0.7189] 	Val:[Loss: 0.2825, Acc: 0.5625]
Epoch: 3392 	Train:[Loss: 0.1919, Acc: 0.7194] 	Val:[Loss: 0.2831, Acc: 0.5625]
Epoch: 3456 	Train:[Loss: 0.1917, Acc: 0.7201] 	Val:[Loss: 0.2838, Acc: 0.5664]
Epoch: 3520 	Train:[Loss: 0.1915, Acc: 0.7207] 	Val:[Loss: 0.2844, Acc: 0.5664]
Epoch: 3584 	Train:[Loss: 0.1913, Acc: 0.7204] 	Val:[Loss: 0.2851, Acc: 0.5703]
Epoch: 3648 	Train:[Loss: 0.1912, Acc: 0.7207] 	Val:[Loss: 0.2857, Acc: 0.5664]
Epoch: 3712 	Train:[Loss: 0.1910, Acc: 0.7214] 	Val:[Loss: 0.2867, Acc: 0.5664]
Epoch: 3776 	Train:[Loss: 0.1909, Acc: 0.7213] 	Val:[Loss: 0.2874, Acc: 0.5664]
Epoch: 3840 	Train:[Loss: 0.1907, Acc: 0.7225] 	Val:[Loss: 0.2879, Acc: 0.5664]
Epoch: 3904 	Train:[Loss: 0.1905, Acc: 0.7225] 	Val:[Loss: 0.2887, Acc: 0.5625]
Epoch: 3968 	Train:[Loss: 0.1904, Acc: 0.7234] 	Val:[Loss: 0.2892, Acc: 0.5664]
Epoch: 4032 	Train:[Loss: 0.1902, Acc: 0.7236] 	Val:[Loss: 0.2893, Acc: 0.5664]
Epoch: 4096 	Train:[Loss: 0.1901, Acc: 0.7238] 	Val:[Loss: 0.2904, Acc: 0.5586]
Epoch: 4160 	Train:[Loss: 0.1899, Acc: 0.7250] 	Val:[Loss: 0.2911, Acc: 0.5586]
Epoch: 4224 	Train:[Loss: 0.1899, Acc: 0.7238] 	Val:[Loss: 0.2916, Acc: 0.5664]
Epoch: 4288 	Train:[Loss: 0.1896, Acc: 0.7245] 	Val:[Loss: 0.2928, Acc: 0.5625]
Epoch: 4352 	Train:[Loss: 0.1894, Acc: 0.7250] 	Val:[Loss: 0.2929, Acc: 0.5625]
Epoch: 4416 	Train:[Loss: 0.1895, Acc: 0.7241] 	Val:[Loss: 0.2940, Acc: 0.5547]
Epoch: 4480 	Train:[Loss: 0.1893, Acc: 0.7252] 	Val:[Loss: 0.2937, Acc: 0.5664]
Epoch: 4544 	Train:[Loss: 0.1891, Acc: 0.7250] 	Val:[Loss: 0.2943, Acc: 0.5664]
Epoch: 4608 	Train:[Loss: 0.1889, Acc: 0.7268] 	Val:[Loss: 0.2947, Acc: 0.5586]
Epoch: 4672 	Train:[Loss: 0.1891, Acc: 0.7261] 	Val:[Loss: 0.2956, Acc: 0.5547]
Epoch: 4736 	Train:[Loss: 0.1889, Acc: 0.7270] 	Val:[Loss: 0.2959, Acc: 0.5508]
Epoch: 4800 	Train:[Loss: 0.1886, Acc: 0.7288] 	Val:[Loss: 0.2945, Acc: 0.5547]
Epoch: 4864 	Train:[Loss: 0.1885, Acc: 0.7275] 	Val:[Loss: 0.2951, Acc: 0.5508]
Epoch: 4928 	Train:[Loss: 0.1884, Acc: 0.7281] 	Val:[Loss: 0.2950, Acc: 0.5547]
Epoch: 4992 	Train:[Loss: 0.1883, Acc: 0.7273] 	Val:[Loss: 0.2943, Acc: 0.5547]
['beauty', 'books', 'clothing', 'electronics', 'food', 'furniture', 'home', 'sports']
[0. 1. 1. 0. 0. 0. 0. 0.]
[[0.15310221 0.70582359 0.33409575 0.43552857 0.21982192 0.24694231
  0.11823362 0.54386625]]
[[0. 1. 0. 0. 0. 0. 0. 1.]]
