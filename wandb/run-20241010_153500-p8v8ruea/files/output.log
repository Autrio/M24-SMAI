Data split into training (915 samples), validation (114 samples), and testing (114 samples) sets.
Number of classes: 6
Feature data normalized using z-score normalization.
Layer: [in:11] [out:16] [activation:ReLU]
Layer: [in:16] [out:32] [activation:ReLU]
Layer: [in:32] [out:32] [activation:ReLU]
Layer: [in:32] [out:16] [activation:ReLU]
Layer: [in:16] [out:6] [activation:Softmax]
                                                                                                                                                                                                            

Epoch: 0 	Train:[Loss: 0.2122, Acc: 0.2958] 	Val:[Loss: 0.1708, Acc: 0.0104]
Epoch: 64 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 128 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 192 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 256 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 320 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 384 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 448 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 512 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 576 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 640 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 704 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 768 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 832 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 896 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 960 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 1024 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 1088 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 1152 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 1216 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 1280 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 1344 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 1408 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 1472 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 1536 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 1600 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 1664 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 1728 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 1792 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 1856 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 1920 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
Epoch: 1984 	Train:[Loss: 0.1076, Acc: 0.4263] 	Val:[Loss: 0.1059, Acc: 0.4375]
