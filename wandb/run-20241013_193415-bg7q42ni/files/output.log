(1143, 6)
(1143, 11)
Data split into training (915 samples), validation (114 samples), and testing (114 samples) sets.
Number of classes: 6
Feature data normalized using z-score normalization.
Layer: [in:11] [out:8] [activation:ReLU]
Layer: [in:8] [out:16] [activation:ReLU]
Layer: [in:16] [out:16] [activation:ReLU]
Layer: [in:16] [out:8] [activation:ReLU]
Layer: [in:8] [out:6] [activation:Softmax]
Training:   0%|▏                                                                                                                       | 2/1000 [00:00<00:29, 33.51epoch/s, Train Acc=0.488, Val Acc=0.396]/home/autrio/college-linx/SMAI/smai-m24-assignments-Autrio/models/MLP/MLP2.py:517: RuntimeWarning: overflow encountered in matmul

Epoch: 0 	Train:[Loss: 0.3902, Acc: 0.4286] 	Val:[Loss: 0.3294, Acc: 0.5104]
  z = x @ self.W.T + self.b
/home/autrio/college-linx/SMAI/smai-m24-assignments-Autrio/models/MLP/MLP2.py:426: RuntimeWarning: invalid value encountered in subtract
  shiftx = x - np.max(x, axis=1, keepdims=True)
/home/autrio/college-linx/SMAI/smai-m24-assignments-Autrio/models/MLP/MLP2.py:628: RuntimeWarning: overflow encountered in matmul
  dLy = dLy @ self.layers[i + 1].W
/home/autrio/college-linx/SMAI/smai-m24-assignments-Autrio/models/MLP/MLP2.py:629: RuntimeWarning: invalid value encountered in multiply
  dLy = dLy * daz
Training:  51%|██████████████████████████████████████████████████████████████▏                                                          | 514/1000 [00:14<00:13, 36.54epoch/s, Train Acc=0.0067, Val Acc=0]
Epoch: 100 	Train:[Loss: nan, Acc: 0.0067] 	Val:[Loss: nan, Acc: 0.0000]
Epoch: 200 	Train:[Loss: nan, Acc: 0.0067] 	Val:[Loss: nan, Acc: 0.0000]
Epoch: 300 	Train:[Loss: nan, Acc: 0.0067] 	Val:[Loss: nan, Acc: 0.0000]
Epoch: 400 	Train:[Loss: nan, Acc: 0.0067] 	Val:[Loss: nan, Acc: 0.0000]
Epoch: 500 	Train:[Loss: nan, Acc: 0.0067] 	Val:[Loss: nan, Acc: 0.0000]
Early stopping triggered at epoch 514.
Model weights restored to epoch 2.
best validation loss::0.3270887822793911
================Test set metrics======================

accuracy ::  0.8304093567251462
precision ::  0.20666666666666664
recall ::  0.053316645807259075
F1-score ::  0.0847652363683045

======================================================
