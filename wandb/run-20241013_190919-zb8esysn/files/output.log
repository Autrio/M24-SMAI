(506,)
(506, 13)
Data split into training (406 samples), validation (50 samples), and testing (50 samples) sets.
Number of classes: 1
Feature data normalized using z-score normalization.
{'lr': 0.02378130819485255, 'batch_size': 128, 'epoch': 5000, 'optimizer': 'mini-batch', 'loss_fn': 'MSELoss', 'activation': 'Tanh', 'type': 'regression', 'early_stopping': True, 'activations': 'Sigmoid', 'model_architecture': 'arch5'}
Layer: [in:13] [out:16] [activation:Sigmoid]
Layer: [in:16] [out:64] [activation:Sigmoid]
Layer: [in:64] [out:128] [activation:Sigmoid]
Layer: [in:128] [out:256] [activation:Sigmoid]
Layer: [in:256] [out:1] [activation:Linear]
                                                                                                                                                                                                           

Epoch: 0 	Train:[Loss: 624.3616, Acc: -5.9253] 	Val:[Loss: 453.4529, Acc: -4.1246]
Epoch: 100 	Train:[Loss: 89.5889, Acc: 0.0063] 	Val:[Loss: 87.9019, Acc: 0.0066]
Epoch: 200 	Train:[Loss: 88.9406, Acc: 0.0135] 	Val:[Loss: 87.0210, Acc: 0.0165]
Epoch: 300 	Train:[Loss: 88.1199, Acc: 0.0226] 	Val:[Loss: 85.9206, Acc: 0.0290]
Epoch: 400 	Train:[Loss: 86.9593, Acc: 0.0355] 	Val:[Loss: 84.3893, Acc: 0.0463]
Epoch: 500 	Train:[Loss: 85.1585, Acc: 0.0554] 	Val:[Loss: 82.0534, Acc: 0.0727]
Epoch: 600 	Train:[Loss: 82.1322, Acc: 0.0890] 	Val:[Loss: 78.1960, Acc: 0.1163]
Epoch: 700 	Train:[Loss: 76.7618, Acc: 0.1486] 	Val:[Loss: 71.4716, Acc: 0.1923]
Epoch: 800 	Train:[Loss: 67.5335, Acc: 0.2509] 	Val:[Loss: 60.0922, Acc: 0.3209]
Epoch: 900 	Train:[Loss: 55.0753, Acc: 0.3891] 	Val:[Loss: 44.8234, Acc: 0.4934]
Epoch: 1000 	Train:[Loss: 43.7371, Acc: 0.5149] 	Val:[Loss: 31.2288, Acc: 0.6471]
Epoch: 1100 	Train:[Loss: 34.3313, Acc: 0.6192] 	Val:[Loss: 21.6822, Acc: 0.7550]
Epoch: 1200 	Train:[Loss: 27.6279, Acc: 0.6936] 	Val:[Loss: 16.4976, Acc: 0.8136]
Epoch: 1300 	Train:[Loss: 24.3079, Acc: 0.7304] 	Val:[Loss: 14.5303, Acc: 0.8358]
Epoch: 1400 	Train:[Loss: 22.5453, Acc: 0.7499] 	Val:[Loss: 13.7196, Acc: 0.8449]
Epoch: 1500 	Train:[Loss: 21.3667, Acc: 0.7630] 	Val:[Loss: 13.2263, Acc: 0.8505]
Epoch: 1600 	Train:[Loss: 20.4688, Acc: 0.7730] 	Val:[Loss: 12.8279, Acc: 0.8550]
Epoch: 1700 	Train:[Loss: 19.7297, Acc: 0.7812] 	Val:[Loss: 12.4553, Acc: 0.8592]
Epoch: 1800 	Train:[Loss: 19.0867, Acc: 0.7883] 	Val:[Loss: 12.0830, Acc: 0.8634]
Epoch: 1900 	Train:[Loss: 18.5027, Acc: 0.7948] 	Val:[Loss: 11.7051, Acc: 0.8677]
Epoch: 2000 	Train:[Loss: 17.9533, Acc: 0.8009] 	Val:[Loss: 11.3256, Acc: 0.8720]
Epoch: 2100 	Train:[Loss: 17.4239, Acc: 0.8067] 	Val:[Loss: 10.9527, Acc: 0.8762]
Epoch: 2200 	Train:[Loss: 16.9090, Acc: 0.8124] 	Val:[Loss: 10.5950, Acc: 0.8803]
Epoch: 2300 	Train:[Loss: 16.4092, Acc: 0.8180] 	Val:[Loss: 10.2573, Acc: 0.8841]
Epoch: 2400 	Train:[Loss: 15.9238, Acc: 0.8234] 	Val:[Loss: 9.9395, Acc: 0.8877]
Epoch: 2500 	Train:[Loss: 15.4499, Acc: 0.8286] 	Val:[Loss: 9.6386, Acc: 0.8911]
Epoch: 2600 	Train:[Loss: 14.9830, Acc: 0.8338] 	Val:[Loss: 9.3515, Acc: 0.8943]
Epoch: 2700 	Train:[Loss: 14.5182, Acc: 0.8390] 	Val:[Loss: 9.0760, Acc: 0.8974]
Epoch: 2800 	Train:[Loss: 14.0508, Acc: 0.8441] 	Val:[Loss: 8.8118, Acc: 0.9004]
Epoch: 2900 	Train:[Loss: 13.5774, Acc: 0.8494] 	Val:[Loss: 8.5607, Acc: 0.9033]
Epoch: 3000 	Train:[Loss: 13.0998, Acc: 0.8547] 	Val:[Loss: 8.3253, Acc: 0.9059]
Epoch: 3100 	Train:[Loss: 12.6263, Acc: 0.8600] 	Val:[Loss: 8.1063, Acc: 0.9084]
Epoch: 3200 	Train:[Loss: 12.1646, Acc: 0.8651] 	Val:[Loss: 7.9016, Acc: 0.9107]
Epoch: 3300 	Train:[Loss: 11.7166, Acc: 0.8700] 	Val:[Loss: 7.7105, Acc: 0.9129]
Epoch: 3400 	Train:[Loss: 11.2835, Acc: 0.8748] 	Val:[Loss: 7.5351, Acc: 0.9148]
Epoch: 3500 	Train:[Loss: 10.8703, Acc: 0.8794] 	Val:[Loss: 7.3763, Acc: 0.9166]
Epoch: 3600 	Train:[Loss: 10.4834, Acc: 0.8837] 	Val:[Loss: 7.2323, Acc: 0.9183]
Epoch: 3700 	Train:[Loss: 10.1255, Acc: 0.8877] 	Val:[Loss: 7.1019, Acc: 0.9197]
Epoch: 3800 	Train:[Loss: 9.7954, Acc: 0.8914] 	Val:[Loss: 6.9859, Acc: 0.9211]
Epoch: 3900 	Train:[Loss: 9.4903, Acc: 0.8947] 	Val:[Loss: 6.8861, Acc: 0.9222]
Epoch: 4000 	Train:[Loss: 9.2076, Acc: 0.8979] 	Val:[Loss: 6.8033, Acc: 0.9231]
Epoch: 4100 	Train:[Loss: 8.9446, Acc: 0.9008] 	Val:[Loss: 6.7373, Acc: 0.9239]
Epoch: 4200 	Train:[Loss: 8.6991, Acc: 0.9035] 	Val:[Loss: 6.6868, Acc: 0.9244]
Epoch: 4300 	Train:[Loss: 8.4692, Acc: 0.9061] 	Val:[Loss: 6.6500, Acc: 0.9248]
Epoch: 4400 	Train:[Loss: 8.2533, Acc: 0.9085] 	Val:[Loss: 6.6251, Acc: 0.9251]
Epoch: 4500 	Train:[Loss: 8.0501, Acc: 0.9107] 	Val:[Loss: 6.6103, Acc: 0.9253]
Epoch: 4600 	Train:[Loss: 7.8584, Acc: 0.9128] 	Val:[Loss: 6.6038, Acc: 0.9254]
Epoch: 4700 	Train:[Loss: 7.6773, Acc: 0.9148] 	Val:[Loss: 6.6041, Acc: 0.9254]
Epoch: 4800 	Train:[Loss: 7.5060, Acc: 0.9167] 	Val:[Loss: 6.6099, Acc: 0.9253]
Epoch: 4900 	Train:[Loss: 7.3438, Acc: 0.9185] 	Val:[Loss: 6.6202, Acc: 0.9252]
Epoch: 4999 	Train:[Loss: 7.1919, Acc: 0.9202] 	Val:[Loss: 6.6338, Acc: 0.9250]
