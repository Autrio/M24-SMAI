(506,)
(506, 13)
Data split into training (406 samples), validation (50 samples), and testing (50 samples) sets.
Number of classes: 1
Feature data normalized using z-score normalization.
Layer: [in:13] [out:16] [activation:Linear]
Layer: [in:16] [out:32] [activation:Linear]
Layer: [in:32] [out:32] [activation:Linear]
Layer: [in:32] [out:16] [activation:Linear]
Layer: [in:16] [out:1] [activation:Linear]
                                                                                                                                                                                                            

Epoch: 0 	Train:[Loss: 659.1634, Acc: -6.3114] 	Val:[Loss: 483.6254, Acc: -4.4656]
Epoch: 128 	Train:[Loss: 25.0318, Acc: 0.7224] 	Val:[Loss: 15.7937, Acc: 0.8215]
Epoch: 256 	Train:[Loss: 23.3230, Acc: 0.7413] 	Val:[Loss: 12.8978, Acc: 0.8542]
Epoch: 384 	Train:[Loss: 23.2284, Acc: 0.7424] 	Val:[Loss: 12.6042, Acc: 0.8576]
Epoch: 512 	Train:[Loss: 23.2233, Acc: 0.7424] 	Val:[Loss: 12.5544, Acc: 0.8581]
Epoch: 640 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5438, Acc: 0.8582]
Epoch: 768 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5414, Acc: 0.8583]
Epoch: 896 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5408, Acc: 0.8583]
Epoch: 1024 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 1152 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 1280 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 1408 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 1536 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 1664 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 1792 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 1920 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 2048 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 2176 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 2304 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 2432 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 2560 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 2688 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 2816 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 2944 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 3072 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 3200 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 3328 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 3456 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 3584 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 3712 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 3840 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 3968 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 4096 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 4224 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 4352 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 4480 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 4608 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 4736 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 4864 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 4992 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
