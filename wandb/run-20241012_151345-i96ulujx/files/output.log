(506,)
(506, 13)
Data split into training (406 samples), validation (50 samples), and testing (50 samples) sets.
Number of classes: 1
Feature data normalized using z-score normalization.
Layer: [in:13] [out:8] [activation:Linear]
Layer: [in:8] [out:16] [activation:Linear]
Layer: [in:16] [out:16] [activation:Linear]
Layer: [in:16] [out:8] [activation:Linear]
Layer: [in:8] [out:1] [activation:Linear]
                                                                                                                                                                                                            

Epoch: 0 	Train:[Loss: 607.8182, Acc: -5.7418] 	Val:[Loss: 459.2610, Acc: -4.1903]
Epoch: 128 	Train:[Loss: 23.2294, Acc: 0.7423] 	Val:[Loss: 12.4943, Acc: 0.8588]
Epoch: 256 	Train:[Loss: 23.2231, Acc: 0.7424] 	Val:[Loss: 12.5282, Acc: 0.8584]
Epoch: 384 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5373, Acc: 0.8583]
Epoch: 512 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5398, Acc: 0.8583]
Epoch: 640 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5404, Acc: 0.8583]
Epoch: 768 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5406, Acc: 0.8583]
Epoch: 896 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5406, Acc: 0.8583]
Epoch: 1024 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 1152 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 1280 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 1408 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 1536 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 1664 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 1792 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 1920 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 2048 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 2176 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 2304 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 2432 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 2560 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 2688 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 2816 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 2944 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 3072 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 3200 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 3328 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 3456 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 3584 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 3712 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 3840 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 3968 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 4096 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 4224 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 4352 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 4480 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 4608 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 4736 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 4864 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
Epoch: 4992 	Train:[Loss: 23.2230, Acc: 0.7424] 	Val:[Loss: 12.5407, Acc: 0.8583]
