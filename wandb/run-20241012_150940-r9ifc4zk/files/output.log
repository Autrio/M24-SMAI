(506,)
(506, 13)
Data split into training (406 samples), validation (50 samples), and testing (50 samples) sets.
Number of classes: 1
Feature data normalized using z-score normalization.
Layer: [in:13] [out:8] [activation:Sigmoid]
Layer: [in:8] [out:16] [activation:Sigmoid]
Layer: [in:16] [out:16] [activation:Sigmoid]
Layer: [in:16] [out:8] [activation:Sigmoid]
Layer: [in:8] [out:1] [activation:Linear]
                                                                                                                                                                                                            

Epoch: 0 	Train:[Loss: 611.6968, Acc: -5.7849] 	Val:[Loss: 584.3138, Acc: -5.6035]
Epoch: 128 	Train:[Loss: 112.5502, Acc: -0.2484] 	Val:[Loss: 106.2722, Acc: -0.2010]
Epoch: 256 	Train:[Loss: 90.3389, Acc: -0.0020] 	Val:[Loss: 88.1541, Acc: 0.0037]
Epoch: 384 	Train:[Loss: 89.6817, Acc: 0.0053] 	Val:[Loss: 88.0418, Acc: 0.0050]
Epoch: 512 	Train:[Loss: 89.5356, Acc: 0.0069] 	Val:[Loss: 87.9493, Acc: 0.0061]
Epoch: 640 	Train:[Loss: 89.3484, Acc: 0.0090] 	Val:[Loss: 87.7257, Acc: 0.0086]
Epoch: 768 	Train:[Loss: 89.0673, Acc: 0.0121] 	Val:[Loss: 87.3693, Acc: 0.0126]
Epoch: 896 	Train:[Loss: 88.6094, Acc: 0.0172] 	Val:[Loss: 86.7823, Acc: 0.0192]
Epoch: 1024 	Train:[Loss: 87.7896, Acc: 0.0262] 	Val:[Loss: 85.7256, Acc: 0.0312]
Epoch: 1152 	Train:[Loss: 86.1856, Acc: 0.0440] 	Val:[Loss: 83.6584, Acc: 0.0545]
Epoch: 1280 	Train:[Loss: 82.9397, Acc: 0.0800] 	Val:[Loss: 79.5474, Acc: 0.1010]
Epoch: 1408 	Train:[Loss: 76.7242, Acc: 0.1490] 	Val:[Loss: 71.9558, Acc: 0.1868]
Epoch: 1536 	Train:[Loss: 67.1087, Acc: 0.2556] 	Val:[Loss: 60.6045, Acc: 0.3151]
Epoch: 1664 	Train:[Loss: 56.9079, Acc: 0.3688] 	Val:[Loss: 48.7609, Acc: 0.4489]
Epoch: 1792 	Train:[Loss: 48.9329, Acc: 0.4572] 	Val:[Loss: 39.2872, Acc: 0.5560]
Epoch: 1920 	Train:[Loss: 42.2200, Acc: 0.5317] 	Val:[Loss: 31.9039, Acc: 0.6394]
Epoch: 2048 	Train:[Loss: 36.5999, Acc: 0.5940] 	Val:[Loss: 27.1088, Acc: 0.6936]
Epoch: 2176 	Train:[Loss: 32.5301, Acc: 0.6392] 	Val:[Loss: 24.2715, Acc: 0.7257]
Epoch: 2304 	Train:[Loss: 29.4128, Acc: 0.6738] 	Val:[Loss: 22.1832, Acc: 0.7493]
Epoch: 2432 	Train:[Loss: 26.9261, Acc: 0.7013] 	Val:[Loss: 20.3916, Acc: 0.7695]
Epoch: 2560 	Train:[Loss: 24.9112, Acc: 0.7237] 	Val:[Loss: 18.8317, Acc: 0.7872]
Epoch: 2688 	Train:[Loss: 23.2577, Acc: 0.7420] 	Val:[Loss: 17.4785, Acc: 0.8025]
Epoch: 2816 	Train:[Loss: 21.8774, Acc: 0.7573] 	Val:[Loss: 16.2858, Acc: 0.8159]
Epoch: 2944 	Train:[Loss: 20.6992, Acc: 0.7704] 	Val:[Loss: 15.2155, Acc: 0.8280]
Epoch: 3072 	Train:[Loss: 19.6696, Acc: 0.7818] 	Val:[Loss: 14.2444, Acc: 0.8390]
Epoch: 3200 	Train:[Loss: 18.7498, Acc: 0.7920] 	Val:[Loss: 13.3587, Acc: 0.8490]
Epoch: 3328 	Train:[Loss: 17.9114, Acc: 0.8013] 	Val:[Loss: 12.5501, Acc: 0.8582]
Epoch: 3456 	Train:[Loss: 17.1317, Acc: 0.8100] 	Val:[Loss: 11.8140, Acc: 0.8665]
Epoch: 3584 	Train:[Loss: 16.3906, Acc: 0.8182] 	Val:[Loss: 11.1483, Acc: 0.8740]
Epoch: 3712 	Train:[Loss: 15.6655, Acc: 0.8262] 	Val:[Loss: 10.5524, Acc: 0.8807]
Epoch: 3840 	Train:[Loss: 14.9219, Acc: 0.8345] 	Val:[Loss: 10.0241, Acc: 0.8867]
Epoch: 3968 	Train:[Loss: 14.0925, Acc: 0.8437] 	Val:[Loss: 9.5557, Acc: 0.8920]
