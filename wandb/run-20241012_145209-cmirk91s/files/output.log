Index(['age', 'gender', 'income', 'education', 'married', 'children', 'city',
       'occupation', 'purchase_amount', 'most bought item', 'labels'],
      dtype='object')
(1000, 8)
(1000, 10)
Data split into training (800 samples), validation (100 samples), and testing (100 samples) sets.
Number of classes: 8
Feature data normalized using z-score normalization.
                                                                                                                                                                                                            
Epoch: 0 	Train:[Loss: 0.7468, Acc: 0.5047] 	Val:[Loss: 0.7162, Acc: 0.5495]
Epoch: 64 	Train:[Loss: 0.6324, Acc: 0.6580] 	Val:[Loss: 0.6485, Acc: 0.6615]
Epoch: 128 	Train:[Loss: 0.6234, Acc: 0.6642] 	Val:[Loss: 0.6501, Acc: 0.6628]
Epoch: 192 	Train:[Loss: 0.6147, Acc: 0.6688] 	Val:[Loss: 0.6521, Acc: 0.6458]
Epoch: 256 	Train:[Loss: 0.6051, Acc: 0.6781] 	Val:[Loss: 0.6572, Acc: 0.6536]
Epoch: 320 	Train:[Loss: 0.5956, Acc: 0.6831] 	Val:[Loss: 0.6603, Acc: 0.6523]
Epoch: 384 	Train:[Loss: 0.5874, Acc: 0.6890] 	Val:[Loss: 0.6654, Acc: 0.6432]
Epoch: 448 	Train:[Loss: 0.5800, Acc: 0.6937] 	Val:[Loss: 0.6728, Acc: 0.6237]
Epoch: 512 	Train:[Loss: 0.5731, Acc: 0.6992] 	Val:[Loss: 0.6842, Acc: 0.6198]
Epoch: 576 	Train:[Loss: 0.5669, Acc: 0.7090] 	Val:[Loss: 0.6966, Acc: 0.6146]
Epoch: 640 	Train:[Loss: 0.5614, Acc: 0.7119] 	Val:[Loss: 0.7070, Acc: 0.6185]
Epoch: 704 	Train:[Loss: 0.5565, Acc: 0.7145] 	Val:[Loss: 0.7185, Acc: 0.6159]
Epoch: 768 	Train:[Loss: 0.5514, Acc: 0.7163] 	Val:[Loss: 0.7312, Acc: 0.6107]
Epoch: 832 	Train:[Loss: 0.5465, Acc: 0.7196] 	Val:[Loss: 0.7443, Acc: 0.6120]
Epoch: 896 	Train:[Loss: 0.5420, Acc: 0.7215] 	Val:[Loss: 0.7557, Acc: 0.6068]
Epoch: 960 	Train:[Loss: 0.5380, Acc: 0.7238] 	Val:[Loss: 0.7650, Acc: 0.6068]
Epoch: 1024 	Train:[Loss: 0.5342, Acc: 0.7253] 	Val:[Loss: 0.7719, Acc: 0.6042]
Epoch: 1088 	Train:[Loss: 0.5306, Acc: 0.7264] 	Val:[Loss: 0.7779, Acc: 0.6029]
Epoch: 1152 	Train:[Loss: 0.5273, Acc: 0.7292] 	Val:[Loss: 0.7833, Acc: 0.6003]
Epoch: 1216 	Train:[Loss: 0.5241, Acc: 0.7327] 	Val:[Loss: 0.7900, Acc: 0.5938]
Epoch: 1280 	Train:[Loss: 0.5212, Acc: 0.7344] 	Val:[Loss: 0.7986, Acc: 0.5938]
Epoch: 1344 	Train:[Loss: 0.5182, Acc: 0.7354] 	Val:[Loss: 0.8083, Acc: 0.5820]
Epoch: 1408 	Train:[Loss: 0.5154, Acc: 0.7363] 	Val:[Loss: 0.8171, Acc: 0.5781]
Epoch: 1472 	Train:[Loss: 0.5126, Acc: 0.7373] 	Val:[Loss: 0.8243, Acc: 0.5872]
Epoch: 1536 	Train:[Loss: 0.5100, Acc: 0.7384] 	Val:[Loss: 0.8298, Acc: 0.5885]
Epoch: 1600 	Train:[Loss: 0.5075, Acc: 0.7406] 	Val:[Loss: 0.8353, Acc: 0.5859]
Epoch: 1664 	Train:[Loss: 0.5053, Acc: 0.7414] 	Val:[Loss: 0.8414, Acc: 0.5820]
Epoch: 1728 	Train:[Loss: 0.5033, Acc: 0.7409] 	Val:[Loss: 0.8470, Acc: 0.5872]
Epoch: 1792 	Train:[Loss: 0.5016, Acc: 0.7419] 	Val:[Loss: 0.8511, Acc: 0.5833]
Epoch: 1856 	Train:[Loss: 0.5000, Acc: 0.7433] 	Val:[Loss: 0.8537, Acc: 0.5781]
Epoch: 1920 	Train:[Loss: 0.4984, Acc: 0.7432] 	Val:[Loss: 0.8559, Acc: 0.5833]
Epoch: 1984 	Train:[Loss: 0.4968, Acc: 0.7435] 	Val:[Loss: 0.8580, Acc: 0.5807]
Epoch: 2048 	Train:[Loss: 0.4954, Acc: 0.7435] 	Val:[Loss: 0.8608, Acc: 0.5781]
Epoch: 2112 	Train:[Loss: 0.4942, Acc: 0.7428] 	Val:[Loss: 0.8620, Acc: 0.5794]
Epoch: 2176 	Train:[Loss: 0.4932, Acc: 0.7437] 	Val:[Loss: 0.8661, Acc: 0.5846]
Epoch: 2240 	Train:[Loss: 0.4920, Acc: 0.7435] 	Val:[Loss: 0.8683, Acc: 0.5872]
Epoch: 2304 	Train:[Loss: 0.4914, Acc: 0.7463] 	Val:[Loss: 0.8732, Acc: 0.5833]
Epoch: 2368 	Train:[Loss: 0.4913, Acc: 0.7471] 	Val:[Loss: 0.8742, Acc: 0.5833]
Epoch: 2432 	Train:[Loss: 0.4919, Acc: 0.7464] 	Val:[Loss: 0.8782, Acc: 0.5872]
Epoch: 2496 	Train:[Loss: 0.4901, Acc: 0.7484] 	Val:[Loss: 0.8779, Acc: 0.5911]
Epoch: 2560 	Train:[Loss: 0.4888, Acc: 0.7492] 	Val:[Loss: 0.8792, Acc: 0.5872]
Epoch: 2624 	Train:[Loss: 0.4893, Acc: 0.7500] 	Val:[Loss: 0.8787, Acc: 0.5846]
Epoch: 2688 	Train:[Loss: 0.4886, Acc: 0.7472] 	Val:[Loss: 0.8882, Acc: 0.5859]
Epoch: 2752 	Train:[Loss: 0.4874, Acc: 0.7503] 	Val:[Loss: 0.8897, Acc: 0.5898]
Epoch: 2816 	Train:[Loss: 0.4948, Acc: 0.7453] 	Val:[Loss: 0.8861, Acc: 0.5911]
Epoch: 2880 	Train:[Loss: 0.4883, Acc: 0.7476] 	Val:[Loss: 0.8965, Acc: 0.5938]
Epoch: 2944 	Train:[Loss: 0.4858, Acc: 0.7489] 	Val:[Loss: 0.8983, Acc: 0.5924]
Epoch: 3008 	Train:[Loss: 0.4838, Acc: 0.7490] 	Val:[Loss: 0.9035, Acc: 0.5911]
Epoch: 3072 	Train:[Loss: 0.4860, Acc: 0.7450] 	Val:[Loss: 0.9123, Acc: 0.5846]
Epoch: 3136 	Train:[Loss: 0.4834, Acc: 0.7495] 	Val:[Loss: 0.9128, Acc: 0.5846]
Epoch: 3200 	Train:[Loss: 0.4840, Acc: 0.7459] 	Val:[Loss: 0.9185, Acc: 0.5938]
Epoch: 3264 	Train:[Loss: 0.4866, Acc: 0.7464] 	Val:[Loss: 0.9223, Acc: 0.5911]
Epoch: 3328 	Train:[Loss: 0.4836, Acc: 0.7487] 	Val:[Loss: 0.9308, Acc: 0.5859]
Epoch: 3392 	Train:[Loss: 0.4847, Acc: 0.7448] 	Val:[Loss: 0.9275, Acc: 0.5924]
Epoch: 3456 	Train:[Loss: 0.4866, Acc: 0.7438] 	Val:[Loss: 0.9301, Acc: 0.5833]
Epoch: 3520 	Train:[Loss: 0.4822, Acc: 0.7489] 	Val:[Loss: 0.9328, Acc: 0.5964]
Epoch: 3584 	Train:[Loss: 0.4800, Acc: 0.7529] 	Val:[Loss: 0.9206, Acc: 0.5924]
Epoch: 3648 	Train:[Loss: 0.4800, Acc: 0.7484] 	Val:[Loss: 0.9273, Acc: 0.5820]
Epoch: 3712 	Train:[Loss: 0.4834, Acc: 0.7459] 	Val:[Loss: 0.9313, Acc: 0.5781]
Epoch: 3776 	Train:[Loss: 0.4836, Acc: 0.7505] 	Val:[Loss: 0.9225, Acc: 0.5872]
Epoch: 3840 	Train:[Loss: 0.4937, Acc: 0.7467] 	Val:[Loss: 0.9313, Acc: 0.5924]
Epoch: 3904 	Train:[Loss: 0.4809, Acc: 0.7533] 	Val:[Loss: 0.9254, Acc: 0.5924]
Epoch: 3968 	Train:[Loss: 0.4837, Acc: 0.7508] 	Val:[Loss: 0.9329, Acc: 0.5807]
Epoch: 4032 	Train:[Loss: 0.4949, Acc: 0.7414] 	Val:[Loss: 0.9237, Acc: 0.5833]
Epoch: 4096 	Train:[Loss: 0.4845, Acc: 0.7456] 	Val:[Loss: 0.9292, Acc: 0.5820]
Epoch: 4160 	Train:[Loss: 0.4881, Acc: 0.7412] 	Val:[Loss: 0.9164, Acc: 0.5859]
Epoch: 4224 	Train:[Loss: 0.4816, Acc: 0.7500] 	Val:[Loss: 0.9264, Acc: 0.5820]
Epoch: 4288 	Train:[Loss: 0.4880, Acc: 0.7467] 	Val:[Loss: 0.9332, Acc: 0.5885]
Epoch: 4352 	Train:[Loss: 0.4807, Acc: 0.7498] 	Val:[Loss: 0.9262, Acc: 0.5872]
Epoch: 4416 	Train:[Loss: 0.4823, Acc: 0.7476] 	Val:[Loss: 0.9093, Acc: 0.5846]
Epoch: 4480 	Train:[Loss: 0.4807, Acc: 0.7515] 	Val:[Loss: 0.9373, Acc: 0.5898]
Epoch: 4544 	Train:[Loss: 0.4765, Acc: 0.7544] 	Val:[Loss: 0.9331, Acc: 0.5885]
Epoch: 4608 	Train:[Loss: 0.4820, Acc: 0.7526] 	Val:[Loss: 0.9365, Acc: 0.5911]
Epoch: 4672 	Train:[Loss: 0.4833, Acc: 0.7521] 	Val:[Loss: 0.9362, Acc: 0.5859]
Epoch: 4736 	Train:[Loss: 0.4720, Acc: 0.7563] 	Val:[Loss: 0.9312, Acc: 0.5833]
Epoch: 4800 	Train:[Loss: 0.4898, Acc: 0.7463] 	Val:[Loss: 0.9333, Acc: 0.5911]
Epoch: 4864 	Train:[Loss: 0.4859, Acc: 0.7482] 	Val:[Loss: 0.9209, Acc: 0.5911]
Epoch: 4928 	Train:[Loss: 0.4882, Acc: 0.7497] 	Val:[Loss: 0.9130, Acc: 0.5924]
Epoch: 4992 	Train:[Loss: 0.4890, Acc: 0.7503] 	Val:[Loss: 0.9236, Acc: 0.5807]
['beauty', 'books', 'clothing', 'electronics', 'food', 'furniture', 'home', 'sports']
[0. 1. 0. 0. 0. 0. 0. 0.]
[[0.10806069 0.37920427 0.45748469 0.27176117 0.23041272 0.4056827
  0.5270351  0.39560108]]
[[0. 0. 0. 0. 0. 0. 1. 0.]]
