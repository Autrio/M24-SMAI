(506,)
(506, 13)
Data split into training (406 samples), validation (50 samples), and testing (50 samples) sets.
Number of classes: 1
Feature data normalized using z-score normalization.
{'lr': 0.04992996552537244, 'batch_size': 128, 'epoch': 4000, 'optimizer': 'mini-batch', 'loss_fn': 'MSELoss', 'activation': 'Tanh', 'type': 'regression', 'early_stopping': True, 'activations': 'Sigmoid', 'model_architecture': 'arch5'}
Layer: [in:13] [out:16] [activation:Sigmoid]
Layer: [in:16] [out:64] [activation:Sigmoid]
Layer: [in:64] [out:128] [activation:Sigmoid]
Layer: [in:128] [out:256] [activation:Sigmoid]
Layer: [in:256] [out:1] [activation:Linear]
Training:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                   | 2228/4000 [04:05<03:22,  8.75epoch/s, Train Acc=0.816, Val Acc=0.892][34m[1mwandb[0m: Ctrl + C detected. Stopping sweep.

Epoch: 0 	Train:[Loss: 622.3543, Acc: -5.9031] 	Val:[Loss: 450.6494, Acc: -4.0929]
Epoch: 100 	Train:[Loss: 89.8598, Acc: 0.0033] 	Val:[Loss: 88.2534, Acc: 0.0026]
Epoch: 200 	Train:[Loss: 89.5244, Acc: 0.0070] 	Val:[Loss: 87.8739, Acc: 0.0069]
Epoch: 300 	Train:[Loss: 89.1164, Acc: 0.0115] 	Val:[Loss: 87.4059, Acc: 0.0122]
Epoch: 400 	Train:[Loss: 88.5703, Acc: 0.0176] 	Val:[Loss: 86.7707, Acc: 0.0194]
Epoch: 500 	Train:[Loss: 87.7865, Acc: 0.0263] 	Val:[Loss: 85.8485, Acc: 0.0298]
Epoch: 600 	Train:[Loss: 86.5961, Acc: 0.0395] 	Val:[Loss: 84.4361, Acc: 0.0458]
Epoch: 700 	Train:[Loss: 84.6882, Acc: 0.0606] 	Val:[Loss: 82.1599, Acc: 0.0715]
Epoch: 800 	Train:[Loss: 81.4546, Acc: 0.0965] 	Val:[Loss: 78.2882, Acc: 0.1152]
Epoch: 900 	Train:[Loss: 75.7189, Acc: 0.1601] 	Val:[Loss: 71.4046, Acc: 0.1930]
Epoch: 1000 	Train:[Loss: 65.7628, Acc: 0.2706] 	Val:[Loss: 59.4173, Acc: 0.3285]
Epoch: 1100 	Train:[Loss: 51.8523, Acc: 0.4249] 	Val:[Loss: 42.4970, Acc: 0.5197]
Epoch: 1200 	Train:[Loss: 39.1540, Acc: 0.5657] 	Val:[Loss: 27.1187, Acc: 0.6935]
Epoch: 1300 	Train:[Loss: 30.5197, Acc: 0.6615] 	Val:[Loss: 17.9080, Acc: 0.7976]
Epoch: 1400 	Train:[Loss: 25.8129, Acc: 0.7137] 	Val:[Loss: 14.0193, Acc: 0.8416]
Epoch: 1500 	Train:[Loss: 23.2684, Acc: 0.7419] 	Val:[Loss: 12.6108, Acc: 0.8575]
Epoch: 1600 	Train:[Loss: 21.5987, Acc: 0.7604] 	Val:[Loss: 11.9708, Acc: 0.8647]
Epoch: 1700 	Train:[Loss: 20.3605, Acc: 0.7742] 	Val:[Loss: 11.5349, Acc: 0.8696]
Epoch: 1800 	Train:[Loss: 19.3849, Acc: 0.7850] 	Val:[Loss: 11.1423, Acc: 0.8741]
Epoch: 1900 	Train:[Loss: 18.5819, Acc: 0.7939] 	Val:[Loss: 10.7506, Acc: 0.8785]
Epoch: 2000 	Train:[Loss: 17.8947, Acc: 0.8015] 	Val:[Loss: 10.3608, Acc: 0.8829]
Epoch: 2100 	Train:[Loss: 17.2812, Acc: 0.8083] 	Val:[Loss: 9.9854, Acc: 0.8872]
Epoch: 2200 	Train:[Loss: 16.7090, Acc: 0.8147] 	Val:[Loss: 9.6353, Acc: 0.8911]
