Index(['age', 'gender', 'income', 'education', 'married', 'children', 'city',
       'occupation', 'purchase_amount', 'most bought item', 'labels'],
      dtype='object')
Data split into training (900 samples), validation (50 samples), and testing (50 samples) sets.
Number of classes: 8
Feature data normalized using z-score normalization.
                                                                                                                                                                                                            
Epoch: 0 	Train:[Loss: 1.2003, Acc: 0.4987] 	Val:[Loss: 0.9947, Acc: 0.5742]
Epoch: 64 	Train:[Loss: 0.6344, Acc: 0.6599] 	Val:[Loss: 0.6391, Acc: 0.6367]
Epoch: 128 	Train:[Loss: 0.6254, Acc: 0.6648] 	Val:[Loss: 0.6407, Acc: 0.6445]
Epoch: 192 	Train:[Loss: 0.6205, Acc: 0.6694] 	Val:[Loss: 0.6463, Acc: 0.6445]
Epoch: 256 	Train:[Loss: 0.6163, Acc: 0.6710] 	Val:[Loss: 0.6509, Acc: 0.6484]
Epoch: 320 	Train:[Loss: 0.6133, Acc: 0.6738] 	Val:[Loss: 0.6539, Acc: 0.6484]
Epoch: 384 	Train:[Loss: 0.6109, Acc: 0.6762] 	Val:[Loss: 0.6570, Acc: 0.6523]
Epoch: 448 	Train:[Loss: 0.6085, Acc: 0.6798] 	Val:[Loss: 0.6591, Acc: 0.6484]
Epoch: 512 	Train:[Loss: 0.6063, Acc: 0.6791] 	Val:[Loss: 0.6586, Acc: 0.6406]
Epoch: 576 	Train:[Loss: 0.6044, Acc: 0.6809] 	Val:[Loss: 0.6575, Acc: 0.6484]
Epoch: 640 	Train:[Loss: 0.6025, Acc: 0.6830] 	Val:[Loss: 0.6560, Acc: 0.6523]
Epoch: 704 	Train:[Loss: 0.6006, Acc: 0.6843] 	Val:[Loss: 0.6552, Acc: 0.6562]
Epoch: 768 	Train:[Loss: 0.5987, Acc: 0.6865] 	Val:[Loss: 0.6554, Acc: 0.6523]
Epoch: 832 	Train:[Loss: 0.5969, Acc: 0.6875] 	Val:[Loss: 0.6558, Acc: 0.6602]
Epoch: 896 	Train:[Loss: 0.5951, Acc: 0.6876] 	Val:[Loss: 0.6563, Acc: 0.6680]
Epoch: 960 	Train:[Loss: 0.5934, Acc: 0.6897] 	Val:[Loss: 0.6563, Acc: 0.6641]
Epoch: 1024 	Train:[Loss: 0.5918, Acc: 0.6927] 	Val:[Loss: 0.6566, Acc: 0.6602]
Epoch: 1088 	Train:[Loss: 0.5904, Acc: 0.6939] 	Val:[Loss: 0.6574, Acc: 0.6641]
Epoch: 1152 	Train:[Loss: 0.5891, Acc: 0.6948] 	Val:[Loss: 0.6584, Acc: 0.6602]
Epoch: 1216 	Train:[Loss: 0.5878, Acc: 0.6955] 	Val:[Loss: 0.6600, Acc: 0.6680]
Epoch: 1280 	Train:[Loss: 0.5864, Acc: 0.6956] 	Val:[Loss: 0.6621, Acc: 0.6641]
Epoch: 1344 	Train:[Loss: 0.5847, Acc: 0.6971] 	Val:[Loss: 0.6639, Acc: 0.6719]
Epoch: 1408 	Train:[Loss: 0.5833, Acc: 0.6963] 	Val:[Loss: 0.6652, Acc: 0.6719]
Epoch: 1472 	Train:[Loss: 0.5822, Acc: 0.6978] 	Val:[Loss: 0.6673, Acc: 0.6680]
Epoch: 1536 	Train:[Loss: 0.5811, Acc: 0.6989] 	Val:[Loss: 0.6692, Acc: 0.6719]
Epoch: 1600 	Train:[Loss: 0.5800, Acc: 0.6987] 	Val:[Loss: 0.6706, Acc: 0.6719]
Epoch: 1664 	Train:[Loss: 0.5791, Acc: 0.6996] 	Val:[Loss: 0.6716, Acc: 0.6680]
Epoch: 1728 	Train:[Loss: 0.5783, Acc: 0.7002] 	Val:[Loss: 0.6729, Acc: 0.6680]
Epoch: 1792 	Train:[Loss: 0.5775, Acc: 0.6999] 	Val:[Loss: 0.6746, Acc: 0.6758]
Epoch: 1856 	Train:[Loss: 0.5767, Acc: 0.6995] 	Val:[Loss: 0.6756, Acc: 0.6719]
Epoch: 1920 	Train:[Loss: 0.5760, Acc: 0.6996] 	Val:[Loss: 0.6779, Acc: 0.6680]
Epoch: 1984 	Train:[Loss: 0.5752, Acc: 0.7006] 	Val:[Loss: 0.6808, Acc: 0.6602]
Epoch: 2048 	Train:[Loss: 0.5743, Acc: 0.7012] 	Val:[Loss: 0.6839, Acc: 0.6680]
Epoch: 2112 	Train:[Loss: 0.5735, Acc: 0.7026] 	Val:[Loss: 0.6894, Acc: 0.6719]
Epoch: 2176 	Train:[Loss: 0.5728, Acc: 0.7006] 	Val:[Loss: 0.6941, Acc: 0.6719]
Epoch: 2240 	Train:[Loss: 0.5722, Acc: 0.7027] 	Val:[Loss: 0.6997, Acc: 0.6602]
Epoch: 2304 	Train:[Loss: 0.5715, Acc: 0.7015] 	Val:[Loss: 0.7050, Acc: 0.6602]
Epoch: 2368 	Train:[Loss: 0.5715, Acc: 0.7015] 	Val:[Loss: 0.7136, Acc: 0.6602]
Epoch: 2432 	Train:[Loss: 0.5703, Acc: 0.7024] 	Val:[Loss: 0.7208, Acc: 0.6523]
Epoch: 2496 	Train:[Loss: 0.5700, Acc: 0.7040] 	Val:[Loss: 0.7266, Acc: 0.6602]
Epoch: 2560 	Train:[Loss: 0.5698, Acc: 0.7051] 	Val:[Loss: 0.7266, Acc: 0.6484]
Epoch: 2624 	Train:[Loss: 0.5683, Acc: 0.7047] 	Val:[Loss: 0.7311, Acc: 0.6523]
Epoch: 2688 	Train:[Loss: 0.5684, Acc: 0.7047] 	Val:[Loss: 0.7327, Acc: 0.6523]
Epoch: 2752 	Train:[Loss: 0.5681, Acc: 0.7041] 	Val:[Loss: 0.7388, Acc: 0.6523]
Epoch: 2816 	Train:[Loss: 0.5671, Acc: 0.7059] 	Val:[Loss: 0.7395, Acc: 0.6523]
Epoch: 2880 	Train:[Loss: 0.5665, Acc: 0.7059] 	Val:[Loss: 0.7431, Acc: 0.6445]
Epoch: 2944 	Train:[Loss: 0.5654, Acc: 0.7068] 	Val:[Loss: 0.7477, Acc: 0.6406]
Epoch: 3008 	Train:[Loss: 0.5647, Acc: 0.7070] 	Val:[Loss: 0.7528, Acc: 0.6328]
Epoch: 3072 	Train:[Loss: 0.5646, Acc: 0.7065] 	Val:[Loss: 0.7610, Acc: 0.6250]
Epoch: 3136 	Train:[Loss: 0.5645, Acc: 0.7056] 	Val:[Loss: 0.7641, Acc: 0.6250]
Epoch: 3200 	Train:[Loss: 0.5650, Acc: 0.7059] 	Val:[Loss: 0.7627, Acc: 0.6250]
Epoch: 3264 	Train:[Loss: 0.5646, Acc: 0.7052] 	Val:[Loss: 0.7704, Acc: 0.6289]
Epoch: 3328 	Train:[Loss: 0.5636, Acc: 0.7062] 	Val:[Loss: 0.7634, Acc: 0.6367]
Epoch: 3392 	Train:[Loss: 0.5622, Acc: 0.7097] 	Val:[Loss: 0.7677, Acc: 0.6289]
Epoch: 3456 	Train:[Loss: 0.5627, Acc: 0.7056] 	Val:[Loss: 0.7691, Acc: 0.6172]
Epoch: 3520 	Train:[Loss: 0.5618, Acc: 0.7073] 	Val:[Loss: 0.7766, Acc: 0.6172]
Epoch: 3584 	Train:[Loss: 0.5607, Acc: 0.7083] 	Val:[Loss: 0.7869, Acc: 0.6133]
Epoch: 3648 	Train:[Loss: 0.5610, Acc: 0.7079] 	Val:[Loss: 0.7937, Acc: 0.6094]
Epoch: 3712 	Train:[Loss: 0.5581, Acc: 0.7088] 	Val:[Loss: 0.7888, Acc: 0.6133]
Epoch: 3776 	Train:[Loss: 0.5589, Acc: 0.7081] 	Val:[Loss: 0.7902, Acc: 0.6094]
Epoch: 3840 	Train:[Loss: 0.5599, Acc: 0.7080] 	Val:[Loss: 0.7951, Acc: 0.6172]
Epoch: 3904 	Train:[Loss: 0.5572, Acc: 0.7094] 	Val:[Loss: 0.7963, Acc: 0.6133]
Epoch: 3968 	Train:[Loss: 0.5601, Acc: 0.7084] 	Val:[Loss: 0.7974, Acc: 0.6094]
Epoch: 4032 	Train:[Loss: 0.5594, Acc: 0.7093] 	Val:[Loss: 0.8006, Acc: 0.6172]
Epoch: 4096 	Train:[Loss: 0.5577, Acc: 0.7094] 	Val:[Loss: 0.8107, Acc: 0.6016]
Epoch: 4160 	Train:[Loss: 0.5563, Acc: 0.7119] 	Val:[Loss: 0.8050, Acc: 0.6133]
Epoch: 4224 	Train:[Loss: 0.5561, Acc: 0.7114] 	Val:[Loss: 0.8101, Acc: 0.6133]
Epoch: 4288 	Train:[Loss: 0.5544, Acc: 0.7119] 	Val:[Loss: 0.8121, Acc: 0.6133]
Epoch: 4352 	Train:[Loss: 0.5554, Acc: 0.7130] 	Val:[Loss: 0.8116, Acc: 0.6250]
Epoch: 4416 	Train:[Loss: 0.5530, Acc: 0.7150] 	Val:[Loss: 0.8151, Acc: 0.6094]
Epoch: 4480 	Train:[Loss: 0.5537, Acc: 0.7122] 	Val:[Loss: 0.8221, Acc: 0.6133]
Epoch: 4544 	Train:[Loss: 0.5541, Acc: 0.7136] 	Val:[Loss: 0.8204, Acc: 0.6289]
Epoch: 4608 	Train:[Loss: 0.5537, Acc: 0.7130] 	Val:[Loss: 0.8217, Acc: 0.6289]
Epoch: 4672 	Train:[Loss: 0.5527, Acc: 0.7126] 	Val:[Loss: 0.8307, Acc: 0.6211]
Epoch: 4736 	Train:[Loss: 0.5539, Acc: 0.7118] 	Val:[Loss: 0.8275, Acc: 0.6133]
Epoch: 4800 	Train:[Loss: 0.5530, Acc: 0.7130] 	Val:[Loss: 0.8341, Acc: 0.6250]
Epoch: 4864 	Train:[Loss: 0.5529, Acc: 0.7139] 	Val:[Loss: 0.8386, Acc: 0.6172]
Epoch: 4928 	Train:[Loss: 0.5528, Acc: 0.7148] 	Val:[Loss: 0.8360, Acc: 0.6016]
Epoch: 4992 	Train:[Loss: 0.5525, Acc: 0.7140] 	Val:[Loss: 0.8388, Acc: 0.6055]
['beauty', 'books', 'clothing', 'electronics', 'food', 'furniture', 'home', 'sports']
[0. 1. 1. 0. 0. 0. 0. 0.]
[[0.05267888 0.45661593 0.38910129 0.24707277 0.31704464 0.49000748
  0.32341955 0.37409129]]
[[0. 0. 0. 0. 0. 0. 0. 0.]]
