(506,)
(506, 13)
Data split into training (406 samples), validation (50 samples), and testing (50 samples) sets.
Number of classes: 1
Feature data normalized using z-score normalization.
Layer: [in:13] [out:8] [activation:Tanh]
Layer: [in:8] [out:16] [activation:Tanh]
Layer: [in:16] [out:16] [activation:Tanh]
Layer: [in:16] [out:8] [activation:Tanh]
Layer: [in:8] [out:1] [activation:Linear]
                                                                                                                                                                                                           

Epoch: 0 	Train:[Loss: 611.7635, Acc: -5.9881] 	Val:[Loss: 582.2694, Acc: -5.5804]
Epoch: 100 	Train:[Loss: 324.6430, Acc: -2.7082] 	Val:[Loss: 306.7663, Acc: -2.4669]
Epoch: 200 	Train:[Loss: 171.4482, Acc: -0.9578] 	Val:[Loss: 160.4037, Acc: -0.8128]
Epoch: 300 	Train:[Loss: 110.5639, Acc: -0.2620] 	Val:[Loss: 103.1196, Acc: -0.1654]
Epoch: 400 	Train:[Loss: 80.8018, Acc: 0.0779] 	Val:[Loss: 74.8584, Acc: 0.1540]
Epoch: 500 	Train:[Loss: 66.9142, Acc: 0.2366] 	Val:[Loss: 60.7776, Acc: 0.3131]
Epoch: 600 	Train:[Loss: 59.4363, Acc: 0.3220] 	Val:[Loss: 53.2013, Acc: 0.3988]
Epoch: 700 	Train:[Loss: 54.6752, Acc: 0.3764] 	Val:[Loss: 49.0102, Acc: 0.4461]
Epoch: 800 	Train:[Loss: 51.2944, Acc: 0.4149] 	Val:[Loss: 47.3932, Acc: 0.4644]
Epoch: 900 	Train:[Loss: 48.7916, Acc: 0.4436] 	Val:[Loss: 48.1410, Acc: 0.4559]
Epoch: 1000 	Train:[Loss: 46.5499, Acc: 0.4694] 	Val:[Loss: 50.3044, Acc: 0.4315]
Epoch: 1100 	Train:[Loss: 44.2952, Acc: 0.4953] 	Val:[Loss: 50.5521, Acc: 0.4287]
Epoch: 1200 	Train:[Loss: 42.2664, Acc: 0.5185] 	Val:[Loss: 47.8792, Acc: 0.4589]
Epoch: 1300 	Train:[Loss: 40.0617, Acc: 0.5437] 	Val:[Loss: 44.4985, Acc: 0.4971]
Epoch: 1400 	Train:[Loss: 36.6161, Acc: 0.5829] 	Val:[Loss: 40.5889, Acc: 0.5413]
Epoch: 1500 	Train:[Loss: 33.6740, Acc: 0.6165] 	Val:[Loss: 36.5026, Acc: 0.5875]
Epoch: 1600 	Train:[Loss: 31.2778, Acc: 0.6437] 	Val:[Loss: 33.2365, Acc: 0.6244]
Epoch: 1700 	Train:[Loss: 29.0878, Acc: 0.6687] 	Val:[Loss: 30.4520, Acc: 0.6559]
Epoch: 1800 	Train:[Loss: 27.0370, Acc: 0.6920] 	Val:[Loss: 27.9570, Acc: 0.6840]
Epoch: 1900 	Train:[Loss: 25.1504, Acc: 0.7135] 	Val:[Loss: 25.7855, Acc: 0.7086]
Epoch: 2000 	Train:[Loss: 23.4767, Acc: 0.7326] 	Val:[Loss: 24.0084, Acc: 0.7287]
Epoch: 2100 	Train:[Loss: 22.0251, Acc: 0.7491] 	Val:[Loss: 22.5903, Acc: 0.7447]
Epoch: 2200 	Train:[Loss: 20.7691, Acc: 0.7634] 	Val:[Loss: 21.4400, Acc: 0.7577]
Epoch: 2300 	Train:[Loss: 19.6732, Acc: 0.7758] 	Val:[Loss: 20.4748, Acc: 0.7686]
Epoch: 2400 	Train:[Loss: 18.7056, Acc: 0.7869] 	Val:[Loss: 19.6341, Acc: 0.7781]
Epoch: 2500 	Train:[Loss: 17.8403, Acc: 0.7967] 	Val:[Loss: 18.8763, Acc: 0.7867]
Epoch: 2600 	Train:[Loss: 17.0575, Acc: 0.8056] 	Val:[Loss: 18.1751, Acc: 0.7946]
Epoch: 2700 	Train:[Loss: 16.3427, Acc: 0.8138] 	Val:[Loss: 17.5178, Acc: 0.8020]
Epoch: 2800 	Train:[Loss: 15.6851, Acc: 0.8213] 	Val:[Loss: 16.9018, Acc: 0.8090]
Epoch: 2900 	Train:[Loss: 15.0769, Acc: 0.8282] 	Val:[Loss: 16.3297, Acc: 0.8155]
Epoch: 3000 	Train:[Loss: 14.5110, Acc: 0.8346] 	Val:[Loss: 15.8038, Acc: 0.8214]
Epoch: 3100 	Train:[Loss: 13.9805, Acc: 0.8407] 	Val:[Loss: 15.3217, Acc: 0.8268]
Epoch: 3200 	Train:[Loss: 13.4801, Acc: 0.8464] 	Val:[Loss: 14.8735, Acc: 0.8319]
Epoch: 3300 	Train:[Loss: 13.0069, Acc: 0.8518] 	Val:[Loss: 14.4434, Acc: 0.8368]
Epoch: 3400 	Train:[Loss: 12.5595, Acc: 0.8569] 	Val:[Loss: 14.0175, Acc: 0.8416]
Epoch: 3500 	Train:[Loss: 12.1365, Acc: 0.8617] 	Val:[Loss: 13.5913, Acc: 0.8464]
Epoch: 3600 	Train:[Loss: 11.7354, Acc: 0.8662] 	Val:[Loss: 13.1678, Acc: 0.8512]
Epoch: 3700 	Train:[Loss: 11.3543, Acc: 0.8706] 	Val:[Loss: 12.7526, Acc: 0.8559]
Epoch: 3800 	Train:[Loss: 10.9917, Acc: 0.8747] 	Val:[Loss: 12.3513, Acc: 0.8604]
Epoch: 3900 	Train:[Loss: 10.6468, Acc: 0.8786] 	Val:[Loss: 11.9676, Acc: 0.8648]
Epoch: 3999 	Train:[Loss: 10.3218, Acc: 0.8823] 	Val:[Loss: 11.6069, Acc: 0.8688]
================Test set metrics======================

R2 Score ::  0.8129043577596151
MSE ::  11.896484644063129
RMSE ::  3.4491280991089805
MAE ::  2.498106500214158

======================================================
