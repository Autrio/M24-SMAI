Index(['age', 'gender', 'income', 'education', 'married', 'children', 'city',
       'occupation', 'purchase_amount', 'most bought item', 'labels'],
      dtype='object')
Data split into training (900 samples), validation (50 samples), and testing (50 samples) sets.
Number of classes: 8
Feature data normalized using z-score normalization.
                                                                                                                                                                                                            
Epoch: 0 	Train:[Loss: 1.1193, Acc: 0.4722] 	Val:[Loss: 1.0788, Acc: 0.4570]
Epoch: 64 	Train:[Loss: 0.6304, Acc: 0.6634] 	Val:[Loss: 0.6576, Acc: 0.6406]
Epoch: 128 	Train:[Loss: 0.6216, Acc: 0.6709] 	Val:[Loss: 0.6523, Acc: 0.6406]
Epoch: 192 	Train:[Loss: 0.6163, Acc: 0.6727] 	Val:[Loss: 0.6561, Acc: 0.6562]
Epoch: 256 	Train:[Loss: 0.6122, Acc: 0.6747] 	Val:[Loss: 0.6636, Acc: 0.6641]
Epoch: 320 	Train:[Loss: 0.6081, Acc: 0.6775] 	Val:[Loss: 0.6756, Acc: 0.6445]
Epoch: 384 	Train:[Loss: 0.6043, Acc: 0.6800] 	Val:[Loss: 0.6973, Acc: 0.6133]
Epoch: 448 	Train:[Loss: 0.6009, Acc: 0.6814] 	Val:[Loss: 0.7108, Acc: 0.6094]
Epoch: 512 	Train:[Loss: 0.5983, Acc: 0.6821] 	Val:[Loss: 0.7178, Acc: 0.6016]
Epoch: 576 	Train:[Loss: 0.5959, Acc: 0.6836] 	Val:[Loss: 0.7216, Acc: 0.6016]
Epoch: 640 	Train:[Loss: 0.5928, Acc: 0.6867] 	Val:[Loss: 0.7268, Acc: 0.6055]
Epoch: 704 	Train:[Loss: 0.5902, Acc: 0.6879] 	Val:[Loss: 0.7266, Acc: 0.6133]
Epoch: 768 	Train:[Loss: 0.5879, Acc: 0.6895] 	Val:[Loss: 0.7298, Acc: 0.6094]
Epoch: 832 	Train:[Loss: 0.5858, Acc: 0.6892] 	Val:[Loss: 0.7353, Acc: 0.6094]
Epoch: 896 	Train:[Loss: 0.5839, Acc: 0.6906] 	Val:[Loss: 0.7400, Acc: 0.6133]
Epoch: 960 	Train:[Loss: 0.5821, Acc: 0.6924] 	Val:[Loss: 0.7423, Acc: 0.6094]
Epoch: 1024 	Train:[Loss: 0.5805, Acc: 0.6925] 	Val:[Loss: 0.7438, Acc: 0.6094]
Epoch: 1088 	Train:[Loss: 0.5789, Acc: 0.6932] 	Val:[Loss: 0.7439, Acc: 0.6094]
Epoch: 1152 	Train:[Loss: 0.5777, Acc: 0.6961] 	Val:[Loss: 0.7428, Acc: 0.6094]
Epoch: 1216 	Train:[Loss: 0.5762, Acc: 0.6977] 	Val:[Loss: 0.7416, Acc: 0.6055]
Epoch: 1280 	Train:[Loss: 0.5745, Acc: 0.6971] 	Val:[Loss: 0.7419, Acc: 0.6094]
Epoch: 1344 	Train:[Loss: 0.5730, Acc: 0.6981] 	Val:[Loss: 0.7427, Acc: 0.6094]
Epoch: 1408 	Train:[Loss: 0.5711, Acc: 0.6978] 	Val:[Loss: 0.7437, Acc: 0.6094]
Epoch: 1472 	Train:[Loss: 0.5701, Acc: 0.6978] 	Val:[Loss: 0.7395, Acc: 0.6133]
Epoch: 1536 	Train:[Loss: 0.5693, Acc: 0.6981] 	Val:[Loss: 0.7365, Acc: 0.6172]
Epoch: 1600 	Train:[Loss: 0.5686, Acc: 0.6991] 	Val:[Loss: 0.7341, Acc: 0.6211]
Epoch: 1664 	Train:[Loss: 0.5666, Acc: 0.7001] 	Val:[Loss: 0.7347, Acc: 0.6172]
Epoch: 1728 	Train:[Loss: 0.5653, Acc: 0.7020] 	Val:[Loss: 0.7369, Acc: 0.6172]
Epoch: 1792 	Train:[Loss: 0.5642, Acc: 0.7023] 	Val:[Loss: 0.7377, Acc: 0.6172]
Epoch: 1856 	Train:[Loss: 0.5644, Acc: 0.7030] 	Val:[Loss: 0.7391, Acc: 0.6133]
Epoch: 1920 	Train:[Loss: 0.5620, Acc: 0.7034] 	Val:[Loss: 0.7407, Acc: 0.6016]
Epoch: 1984 	Train:[Loss: 0.5612, Acc: 0.7040] 	Val:[Loss: 0.7446, Acc: 0.6055]
Epoch: 2048 	Train:[Loss: 0.5602, Acc: 0.7038] 	Val:[Loss: 0.7488, Acc: 0.6016]
Epoch: 2112 	Train:[Loss: 0.5599, Acc: 0.7033] 	Val:[Loss: 0.7513, Acc: 0.6016]
Epoch: 2176 	Train:[Loss: 0.5589, Acc: 0.7030] 	Val:[Loss: 0.7543, Acc: 0.6016]
Epoch: 2240 	Train:[Loss: 0.5582, Acc: 0.7045] 	Val:[Loss: 0.7525, Acc: 0.6016]
Epoch: 2304 	Train:[Loss: 0.5577, Acc: 0.7052] 	Val:[Loss: 0.7626, Acc: 0.5977]
Epoch: 2368 	Train:[Loss: 0.5561, Acc: 0.7037] 	Val:[Loss: 0.7622, Acc: 0.5977]
Epoch: 2432 	Train:[Loss: 0.5611, Acc: 0.7013] 	Val:[Loss: 0.7615, Acc: 0.5977]
Epoch: 2496 	Train:[Loss: 0.5570, Acc: 0.7035] 	Val:[Loss: 0.7753, Acc: 0.5938]
Epoch: 2560 	Train:[Loss: 0.5553, Acc: 0.7045] 	Val:[Loss: 0.7690, Acc: 0.6055]
Epoch: 2624 	Train:[Loss: 0.5548, Acc: 0.7037] 	Val:[Loss: 0.7794, Acc: 0.6016]
Epoch: 2688 	Train:[Loss: 0.5556, Acc: 0.7049] 	Val:[Loss: 0.7837, Acc: 0.6055]
Epoch: 2752 	Train:[Loss: 0.5560, Acc: 0.7038] 	Val:[Loss: 0.7762, Acc: 0.6094]
Epoch: 2816 	Train:[Loss: 0.5586, Acc: 0.7024] 	Val:[Loss: 0.7899, Acc: 0.5977]
Epoch: 2880 	Train:[Loss: 0.5558, Acc: 0.7031] 	Val:[Loss: 0.7796, Acc: 0.6094]
Epoch: 2944 	Train:[Loss: 0.5543, Acc: 0.7055] 	Val:[Loss: 0.7931, Acc: 0.6094]
Epoch: 3008 	Train:[Loss: 0.5529, Acc: 0.7062] 	Val:[Loss: 0.7873, Acc: 0.6133]
Epoch: 3072 	Train:[Loss: 0.5543, Acc: 0.7048] 	Val:[Loss: 0.7922, Acc: 0.6172]
Epoch: 3136 	Train:[Loss: 0.5525, Acc: 0.7061] 	Val:[Loss: 0.8013, Acc: 0.6133]
Epoch: 3200 	Train:[Loss: 0.5527, Acc: 0.7076] 	Val:[Loss: 0.7909, Acc: 0.6133]
Epoch: 3264 	Train:[Loss: 0.5530, Acc: 0.7049] 	Val:[Loss: 0.7857, Acc: 0.6094]
Epoch: 3328 	Train:[Loss: 0.5541, Acc: 0.7049] 	Val:[Loss: 0.7919, Acc: 0.6055]
Epoch: 3392 	Train:[Loss: 0.5520, Acc: 0.7041] 	Val:[Loss: 0.7871, Acc: 0.6172]
Epoch: 3456 	Train:[Loss: 0.5502, Acc: 0.7069] 	Val:[Loss: 0.7949, Acc: 0.5977]
Epoch: 3520 	Train:[Loss: 0.5593, Acc: 0.7041] 	Val:[Loss: 0.7653, Acc: 0.6172]
Epoch: 3584 	Train:[Loss: 0.5506, Acc: 0.7077] 	Val:[Loss: 0.7913, Acc: 0.6133]
Epoch: 3648 	Train:[Loss: 0.5570, Acc: 0.6996] 	Val:[Loss: 0.7897, Acc: 0.6094]
Epoch: 3712 	Train:[Loss: 0.5498, Acc: 0.7065] 	Val:[Loss: 0.7878, Acc: 0.6094]
Epoch: 3776 	Train:[Loss: 0.5492, Acc: 0.7070] 	Val:[Loss: 0.7704, Acc: 0.6094]
Epoch: 3840 	Train:[Loss: 0.5539, Acc: 0.7042] 	Val:[Loss: 0.7929, Acc: 0.6016]
Epoch: 3904 	Train:[Loss: 0.5502, Acc: 0.7081] 	Val:[Loss: 0.7859, Acc: 0.6016]
Epoch: 3968 	Train:[Loss: 0.5490, Acc: 0.7080] 	Val:[Loss: 0.7864, Acc: 0.6055]
Epoch: 4032 	Train:[Loss: 0.5487, Acc: 0.7059] 	Val:[Loss: 0.7785, Acc: 0.6055]
Epoch: 4096 	Train:[Loss: 0.5522, Acc: 0.7052] 	Val:[Loss: 0.7642, Acc: 0.6094]
Epoch: 4160 	Train:[Loss: 0.5480, Acc: 0.7086] 	Val:[Loss: 0.7939, Acc: 0.6094]
Epoch: 4224 	Train:[Loss: 0.5502, Acc: 0.7061] 	Val:[Loss: 0.7754, Acc: 0.6016]
Epoch: 4288 	Train:[Loss: 0.5542, Acc: 0.7055] 	Val:[Loss: 0.7778, Acc: 0.6211]
Epoch: 4352 	Train:[Loss: 0.5490, Acc: 0.7059] 	Val:[Loss: 0.7865, Acc: 0.6133]
Epoch: 4416 	Train:[Loss: 0.5501, Acc: 0.7087] 	Val:[Loss: 0.7939, Acc: 0.6055]
Epoch: 4480 	Train:[Loss: 0.5486, Acc: 0.7056] 	Val:[Loss: 0.8031, Acc: 0.6055]
Epoch: 4544 	Train:[Loss: 0.5656, Acc: 0.7030] 	Val:[Loss: 0.7983, Acc: 0.6016]
Epoch: 4608 	Train:[Loss: 0.5453, Acc: 0.7093] 	Val:[Loss: 0.8017, Acc: 0.6016]
Epoch: 4672 	Train:[Loss: 0.5451, Acc: 0.7087] 	Val:[Loss: 0.8004, Acc: 0.6016]
Epoch: 4736 	Train:[Loss: 0.5473, Acc: 0.7084] 	Val:[Loss: 0.8029, Acc: 0.5977]
Epoch: 4800 	Train:[Loss: 0.5452, Acc: 0.7104] 	Val:[Loss: 0.7988, Acc: 0.6094]
Epoch: 4864 	Train:[Loss: 0.5567, Acc: 0.7063] 	Val:[Loss: 0.7858, Acc: 0.6172]
Epoch: 4928 	Train:[Loss: 0.5467, Acc: 0.7097] 	Val:[Loss: 0.7915, Acc: 0.6016]
Epoch: 4992 	Train:[Loss: 0.5478, Acc: 0.7065] 	Val:[Loss: 0.7922, Acc: 0.6055]
['beauty', 'books', 'clothing', 'electronics', 'food', 'furniture', 'home', 'sports']
[0. 1. 1. 0. 0. 0. 0. 0.]
[[0.00628817 0.42508625 0.28779696 0.0463448  0.47428241 0.22281142
  0.26965118 0.38787422]]
[[0. 0. 0. 0. 0. 0. 0. 0.]]
