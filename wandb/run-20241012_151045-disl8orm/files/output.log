(506,)
(506, 13)
Data split into training (406 samples), validation (50 samples), and testing (50 samples) sets.
Number of classes: 1
Feature data normalized using z-score normalization.
Layer: [in:13] [out:8] [activation:Sigmoid]
Layer: [in:8] [out:16] [activation:Sigmoid]
Layer: [in:16] [out:16] [activation:Sigmoid]
Layer: [in:16] [out:8] [activation:Sigmoid]
Layer: [in:8] [out:1] [activation:Linear]
                                                                                                                                                                                                            

Epoch: 0 	Train:[Loss: 580.2445, Acc: -5.4360] 	Val:[Loss: 552.4934, Acc: -5.2439]
Epoch: 128 	Train:[Loss: 100.4063, Acc: -0.1137] 	Val:[Loss: 95.7130, Acc: -0.0817]
Epoch: 256 	Train:[Loss: 90.0107, Acc: 0.0016] 	Val:[Loss: 88.1564, Acc: 0.0037]
Epoch: 384 	Train:[Loss: 89.8248, Acc: 0.0037] 	Val:[Loss: 88.1815, Acc: 0.0034]
Epoch: 512 	Train:[Loss: 89.7116, Acc: 0.0049] 	Val:[Loss: 88.0529, Acc: 0.0049]
Epoch: 640 	Train:[Loss: 89.5672, Acc: 0.0065] 	Val:[Loss: 87.8639, Acc: 0.0070]
Epoch: 768 	Train:[Loss: 89.3719, Acc: 0.0087] 	Val:[Loss: 87.6086, Acc: 0.0099]
Epoch: 896 	Train:[Loss: 89.0905, Acc: 0.0118] 	Val:[Loss: 87.2443, Acc: 0.0140]
Epoch: 1024 	Train:[Loss: 88.6515, Acc: 0.0167] 	Val:[Loss: 86.6809, Acc: 0.0204]
Epoch: 1152 	Train:[Loss: 87.8886, Acc: 0.0252] 	Val:[Loss: 85.7094, Acc: 0.0314]
Epoch: 1280 	Train:[Loss: 86.3716, Acc: 0.0420] 	Val:[Loss: 83.7937, Acc: 0.0530]
Epoch: 1408 	Train:[Loss: 83.0634, Acc: 0.0787] 	Val:[Loss: 79.6923, Acc: 0.0994]
Epoch: 1536 	Train:[Loss: 76.2154, Acc: 0.1546] 	Val:[Loss: 71.4763, Acc: 0.1922]
Epoch: 1664 	Train:[Loss: 64.5380, Acc: 0.2842] 	Val:[Loss: 57.7127, Acc: 0.3478]
Epoch: 1792 	Train:[Loss: 52.3001, Acc: 0.4199] 	Val:[Loss: 43.2647, Acc: 0.5111]
Epoch: 1920 	Train:[Loss: 43.6295, Acc: 0.5161] 	Val:[Loss: 33.1589, Acc: 0.6253]
Epoch: 2048 	Train:[Loss: 36.4054, Acc: 0.5962] 	Val:[Loss: 26.7390, Acc: 0.6978]
Epoch: 2176 	Train:[Loss: 31.5431, Acc: 0.6501] 	Val:[Loss: 23.5663, Acc: 0.7337]
Epoch: 2304 	Train:[Loss: 28.2963, Acc: 0.6861] 	Val:[Loss: 21.5946, Acc: 0.7560]
Epoch: 2432 	Train:[Loss: 25.9432, Acc: 0.7122] 	Val:[Loss: 19.8925, Acc: 0.7752]
Epoch: 2560 	Train:[Loss: 24.1008, Acc: 0.7327] 	Val:[Loss: 18.3619, Acc: 0.7925]
Epoch: 2688 	Train:[Loss: 22.5946, Acc: 0.7494] 	Val:[Loss: 17.0244, Acc: 0.8076]
Epoch: 2816 	Train:[Loss: 21.3165, Acc: 0.7636] 	Val:[Loss: 15.8623, Acc: 0.8207]
Epoch: 2944 	Train:[Loss: 20.1878, Acc: 0.7761] 	Val:[Loss: 14.8463, Acc: 0.8322]
Epoch: 3072 	Train:[Loss: 19.1557, Acc: 0.7875] 	Val:[Loss: 13.9540, Acc: 0.8423]
Epoch: 3200 	Train:[Loss: 18.1912, Acc: 0.7982] 	Val:[Loss: 13.1744, Acc: 0.8511]
Epoch: 3328 	Train:[Loss: 17.2847, Acc: 0.8083] 	Val:[Loss: 12.5046, Acc: 0.8587]
Epoch: 3456 	Train:[Loss: 16.4374, Acc: 0.8177] 	Val:[Loss: 11.9447, Acc: 0.8650]
Epoch: 3584 	Train:[Loss: 15.6540, Acc: 0.8264] 	Val:[Loss: 11.4912, Acc: 0.8701]
Epoch: 3712 	Train:[Loss: 14.9373, Acc: 0.8343] 	Val:[Loss: 11.1348, Acc: 0.8742]
Epoch: 3840 	Train:[Loss: 14.2871, Acc: 0.8415] 	Val:[Loss: 10.8609, Acc: 0.8773]
Epoch: 3968 	Train:[Loss: 13.7008, Acc: 0.8480] 	Val:[Loss: 10.6523, Acc: 0.8796]
