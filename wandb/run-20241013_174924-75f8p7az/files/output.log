(506,)
(506, 13)
Data split into training (406 samples), validation (50 samples), and testing (50 samples) sets.
Number of classes: 1
Feature data normalized using z-score normalization.
Layer: [in:13] [out:8] [activation:Tanh]
Layer: [in:8] [out:16] [activation:Tanh]
Layer: [in:16] [out:16] [activation:Tanh]
Layer: [in:16] [out:8] [activation:Tanh]
Layer: [in:8] [out:1] [activation:Linear]
                                                                                                                                                                                                           

Epoch: 0 	Train:[Loss: 599.9823, Acc: -5.8521] 	Val:[Loss: 569.8685, Acc: -5.4403]
Epoch: 100 	Train:[Loss: 303.8189, Acc: -2.4699] 	Val:[Loss: 285.2847, Acc: -2.2241]
Epoch: 200 	Train:[Loss: 166.9470, Acc: -0.9063] 	Val:[Loss: 156.2485, Acc: -0.7658]
Epoch: 300 	Train:[Loss: 108.9515, Acc: -0.2436] 	Val:[Loss: 102.0261, Acc: -0.1530]
Epoch: 400 	Train:[Loss: 82.1545, Acc: 0.0626] 	Val:[Loss: 75.3060, Acc: 0.1489]
Epoch: 500 	Train:[Loss: 68.3194, Acc: 0.2208] 	Val:[Loss: 62.5188, Acc: 0.2935]
Epoch: 600 	Train:[Loss: 59.9068, Acc: 0.3169] 	Val:[Loss: 54.8910, Acc: 0.3797]
Epoch: 700 	Train:[Loss: 55.0115, Acc: 0.3729] 	Val:[Loss: 49.1621, Acc: 0.4444]
Epoch: 800 	Train:[Loss: 51.4510, Acc: 0.4135] 	Val:[Loss: 44.6535, Acc: 0.4954]
Epoch: 900 	Train:[Loss: 48.2711, Acc: 0.4498] 	Val:[Loss: 41.9734, Acc: 0.5256]
Epoch: 1000 	Train:[Loss: 45.2581, Acc: 0.4842] 	Val:[Loss: 40.1346, Acc: 0.5464]
Epoch: 1100 	Train:[Loss: 42.8768, Acc: 0.5115] 	Val:[Loss: 38.2240, Acc: 0.5680]
Epoch: 1200 	Train:[Loss: 40.8990, Acc: 0.5341] 	Val:[Loss: 36.4327, Acc: 0.5883]
Epoch: 1300 	Train:[Loss: 39.1207, Acc: 0.5545] 	Val:[Loss: 34.7292, Acc: 0.6075]
Epoch: 1400 	Train:[Loss: 37.2743, Acc: 0.5755] 	Val:[Loss: 32.9299, Acc: 0.6278]
Epoch: 1500 	Train:[Loss: 34.8773, Acc: 0.6028] 	Val:[Loss: 30.5997, Acc: 0.6542]
Epoch: 1600 	Train:[Loss: 31.9992, Acc: 0.6357] 	Val:[Loss: 27.6239, Acc: 0.6878]
Epoch: 1700 	Train:[Loss: 29.4020, Acc: 0.6654] 	Val:[Loss: 25.2871, Acc: 0.7142]
Epoch: 1800 	Train:[Loss: 27.3407, Acc: 0.6890] 	Val:[Loss: 23.5312, Acc: 0.7341]
Epoch: 1900 	Train:[Loss: 25.5603, Acc: 0.7093] 	Val:[Loss: 22.1182, Acc: 0.7500]
Epoch: 2000 	Train:[Loss: 23.9590, Acc: 0.7275] 	Val:[Loss: 20.8584, Acc: 0.7643]
Epoch: 2100 	Train:[Loss: 22.5444, Acc: 0.7436] 	Val:[Loss: 19.7250, Acc: 0.7771]
Epoch: 2200 	Train:[Loss: 21.3200, Acc: 0.7575] 	Val:[Loss: 18.7635, Acc: 0.7879]
Epoch: 2300 	Train:[Loss: 20.2320, Acc: 0.7699] 	Val:[Loss: 17.9566, Acc: 0.7971]
Epoch: 2400 	Train:[Loss: 19.2457, Acc: 0.7811] 	Val:[Loss: 17.2604, Acc: 0.8049]
Epoch: 2500 	Train:[Loss: 18.3460, Acc: 0.7913] 	Val:[Loss: 16.6418, Acc: 0.8119]
Epoch: 2600 	Train:[Loss: 17.5239, Acc: 0.8007] 	Val:[Loss: 16.0809, Acc: 0.8183]
Epoch: 2700 	Train:[Loss: 16.7712, Acc: 0.8092] 	Val:[Loss: 15.5674, Acc: 0.8241]
Epoch: 2800 	Train:[Loss: 16.0804, Acc: 0.8171] 	Val:[Loss: 15.0955, Acc: 0.8294]
Epoch: 2900 	Train:[Loss: 15.4457, Acc: 0.8243] 	Val:[Loss: 14.6601, Acc: 0.8343]
Epoch: 3000 	Train:[Loss: 14.8620, Acc: 0.8310] 	Val:[Loss: 14.2561, Acc: 0.8389]
Epoch: 3100 	Train:[Loss: 14.3244, Acc: 0.8371] 	Val:[Loss: 13.8795, Acc: 0.8431]
Epoch: 3200 	Train:[Loss: 13.8285, Acc: 0.8428] 	Val:[Loss: 13.5273, Acc: 0.8471]
Epoch: 3300 	Train:[Loss: 13.3703, Acc: 0.8480] 	Val:[Loss: 13.1973, Acc: 0.8509]
Epoch: 3400 	Train:[Loss: 12.9465, Acc: 0.8528] 	Val:[Loss: 12.8881, Acc: 0.8543]
Epoch: 3500 	Train:[Loss: 12.5537, Acc: 0.8573] 	Val:[Loss: 12.5989, Acc: 0.8576]
Epoch: 3600 	Train:[Loss: 12.1882, Acc: 0.8615] 	Val:[Loss: 12.3283, Acc: 0.8607]
Epoch: 3700 	Train:[Loss: 11.8461, Acc: 0.8653] 	Val:[Loss: 12.0744, Acc: 0.8635]
Epoch: 3800 	Train:[Loss: 11.5239, Acc: 0.8690] 	Val:[Loss: 11.8345, Acc: 0.8663]
Epoch: 3900 	Train:[Loss: 11.2184, Acc: 0.8725] 	Val:[Loss: 11.6059, Acc: 0.8688]
Epoch: 3999 	Train:[Loss: 10.9296, Acc: 0.8758] 	Val:[Loss: 11.3885, Acc: 0.8713]
================Test set metrics======================
/home/autrio/college-linx/SMAI/smai-m24-assignments-Autrio/models/MLP/MLP2.py:216: RuntimeWarning: divide by zero encountered in scalar divide
  r2 = 1 - (ss_residual / ss_total)

R2 Score ::  -inf
MSE ::  468.96299999999997
RMSE ::  21.655553560230224
MAE ::  20.134

======================================================
