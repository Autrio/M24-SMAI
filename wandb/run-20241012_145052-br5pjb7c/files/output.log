(506,)
(506, 13)
Data split into training (406 samples), validation (50 samples), and testing (50 samples) sets.
Number of classes: 1
Feature data normalized using z-score normalization.
Layer: [in:13] [out:8] [activation:Tanh]
Layer: [in:8] [out:16] [activation:Tanh]
Layer: [in:16] [out:16] [activation:Tanh]
Layer: [in:16] [out:8] [activation:Tanh]
Layer: [in:8] [out:1] [activation:Linear]
                                                                                                                                                                                                            

Epoch: 0 	Train:[Loss: 608.1038, Acc: -5.9463] 	Val:[Loss: 591.3138, Acc: -5.6826]
Epoch: 64 	Train:[Loss: 473.5387, Acc: -4.4098] 	Val:[Loss: 449.9631, Acc: -4.0852]
Epoch: 128 	Train:[Loss: 306.3314, Acc: -2.4990] 	Val:[Loss: 289.2712, Acc: -2.2692]
Epoch: 192 	Train:[Loss: 195.2507, Acc: -1.2298] 	Val:[Loss: 184.1324, Acc: -1.0809]
Epoch: 256 	Train:[Loss: 138.8182, Acc: -0.5850] 	Val:[Loss: 131.6259, Acc: -0.4875]
Epoch: 320 	Train:[Loss: 109.0068, Acc: -0.2443] 	Val:[Loss: 104.3245, Acc: -0.1790]
Epoch: 384 	Train:[Loss: 89.5880, Acc: -0.0224] 	Val:[Loss: 82.3374, Acc: 0.0695]
Epoch: 448 	Train:[Loss: 77.3365, Acc: 0.1175] 	Val:[Loss: 68.6759, Acc: 0.2239]
Epoch: 512 	Train:[Loss: 69.4171, Acc: 0.2079] 	Val:[Loss: 60.4780, Acc: 0.3165]
Epoch: 576 	Train:[Loss: 64.1651, Acc: 0.2679] 	Val:[Loss: 55.3005, Acc: 0.3750]
Epoch: 640 	Train:[Loss: 60.4935, Acc: 0.3098] 	Val:[Loss: 52.0538, Acc: 0.4117]
Epoch: 704 	Train:[Loss: 57.2733, Acc: 0.3466] 	Val:[Loss: 49.6804, Acc: 0.4385]
Epoch: 768 	Train:[Loss: 54.2555, Acc: 0.3811] 	Val:[Loss: 47.5760, Acc: 0.4623]
Epoch: 832 	Train:[Loss: 51.6675, Acc: 0.4106] 	Val:[Loss: 45.5094, Acc: 0.4857]
Epoch: 896 	Train:[Loss: 49.1277, Acc: 0.4398] 	Val:[Loss: 43.1608, Acc: 0.5122]
Epoch: 960 	Train:[Loss: 46.2470, Acc: 0.4728] 	Val:[Loss: 40.3456, Acc: 0.5440]
Epoch: 1024 	Train:[Loss: 43.1911, Acc: 0.5078] 	Val:[Loss: 37.4655, Acc: 0.5766]
Epoch: 1088 	Train:[Loss: 40.3479, Acc: 0.5403] 	Val:[Loss: 35.2714, Acc: 0.6014]
Epoch: 1152 	Train:[Loss: 37.7315, Acc: 0.5702] 	Val:[Loss: 33.1933, Acc: 0.6249]
Epoch: 1216 	Train:[Loss: 35.3788, Acc: 0.5970] 	Val:[Loss: 31.0276, Acc: 0.6493]
Epoch: 1280 	Train:[Loss: 33.3141, Acc: 0.6206] 	Val:[Loss: 28.9567, Acc: 0.6728]
Epoch: 1344 	Train:[Loss: 31.5175, Acc: 0.6411] 	Val:[Loss: 27.1153, Acc: 0.6936]
Epoch: 1408 	Train:[Loss: 29.9472, Acc: 0.6590] 	Val:[Loss: 25.5216, Acc: 0.7116]
Epoch: 1472 	Train:[Loss: 28.5618, Acc: 0.6748] 	Val:[Loss: 24.1463, Acc: 0.7271]
Epoch: 1536 	Train:[Loss: 27.3274, Acc: 0.6889] 	Val:[Loss: 22.9517, Acc: 0.7406]
Epoch: 1600 	Train:[Loss: 26.2169, Acc: 0.7015] 	Val:[Loss: 21.9050, Acc: 0.7524]
Epoch: 1664 	Train:[Loss: 25.2087, Acc: 0.7130] 	Val:[Loss: 20.9803, Acc: 0.7629]
Epoch: 1728 	Train:[Loss: 24.2863, Acc: 0.7235] 	Val:[Loss: 20.1593, Acc: 0.7722]
Epoch: 1792 	Train:[Loss: 23.4379, Acc: 0.7332] 	Val:[Loss: 19.4291, Acc: 0.7804]
Epoch: 1856 	Train:[Loss: 22.6545, Acc: 0.7421] 	Val:[Loss: 18.7806, Acc: 0.7878]
Epoch: 1920 	Train:[Loss: 21.9288, Acc: 0.7503] 	Val:[Loss: 18.2064, Acc: 0.7942]
Epoch: 1984 	Train:[Loss: 21.2528, Acc: 0.7580] 	Val:[Loss: 17.6990, Acc: 0.8000]
Epoch: 2048 	Train:[Loss: 20.6187, Acc: 0.7652] 	Val:[Loss: 17.2498, Acc: 0.8051]
Epoch: 2112 	Train:[Loss: 20.0190, Acc: 0.7721] 	Val:[Loss: 16.8493, Acc: 0.8096]
Epoch: 2176 	Train:[Loss: 19.4472, Acc: 0.7786] 	Val:[Loss: 16.4876, Acc: 0.8137]
Epoch: 2240 	Train:[Loss: 18.8992, Acc: 0.7848] 	Val:[Loss: 16.1545, Acc: 0.8174]
Epoch: 2304 	Train:[Loss: 18.3738, Acc: 0.7908] 	Val:[Loss: 15.8392, Acc: 0.8210]
Epoch: 2368 	Train:[Loss: 17.8719, Acc: 0.7965] 	Val:[Loss: 15.5319, Acc: 0.8245]
Epoch: 2432 	Train:[Loss: 17.3943, Acc: 0.8019] 	Val:[Loss: 15.2261, Acc: 0.8279]
Epoch: 2496 	Train:[Loss: 16.9407, Acc: 0.8071] 	Val:[Loss: 14.9180, Acc: 0.8314]
Epoch: 2560 	Train:[Loss: 16.5101, Acc: 0.8120] 	Val:[Loss: 14.6044, Acc: 0.8350]
Epoch: 2624 	Train:[Loss: 16.1009, Acc: 0.8167] 	Val:[Loss: 14.2814, Acc: 0.8386]
Epoch: 2688 	Train:[Loss: 15.7115, Acc: 0.8211] 	Val:[Loss: 13.9443, Acc: 0.8424]
Epoch: 2752 	Train:[Loss: 15.3402, Acc: 0.8253] 	Val:[Loss: 13.5875, Acc: 0.8464]
Epoch: 2816 	Train:[Loss: 14.9854, Acc: 0.8293] 	Val:[Loss: 13.2055, Acc: 0.8508]
Epoch: 2880 	Train:[Loss: 14.6450, Acc: 0.8332] 	Val:[Loss: 12.7939, Acc: 0.8554]
Epoch: 2944 	Train:[Loss: 14.3170, Acc: 0.8369] 	Val:[Loss: 12.3514, Acc: 0.8604]
Epoch: 3008 	Train:[Loss: 13.9995, Acc: 0.8406] 	Val:[Loss: 11.8839, Acc: 0.8657]
Epoch: 3072 	Train:[Loss: 13.6907, Acc: 0.8441] 	Val:[Loss: 11.4085, Acc: 0.8711]
Epoch: 3136 	Train:[Loss: 13.3901, Acc: 0.8475] 	Val:[Loss: 10.9511, Acc: 0.8762]
Epoch: 3200 	Train:[Loss: 13.0980, Acc: 0.8508] 	Val:[Loss: 10.5356, Acc: 0.8809]
Epoch: 3264 	Train:[Loss: 12.8146, Acc: 0.8541] 	Val:[Loss: 10.1712, Acc: 0.8851]
Epoch: 3328 	Train:[Loss: 12.5400, Acc: 0.8572] 	Val:[Loss: 9.8532, Acc: 0.8886]
Epoch: 3392 	Train:[Loss: 12.2740, Acc: 0.8602] 	Val:[Loss: 9.5709, Acc: 0.8918]
Epoch: 3456 	Train:[Loss: 12.0171, Acc: 0.8632] 	Val:[Loss: 9.3145, Acc: 0.8947]
Epoch: 3520 	Train:[Loss: 11.7698, Acc: 0.8660] 	Val:[Loss: 9.0774, Acc: 0.8974]
Epoch: 3584 	Train:[Loss: 11.5317, Acc: 0.8687] 	Val:[Loss: 8.8564, Acc: 0.8999]
Epoch: 3648 	Train:[Loss: 11.3019, Acc: 0.8713] 	Val:[Loss: 8.6500, Acc: 0.9022]
Epoch: 3712 	Train:[Loss: 11.0790, Acc: 0.8739] 	Val:[Loss: 8.4579, Acc: 0.9044]
Epoch: 3776 	Train:[Loss: 10.8618, Acc: 0.8763] 	Val:[Loss: 8.2796, Acc: 0.9064]
Epoch: 3840 	Train:[Loss: 10.6489, Acc: 0.8788] 	Val:[Loss: 8.1142, Acc: 0.9083]
Epoch: 3904 	Train:[Loss: 10.4394, Acc: 0.8812] 	Val:[Loss: 7.9603, Acc: 0.9100]
Epoch: 3968 	Train:[Loss: 10.2319, Acc: 0.8835] 	Val:[Loss: 7.8159, Acc: 0.9117]
32.4
[[33.31402384]]
