Data split into training (915 samples), validation (114 samples), and testing (114 samples) sets.
Number of classes: 6
Feature data normalized using z-score normalization.
Layer: [in:11] [out:16] [activation:Tanh]
Layer: [in:16] [out:32] [activation:Tanh]
Layer: [in:32] [out:64] [activation:Tanh]
Layer: [in:64] [out:32] [activation:Tanh]
Layer: [in:32] [out:16] [activation:Tanh]
Layer: [in:16] [out:6] [activation:Softmax]
                                                                                                                                                                                                            

Epoch: 0 	Train:[Loss: 2.6366, Acc: 0.3438] 	Val:[Loss: 2.1708, Acc: 0.4375]
Epoch: 128 	Train:[Loss: 0.9906, Acc: 0.5690] 	Val:[Loss: 0.9916, Acc: 0.5417]
Epoch: 256 	Train:[Loss: 0.9346, Acc: 0.6094] 	Val:[Loss: 0.9206, Acc: 0.5833]
Epoch: 384 	Train:[Loss: 0.9039, Acc: 0.6263] 	Val:[Loss: 0.9652, Acc: 0.5417]
Epoch: 512 	Train:[Loss: 0.8767, Acc: 0.6224] 	Val:[Loss: 0.9562, Acc: 0.5833]
Epoch: 640 	Train:[Loss: 0.9326, Acc: 0.6185] 	Val:[Loss: 0.9991, Acc: 0.5729]
Epoch: 768 	Train:[Loss: 0.8297, Acc: 0.6471] 	Val:[Loss: 1.1678, Acc: 0.5833]
Epoch: 896 	Train:[Loss: 0.8178, Acc: 0.6641] 	Val:[Loss: 1.0451, Acc: 0.5938]
Epoch: 1024 	Train:[Loss: 0.8249, Acc: 0.6628] 	Val:[Loss: 1.2601, Acc: 0.5625]
Epoch: 1152 	Train:[Loss: 0.7390, Acc: 0.6953] 	Val:[Loss: 1.1240, Acc: 0.6042]
Epoch: 1280 	Train:[Loss: 0.6596, Acc: 0.7396] 	Val:[Loss: 1.0454, Acc: 0.6771]
Epoch: 1408 	Train:[Loss: 0.6951, Acc: 0.7122] 	Val:[Loss: 1.2517, Acc: 0.5833]
Epoch: 1536 	Train:[Loss: 1.0085, Acc: 0.5833] 	Val:[Loss: 1.1235, Acc: 0.6146]
Epoch: 1664 	Train:[Loss: 0.5639, Acc: 0.7865] 	Val:[Loss: 1.1248, Acc: 0.6875]
Epoch: 1792 	Train:[Loss: 0.8159, Acc: 0.6719] 	Val:[Loss: 1.4155, Acc: 0.5625]
Epoch: 1920 	Train:[Loss: 0.7365, Acc: 0.7005] 	Val:[Loss: 1.1677, Acc: 0.5625]
Epoch: 2048 	Train:[Loss: 0.5473, Acc: 0.7956] 	Val:[Loss: 1.2407, Acc: 0.5625]
Epoch: 2176 	Train:[Loss: 0.6336, Acc: 0.7357] 	Val:[Loss: 1.3561, Acc: 0.5521]
Epoch: 2304 	Train:[Loss: 0.5693, Acc: 0.7643] 	Val:[Loss: 1.4368, Acc: 0.5312]
Epoch: 2432 	Train:[Loss: 0.4508, Acc: 0.8307] 	Val:[Loss: 1.2225, Acc: 0.5938]
Epoch: 2560 	Train:[Loss: 0.4986, Acc: 0.7982] 	Val:[Loss: 1.2961, Acc: 0.5729]
Epoch: 2688 	Train:[Loss: 0.4232, Acc: 0.8424] 	Val:[Loss: 1.2709, Acc: 0.5625]
Epoch: 2816 	Train:[Loss: 0.4066, Acc: 0.8477] 	Val:[Loss: 1.2255, Acc: 0.6146]
Epoch: 2944 	Train:[Loss: 0.4143, Acc: 0.8464] 	Val:[Loss: 1.2446, Acc: 0.5938]
Epoch: 3072 	Train:[Loss: 0.6148, Acc: 0.7604] 	Val:[Loss: 1.7307, Acc: 0.5625]
Epoch: 3200 	Train:[Loss: 0.3948, Acc: 0.8555] 	Val:[Loss: 1.2443, Acc: 0.6562]
Epoch: 3328 	Train:[Loss: 0.5977, Acc: 0.7500] 	Val:[Loss: 1.3640, Acc: 0.5312]
Epoch: 3456 	Train:[Loss: 0.4857, Acc: 0.8164] 	Val:[Loss: 1.4595, Acc: 0.5104]
Epoch: 3584 	Train:[Loss: 0.7518, Acc: 0.7227] 	Val:[Loss: 1.6626, Acc: 0.5729]
Epoch: 3712 	Train:[Loss: 0.3921, Acc: 0.8555] 	Val:[Loss: 1.4636, Acc: 0.6146]
Epoch: 3840 	Train:[Loss: 0.9005, Acc: 0.6680] 	Val:[Loss: 1.7318, Acc: 0.5104]
Epoch: 3968 	Train:[Loss: 0.3280, Acc: 0.8815] 	Val:[Loss: 1.4658, Acc: 0.6250]
