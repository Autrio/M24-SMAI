Data split into training (915 samples), validation (114 samples), and testing (114 samples) sets.
Number of classes: 6
Feature data normalized using z-score normalization.
Layer: [in:11] [out:32] [activation:Tanh]
Layer: [in:32] [out:128] [activation:Tanh]
Layer: [in:128] [out:128] [activation:Tanh]
Layer: [in:128] [out:32] [activation:Tanh]
Layer: [in:32] [out:6] [activation:Softmax]
                                                                                                                                                                                                            

Epoch: 0 	Train:[Loss: 6.8916, Acc: 0.1263] 	Val:[Loss: 2.4601, Acc: 0.2500]
Epoch: 128 	Train:[Loss: 0.9006, Acc: 0.6120] 	Val:[Loss: 1.0939, Acc: 0.5521]
Epoch: 256 	Train:[Loss: 0.8147, Acc: 0.6510] 	Val:[Loss: 1.0753, Acc: 0.5625]
Epoch: 384 	Train:[Loss: 0.7689, Acc: 0.6732] 	Val:[Loss: 1.1353, Acc: 0.6146]
Epoch: 512 	Train:[Loss: 0.7968, Acc: 0.6523] 	Val:[Loss: 1.0847, Acc: 0.6667]
Epoch: 640 	Train:[Loss: 0.8066, Acc: 0.6615] 	Val:[Loss: 1.1239, Acc: 0.6042]
Epoch: 768 	Train:[Loss: 0.7726, Acc: 0.6654] 	Val:[Loss: 1.1863, Acc: 0.6250]
Epoch: 896 	Train:[Loss: 0.5787, Acc: 0.7526] 	Val:[Loss: 1.1360, Acc: 0.6146]
Epoch: 1024 	Train:[Loss: 0.5479, Acc: 0.7669] 	Val:[Loss: 1.1007, Acc: 0.6458]
Epoch: 1152 	Train:[Loss: 0.6325, Acc: 0.7461] 	Val:[Loss: 1.1172, Acc: 0.6354]
Epoch: 1280 	Train:[Loss: 0.6282, Acc: 0.7383] 	Val:[Loss: 1.2407, Acc: 0.6250]
Epoch: 1408 	Train:[Loss: 0.4813, Acc: 0.7917] 	Val:[Loss: 1.2145, Acc: 0.6250]
Epoch: 1536 	Train:[Loss: 0.6147, Acc: 0.7422] 	Val:[Loss: 1.1052, Acc: 0.6667]
Epoch: 1664 	Train:[Loss: 0.4282, Acc: 0.8372] 	Val:[Loss: 1.2388, Acc: 0.6146]
Epoch: 1792 	Train:[Loss: 0.4084, Acc: 0.8320] 	Val:[Loss: 1.3320, Acc: 0.6354]
Epoch: 1920 	Train:[Loss: 0.3821, Acc: 0.8711] 	Val:[Loss: 1.3666, Acc: 0.6146]
Epoch: 2048 	Train:[Loss: 0.4178, Acc: 0.8346] 	Val:[Loss: 1.2854, Acc: 0.5938]
Epoch: 2176 	Train:[Loss: 0.4577, Acc: 0.8138] 	Val:[Loss: 1.2807, Acc: 0.6354]
Epoch: 2304 	Train:[Loss: 0.3451, Acc: 0.8698] 	Val:[Loss: 1.3022, Acc: 0.6562]
Epoch: 2432 	Train:[Loss: 0.4361, Acc: 0.8086] 	Val:[Loss: 1.4497, Acc: 0.5938]
Epoch: 2560 	Train:[Loss: 0.5015, Acc: 0.8073] 	Val:[Loss: 1.4904, Acc: 0.5625]
Epoch: 2688 	Train:[Loss: 0.3342, Acc: 0.8789] 	Val:[Loss: 1.4991, Acc: 0.6458]
Epoch: 2816 	Train:[Loss: 0.3985, Acc: 0.8398] 	Val:[Loss: 1.3360, Acc: 0.6562]
Epoch: 2944 	Train:[Loss: 0.2762, Acc: 0.9089] 	Val:[Loss: 1.4235, Acc: 0.6250]
Epoch: 3072 	Train:[Loss: 0.5304, Acc: 0.7904] 	Val:[Loss: 1.6296, Acc: 0.6042]
Epoch: 3200 	Train:[Loss: 0.2162, Acc: 0.9388] 	Val:[Loss: 1.4623, Acc: 0.6354]
Epoch: 3328 	Train:[Loss: 0.4905, Acc: 0.7982] 	Val:[Loss: 1.4828, Acc: 0.6250]
Epoch: 3456 	Train:[Loss: 0.2279, Acc: 0.9167] 	Val:[Loss: 1.4659, Acc: 0.6562]
Epoch: 3584 	Train:[Loss: 0.1879, Acc: 0.9440] 	Val:[Loss: 1.5750, Acc: 0.6146]
Epoch: 3712 	Train:[Loss: 0.1688, Acc: 0.9479] 	Val:[Loss: 1.5569, Acc: 0.6250]
Epoch: 3840 	Train:[Loss: 0.1995, Acc: 0.9349] 	Val:[Loss: 1.5935, Acc: 0.6458]
Epoch: 3968 	Train:[Loss: 0.3086, Acc: 0.8945] 	Val:[Loss: 1.7378, Acc: 0.5729]
Epoch: 4096 	Train:[Loss: 0.3009, Acc: 0.8919] 	Val:[Loss: 1.7491, Acc: 0.6250]
Epoch: 4224 	Train:[Loss: 0.3778, Acc: 0.8516] 	Val:[Loss: 1.7058, Acc: 0.6146]
Epoch: 4352 	Train:[Loss: 0.1823, Acc: 0.9518] 	Val:[Loss: 1.7718, Acc: 0.6250]
Epoch: 4480 	Train:[Loss: 0.1298, Acc: 0.9661] 	Val:[Loss: 1.9527, Acc: 0.6562]
Epoch: 4608 	Train:[Loss: 0.3552, Acc: 0.8659] 	Val:[Loss: 1.8158, Acc: 0.6667]
Epoch: 4736 	Train:[Loss: 0.1786, Acc: 0.9375] 	Val:[Loss: 1.9718, Acc: 0.5833]
Epoch: 4864 	Train:[Loss: 0.1722, Acc: 0.9414] 	Val:[Loss: 1.6293, Acc: 0.6562]
Epoch: 4992 	Train:[Loss: 0.1462, Acc: 0.9518] 	Val:[Loss: 1.7985, Acc: 0.6771]
