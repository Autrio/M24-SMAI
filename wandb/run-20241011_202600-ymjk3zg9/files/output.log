Index(['age', 'gender', 'income', 'education', 'married', 'children', 'city',
       'occupation', 'purchase_amount', 'most bought item', 'labels'],
      dtype='object')
Data split into training (900 samples), validation (50 samples), and testing (50 samples) sets.
Number of classes: 8
Feature data normalized using z-score normalization.
                                                                                                                                                                                                            
Epoch: 0 	Train:[Loss: 1.4245, Acc: 0.4869] 	Val:[Loss: 1.2263, Acc: 0.4961]
Epoch: 64 	Train:[Loss: 0.6261, Acc: 0.6628] 	Val:[Loss: 0.6648, Acc: 0.6367]
Epoch: 128 	Train:[Loss: 0.6162, Acc: 0.6691] 	Val:[Loss: 0.6556, Acc: 0.6406]
Epoch: 192 	Train:[Loss: 0.6103, Acc: 0.6712] 	Val:[Loss: 0.6618, Acc: 0.6211]
Epoch: 256 	Train:[Loss: 0.6060, Acc: 0.6769] 	Val:[Loss: 0.6687, Acc: 0.6289]
Epoch: 320 	Train:[Loss: 0.6009, Acc: 0.6783] 	Val:[Loss: 0.6782, Acc: 0.6289]
Epoch: 384 	Train:[Loss: 0.5972, Acc: 0.6798] 	Val:[Loss: 0.6847, Acc: 0.6211]
Epoch: 448 	Train:[Loss: 0.5940, Acc: 0.6833] 	Val:[Loss: 0.6885, Acc: 0.6211]
Epoch: 512 	Train:[Loss: 0.5910, Acc: 0.6842] 	Val:[Loss: 0.6975, Acc: 0.6172]
Epoch: 576 	Train:[Loss: 0.5886, Acc: 0.6892] 	Val:[Loss: 0.7062, Acc: 0.6172]
Epoch: 640 	Train:[Loss: 0.5864, Acc: 0.6862] 	Val:[Loss: 0.7142, Acc: 0.6172]
Epoch: 704 	Train:[Loss: 0.5844, Acc: 0.6879] 	Val:[Loss: 0.7190, Acc: 0.6172]
Epoch: 768 	Train:[Loss: 0.5824, Acc: 0.6907] 	Val:[Loss: 0.7209, Acc: 0.6016]
Epoch: 832 	Train:[Loss: 0.5804, Acc: 0.6915] 	Val:[Loss: 0.7265, Acc: 0.6055]
Epoch: 896 	Train:[Loss: 0.5788, Acc: 0.6913] 	Val:[Loss: 0.7317, Acc: 0.6016]
Epoch: 960 	Train:[Loss: 0.5772, Acc: 0.6934] 	Val:[Loss: 0.7335, Acc: 0.6016]
Epoch: 1024 	Train:[Loss: 0.5760, Acc: 0.6928] 	Val:[Loss: 0.7364, Acc: 0.5977]
Epoch: 1088 	Train:[Loss: 0.5749, Acc: 0.6949] 	Val:[Loss: 0.7385, Acc: 0.5977]
Epoch: 1152 	Train:[Loss: 0.5738, Acc: 0.6936] 	Val:[Loss: 0.7392, Acc: 0.6016]
Epoch: 1216 	Train:[Loss: 0.5728, Acc: 0.6953] 	Val:[Loss: 0.7390, Acc: 0.5977]
Epoch: 1280 	Train:[Loss: 0.5717, Acc: 0.6960] 	Val:[Loss: 0.7390, Acc: 0.5898]
Epoch: 1344 	Train:[Loss: 0.5705, Acc: 0.6968] 	Val:[Loss: 0.7385, Acc: 0.5938]
Epoch: 1408 	Train:[Loss: 0.5693, Acc: 0.6974] 	Val:[Loss: 0.7378, Acc: 0.5938]
Epoch: 1472 	Train:[Loss: 0.5681, Acc: 0.6988] 	Val:[Loss: 0.7380, Acc: 0.5898]
Epoch: 1536 	Train:[Loss: 0.5670, Acc: 0.6987] 	Val:[Loss: 0.7380, Acc: 0.5938]
Epoch: 1600 	Train:[Loss: 0.5661, Acc: 0.7008] 	Val:[Loss: 0.7442, Acc: 0.5977]
Epoch: 1664 	Train:[Loss: 0.5647, Acc: 0.7024] 	Val:[Loss: 0.7506, Acc: 0.6055]
Epoch: 1728 	Train:[Loss: 0.5635, Acc: 0.7030] 	Val:[Loss: 0.7554, Acc: 0.6055]
Epoch: 1792 	Train:[Loss: 0.5625, Acc: 0.7044] 	Val:[Loss: 0.7623, Acc: 0.6016]
Epoch: 1856 	Train:[Loss: 0.5616, Acc: 0.7041] 	Val:[Loss: 0.7663, Acc: 0.6055]
Epoch: 1920 	Train:[Loss: 0.5611, Acc: 0.7044] 	Val:[Loss: 0.7691, Acc: 0.5977]
Epoch: 1984 	Train:[Loss: 0.5606, Acc: 0.7049] 	Val:[Loss: 0.7717, Acc: 0.5898]
Epoch: 2048 	Train:[Loss: 0.5595, Acc: 0.7042] 	Val:[Loss: 0.7689, Acc: 0.5898]
Epoch: 2112 	Train:[Loss: 0.5589, Acc: 0.7038] 	Val:[Loss: 0.7688, Acc: 0.5898]
Epoch: 2176 	Train:[Loss: 0.5584, Acc: 0.7049] 	Val:[Loss: 0.7685, Acc: 0.5898]
Epoch: 2240 	Train:[Loss: 0.5581, Acc: 0.7040] 	Val:[Loss: 0.7686, Acc: 0.5938]
Epoch: 2304 	Train:[Loss: 0.5573, Acc: 0.7047] 	Val:[Loss: 0.7708, Acc: 0.5938]
Epoch: 2368 	Train:[Loss: 0.5576, Acc: 0.7037] 	Val:[Loss: 0.7718, Acc: 0.5898]
Epoch: 2432 	Train:[Loss: 0.5565, Acc: 0.7035] 	Val:[Loss: 0.7701, Acc: 0.5859]
Epoch: 2496 	Train:[Loss: 0.5556, Acc: 0.7035] 	Val:[Loss: 0.7759, Acc: 0.5977]
Epoch: 2560 	Train:[Loss: 0.5550, Acc: 0.7027] 	Val:[Loss: 0.7772, Acc: 0.5977]
Epoch: 2624 	Train:[Loss: 0.5559, Acc: 0.7047] 	Val:[Loss: 0.7764, Acc: 0.5977]
Epoch: 2688 	Train:[Loss: 0.5543, Acc: 0.7023] 	Val:[Loss: 0.7772, Acc: 0.5898]
Epoch: 2752 	Train:[Loss: 0.5540, Acc: 0.7024] 	Val:[Loss: 0.7768, Acc: 0.5938]
Epoch: 2816 	Train:[Loss: 0.5534, Acc: 0.7023] 	Val:[Loss: 0.7754, Acc: 0.5781]
Epoch: 2880 	Train:[Loss: 0.5527, Acc: 0.7023] 	Val:[Loss: 0.7809, Acc: 0.5820]
Epoch: 2944 	Train:[Loss: 0.5542, Acc: 0.7010] 	Val:[Loss: 0.7818, Acc: 0.5898]
Epoch: 3008 	Train:[Loss: 0.5529, Acc: 0.7020] 	Val:[Loss: 0.7907, Acc: 0.5898]
Epoch: 3072 	Train:[Loss: 0.5537, Acc: 0.7016] 	Val:[Loss: 0.7889, Acc: 0.5938]
Epoch: 3136 	Train:[Loss: 0.5526, Acc: 0.7023] 	Val:[Loss: 0.8005, Acc: 0.5898]
Epoch: 3200 	Train:[Loss: 0.5524, Acc: 0.7026] 	Val:[Loss: 0.7991, Acc: 0.5859]
Epoch: 3264 	Train:[Loss: 0.5519, Acc: 0.7040] 	Val:[Loss: 0.8091, Acc: 0.5898]
Epoch: 3328 	Train:[Loss: 0.5515, Acc: 0.7012] 	Val:[Loss: 0.8147, Acc: 0.5820]
Epoch: 3392 	Train:[Loss: 0.5498, Acc: 0.7056] 	Val:[Loss: 0.8108, Acc: 0.5742]
Epoch: 3456 	Train:[Loss: 0.5493, Acc: 0.7056] 	Val:[Loss: 0.8208, Acc: 0.5820]
Epoch: 3520 	Train:[Loss: 0.5515, Acc: 0.7047] 	Val:[Loss: 0.8107, Acc: 0.5742]
Epoch: 3584 	Train:[Loss: 0.5490, Acc: 0.7052] 	Val:[Loss: 0.8335, Acc: 0.5703]
Epoch: 3648 	Train:[Loss: 0.5490, Acc: 0.7040] 	Val:[Loss: 0.8352, Acc: 0.5742]
Epoch: 3712 	Train:[Loss: 0.5477, Acc: 0.7056] 	Val:[Loss: 0.8387, Acc: 0.5742]
Epoch: 3776 	Train:[Loss: 0.5479, Acc: 0.7048] 	Val:[Loss: 0.8301, Acc: 0.5820]
Epoch: 3840 	Train:[Loss: 0.5498, Acc: 0.7051] 	Val:[Loss: 0.8172, Acc: 0.5820]
Epoch: 3904 	Train:[Loss: 0.5483, Acc: 0.7054] 	Val:[Loss: 0.8206, Acc: 0.5859]
Epoch: 3968 	Train:[Loss: 0.5501, Acc: 0.7030] 	Val:[Loss: 0.8367, Acc: 0.5938]
Epoch: 4032 	Train:[Loss: 0.5474, Acc: 0.7052] 	Val:[Loss: 0.8283, Acc: 0.5938]
Epoch: 4096 	Train:[Loss: 0.5474, Acc: 0.7076] 	Val:[Loss: 0.8172, Acc: 0.5977]
Epoch: 4160 	Train:[Loss: 0.5464, Acc: 0.7063] 	Val:[Loss: 0.8260, Acc: 0.5938]
Epoch: 4224 	Train:[Loss: 0.5481, Acc: 0.7069] 	Val:[Loss: 0.8342, Acc: 0.6016]
Epoch: 4288 	Train:[Loss: 0.5474, Acc: 0.7088] 	Val:[Loss: 0.8024, Acc: 0.5859]
Epoch: 4352 	Train:[Loss: 0.5468, Acc: 0.7079] 	Val:[Loss: 0.8208, Acc: 0.5977]
Epoch: 4416 	Train:[Loss: 0.5466, Acc: 0.7065] 	Val:[Loss: 0.8138, Acc: 0.5977]
Epoch: 4480 	Train:[Loss: 0.5478, Acc: 0.7083] 	Val:[Loss: 0.8403, Acc: 0.5977]
Epoch: 4544 	Train:[Loss: 0.5457, Acc: 0.7066] 	Val:[Loss: 0.8282, Acc: 0.5938]
Epoch: 4608 	Train:[Loss: 0.5448, Acc: 0.7049] 	Val:[Loss: 0.8286, Acc: 0.6055]
Epoch: 4672 	Train:[Loss: 0.5488, Acc: 0.7059] 	Val:[Loss: 0.8637, Acc: 0.6094]
Epoch: 4736 	Train:[Loss: 0.5444, Acc: 0.7093] 	Val:[Loss: 0.8291, Acc: 0.5898]
Epoch: 4800 	Train:[Loss: 0.5453, Acc: 0.7062] 	Val:[Loss: 0.8244, Acc: 0.6055]
Epoch: 4864 	Train:[Loss: 0.5475, Acc: 0.7066] 	Val:[Loss: 0.8144, Acc: 0.6016]
Epoch: 4928 	Train:[Loss: 0.5491, Acc: 0.7024] 	Val:[Loss: 0.8316, Acc: 0.5938]
Epoch: 4992 	Train:[Loss: 0.5442, Acc: 0.7069] 	Val:[Loss: 0.8206, Acc: 0.6016]
['beauty', 'books', 'clothing', 'electronics', 'food', 'furniture', 'home', 'sports']
[0. 1. 1. 0. 0. 0. 0. 0.]
[[0.1366946  0.64022345 0.41133359 0.29282499 0.50757271 0.26904259
  0.50484175 0.24704535]]
[[0. 1. 0. 0. 1. 0. 1. 0.]]
