(506,)
(506, 13)
Data split into training (406 samples), validation (50 samples), and testing (50 samples) sets.
Number of classes: 1
Feature data normalized using z-score normalization.
Layer: [in:13] [out:32] [activation:Linear]
Layer: [in:32] [out:128] [activation:Linear]
Layer: [in:128] [out:128] [activation:Linear]
Layer: [in:128] [out:32] [activation:Linear]
Layer: [in:32] [out:1] [activation:Linear]
Training:   0%|▏                                                                                                                | 5/4000 [00:00<06:36, 10.08epoch/s, Train Acc=-1.8e+13, Val Acc=-3.18e+109]/home/autrio/college-linx/SMAI/smai-m24-assignments-Autrio/models/MLP/MLP2.py:389: RuntimeWarning: overflow encountered in matmul

Epoch: 0 	Train:[Loss: 675.4355, Acc: -6.4918] 	Val:[Loss: 2807.7974, Acc: -30.7319]
  z = x @ self.W.T + self.b
/home/autrio/college-linx/SMAI/smai-m24-assignments-Autrio/models/MLP/MLP2.py:389: RuntimeWarning: invalid value encountered in matmul
  z = x @ self.W.T + self.b
Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4000/4000 [04:56<00:00, 13.50epoch/s, Train Acc=nan, Val Acc=nan]
Epoch: 128 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 256 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 384 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 512 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 640 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 768 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 896 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 1024 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 1152 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 1280 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 1408 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 1536 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 1664 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 1792 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 1920 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 2048 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 2176 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 2304 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 2432 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 2560 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 2688 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 2816 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 2944 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 3072 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 3200 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 3328 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 3456 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 3584 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 3712 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 3840 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
Epoch: 3968 	Train:[Loss: nan, Acc: nan] 	Val:[Loss: nan, Acc: nan]
