Data split into training (915 samples), validation (114 samples), and testing (114 samples) sets.
Number of classes: 6
Feature data normalized using z-score normalization.
Layer: [in:11] [out:32] [activation:Tanh]
Layer: [in:32] [out:128] [activation:Tanh]
Layer: [in:128] [out:128] [activation:Tanh]
Layer: [in:128] [out:32] [activation:Tanh]
Layer: [in:32] [out:6] [activation:Softmax]
                                                                                                                                                                                                            

Epoch: 0 	Train:[Loss: 7.9013, Acc: 0.1172] 	Val:[Loss: 4.4228, Acc: 0.2917]
Epoch: 128 	Train:[Loss: 0.9849, Acc: 0.5651] 	Val:[Loss: 1.1329, Acc: 0.5625]
Epoch: 256 	Train:[Loss: 0.8562, Acc: 0.6302] 	Val:[Loss: 1.0273, Acc: 0.6250]
Epoch: 384 	Train:[Loss: 0.8072, Acc: 0.6458] 	Val:[Loss: 1.0225, Acc: 0.6250]
Epoch: 512 	Train:[Loss: 0.7217, Acc: 0.6771] 	Val:[Loss: 1.0980, Acc: 0.6458]
Epoch: 640 	Train:[Loss: 0.7807, Acc: 0.6393] 	Val:[Loss: 1.0828, Acc: 0.6354]
Epoch: 768 	Train:[Loss: 0.6561, Acc: 0.7279] 	Val:[Loss: 1.0596, Acc: 0.6250]
Epoch: 896 	Train:[Loss: 0.6847, Acc: 0.7057] 	Val:[Loss: 1.1007, Acc: 0.6250]
Epoch: 1024 	Train:[Loss: 0.6683, Acc: 0.7031] 	Val:[Loss: 1.0574, Acc: 0.6562]
Epoch: 1152 	Train:[Loss: 0.6771, Acc: 0.7109] 	Val:[Loss: 1.0429, Acc: 0.6250]
Epoch: 1280 	Train:[Loss: 0.7233, Acc: 0.7005] 	Val:[Loss: 1.0667, Acc: 0.6250]
Epoch: 1408 	Train:[Loss: 0.6720, Acc: 0.7526] 	Val:[Loss: 1.1050, Acc: 0.6146]
Epoch: 1536 	Train:[Loss: 0.6483, Acc: 0.7279] 	Val:[Loss: 1.2583, Acc: 0.6146]
Epoch: 1664 	Train:[Loss: 0.5544, Acc: 0.7812] 	Val:[Loss: 1.1970, Acc: 0.5833]
Epoch: 1792 	Train:[Loss: 0.7070, Acc: 0.7005] 	Val:[Loss: 1.0912, Acc: 0.6458]
Epoch: 1920 	Train:[Loss: 0.5429, Acc: 0.7812] 	Val:[Loss: 1.3205, Acc: 0.6250]
Epoch: 2048 	Train:[Loss: 0.5046, Acc: 0.8021] 	Val:[Loss: 1.1666, Acc: 0.6562]
Epoch: 2176 	Train:[Loss: 0.5337, Acc: 0.7826] 	Val:[Loss: 1.2088, Acc: 0.6354]
Epoch: 2304 	Train:[Loss: 0.4994, Acc: 0.8086] 	Val:[Loss: 1.2784, Acc: 0.6667]
Epoch: 2432 	Train:[Loss: 0.4693, Acc: 0.8112] 	Val:[Loss: 1.4168, Acc: 0.6146]
Epoch: 2560 	Train:[Loss: 0.3778, Acc: 0.8607] 	Val:[Loss: 1.3431, Acc: 0.6771]
Epoch: 2688 	Train:[Loss: 0.3926, Acc: 0.8516] 	Val:[Loss: 1.2802, Acc: 0.6562]
Epoch: 2816 	Train:[Loss: 0.5617, Acc: 0.7760] 	Val:[Loss: 1.3173, Acc: 0.5833]
Epoch: 2944 	Train:[Loss: 0.4746, Acc: 0.8099] 	Val:[Loss: 1.2639, Acc: 0.6458]
Epoch: 3072 	Train:[Loss: 0.4551, Acc: 0.8138] 	Val:[Loss: 1.2555, Acc: 0.6458]
Epoch: 3200 	Train:[Loss: 0.2615, Acc: 0.9089] 	Val:[Loss: 1.3608, Acc: 0.6562]
Epoch: 3328 	Train:[Loss: 0.5186, Acc: 0.8060] 	Val:[Loss: 1.6585, Acc: 0.5625]
Epoch: 3456 	Train:[Loss: 0.2359, Acc: 0.9167] 	Val:[Loss: 1.4370, Acc: 0.6458]
Epoch: 3584 	Train:[Loss: 0.2581, Acc: 0.9023] 	Val:[Loss: 1.3886, Acc: 0.6562]
Epoch: 3712 	Train:[Loss: 0.3729, Acc: 0.8568] 	Val:[Loss: 1.4357, Acc: 0.6458]
Epoch: 3840 	Train:[Loss: 0.2259, Acc: 0.9219] 	Val:[Loss: 1.3979, Acc: 0.6562]
Epoch: 3968 	Train:[Loss: 0.2073, Acc: 0.9336] 	Val:[Loss: 1.5198, Acc: 0.6562]
Epoch: 4096 	Train:[Loss: 0.1774, Acc: 0.9453] 	Val:[Loss: 1.6450, Acc: 0.6354]
Epoch: 4224 	Train:[Loss: 0.2550, Acc: 0.9115] 	Val:[Loss: 1.6755, Acc: 0.6146]
Epoch: 4352 	Train:[Loss: 0.5093, Acc: 0.7891] 	Val:[Loss: 1.6456, Acc: 0.6250]
Epoch: 4480 	Train:[Loss: 0.1640, Acc: 0.9505] 	Val:[Loss: 1.7033, Acc: 0.6146]
Epoch: 4608 	Train:[Loss: 0.2857, Acc: 0.8828] 	Val:[Loss: 1.7038, Acc: 0.5938]
Epoch: 4736 	Train:[Loss: 0.1445, Acc: 0.9609] 	Val:[Loss: 1.7293, Acc: 0.6146]
Epoch: 4864 	Train:[Loss: 0.1849, Acc: 0.9427] 	Val:[Loss: 1.8901, Acc: 0.5833]
Epoch: 4992 	Train:[Loss: 0.1369, Acc: 0.9635] 	Val:[Loss: 1.8797, Acc: 0.5833]
