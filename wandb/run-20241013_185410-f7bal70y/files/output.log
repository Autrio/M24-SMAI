(506,)
(506, 13)
Data split into training (406 samples), validation (50 samples), and testing (50 samples) sets.
Number of classes: 1
Feature data normalized using z-score normalization.
{'lr': 0.045229041437152936, 'batch_size': 64, 'epoch': 5000, 'optimizer': 'mini-batch', 'loss_fn': 'MSELoss', 'activation': 'Tanh', 'type': 'regression', 'early_stopping': True, 'activations': 'Sigmoid', 'model_architecture': 'arch5'}
Layer: [in:13] [out:16] [activation:Sigmoid]
Layer: [in:16] [out:64] [activation:Sigmoid]
Layer: [in:64] [out:128] [activation:Sigmoid]
Layer: [in:128] [out:256] [activation:Sigmoid]
Layer: [in:256] [out:1] [activation:Linear]
                                                                                                                                                                                                           

Epoch: 0 	Train:[Loss: 640.5775, Acc: -6.1052] 	Val:[Loss: 465.5610, Acc: -4.2615]
Epoch: 100 	Train:[Loss: 89.5177, Acc: 0.0071] 	Val:[Loss: 87.8035, Acc: 0.0077]
Epoch: 200 	Train:[Loss: 89.0608, Acc: 0.0122] 	Val:[Loss: 87.2039, Acc: 0.0145]
Epoch: 300 	Train:[Loss: 88.4394, Acc: 0.0190] 	Val:[Loss: 86.4019, Acc: 0.0235]
Epoch: 400 	Train:[Loss: 87.5264, Acc: 0.0292] 	Val:[Loss: 85.2438, Acc: 0.0366]
Epoch: 500 	Train:[Loss: 86.0961, Acc: 0.0450] 	Val:[Loss: 83.4590, Acc: 0.0568]
Epoch: 600 	Train:[Loss: 83.7197, Acc: 0.0714] 	Val:[Loss: 80.5368, Acc: 0.0898]
Epoch: 700 	Train:[Loss: 79.5706, Acc: 0.1174] 	Val:[Loss: 75.4995, Acc: 0.1468]
Epoch: 800 	Train:[Loss: 72.2808, Acc: 0.1983] 	Val:[Loss: 66.7300, Acc: 0.2459]
Epoch: 900 	Train:[Loss: 61.0380, Acc: 0.3230] 	Val:[Loss: 53.2251, Acc: 0.3985]
Epoch: 1000 	Train:[Loss: 48.5288, Acc: 0.4617] 	Val:[Loss: 38.1827, Acc: 0.5685]
Epoch: 1100 	Train:[Loss: 37.4522, Acc: 0.5846] 	Val:[Loss: 25.7498, Acc: 0.7090]
Epoch: 1200 	Train:[Loss: 29.0366, Acc: 0.6779] 	Val:[Loss: 17.8692, Acc: 0.7981]
Epoch: 1300 	Train:[Loss: 25.0850, Acc: 0.7218] 	Val:[Loss: 14.8188, Acc: 0.8325]
Epoch: 1400 	Train:[Loss: 23.2320, Acc: 0.7423] 	Val:[Loss: 13.6783, Acc: 0.8454]
Epoch: 1500 	Train:[Loss: 21.9738, Acc: 0.7563] 	Val:[Loss: 13.0293, Acc: 0.8528]
Epoch: 1600 	Train:[Loss: 20.9828, Acc: 0.7673] 	Val:[Loss: 12.5468, Acc: 0.8582]
Epoch: 1700 	Train:[Loss: 20.1711, Acc: 0.7763] 	Val:[Loss: 12.1533, Acc: 0.8627]
Epoch: 1800 	Train:[Loss: 19.4933, Acc: 0.7838] 	Val:[Loss: 11.8170, Acc: 0.8665]
Epoch: 1900 	Train:[Loss: 18.9180, Acc: 0.7902] 	Val:[Loss: 11.5180, Acc: 0.8698]
Epoch: 2000 	Train:[Loss: 18.4199, Acc: 0.7957] 	Val:[Loss: 11.2417, Acc: 0.8730]
Epoch: 2100 	Train:[Loss: 17.9768, Acc: 0.8006] 	Val:[Loss: 10.9777, Acc: 0.8759]
Epoch: 2200 	Train:[Loss: 17.5695, Acc: 0.8051] 	Val:[Loss: 10.7192, Acc: 0.8789]
Epoch: 2300 	Train:[Loss: 17.1831, Acc: 0.8094] 	Val:[Loss: 10.4621, Acc: 0.8818]
Epoch: 2400 	Train:[Loss: 16.8096, Acc: 0.8135] 	Val:[Loss: 10.2058, Acc: 0.8847]
Epoch: 2500 	Train:[Loss: 16.4472, Acc: 0.8176] 	Val:[Loss: 9.9517, Acc: 0.8875]
Epoch: 2600 	Train:[Loss: 16.0956, Acc: 0.8215] 	Val:[Loss: 9.7025, Acc: 0.8903]
Epoch: 2700 	Train:[Loss: 15.7535, Acc: 0.8253] 	Val:[Loss: 9.4610, Acc: 0.8931]
Epoch: 2800 	Train:[Loss: 15.4178, Acc: 0.8290] 	Val:[Loss: 9.2288, Acc: 0.8957]
Epoch: 2900 	Train:[Loss: 15.0841, Acc: 0.8327] 	Val:[Loss: 9.0061, Acc: 0.8982]
Epoch: 3000 	Train:[Loss: 14.7460, Acc: 0.8364] 	Val:[Loss: 8.7918, Acc: 0.9006]
Epoch: 3100 	Train:[Loss: 14.3960, Acc: 0.8403] 	Val:[Loss: 8.5839, Acc: 0.9030]
Epoch: 3200 	Train:[Loss: 14.0248, Acc: 0.8444] 	Val:[Loss: 8.3793, Acc: 0.9053]
Epoch: 3300 	Train:[Loss: 13.6226, Acc: 0.8489] 	Val:[Loss: 8.1742, Acc: 0.9076]
Epoch: 3400 	Train:[Loss: 13.1818, Acc: 0.8538] 	Val:[Loss: 7.9641, Acc: 0.9100]
Epoch: 3500 	Train:[Loss: 12.7021, Acc: 0.8591] 	Val:[Loss: 7.7445, Acc: 0.9125]
Epoch: 3600 	Train:[Loss: 12.1914, Acc: 0.8648] 	Val:[Loss: 7.5139, Acc: 0.9151]
Epoch: 3700 	Train:[Loss: 11.6632, Acc: 0.8706] 	Val:[Loss: 7.2765, Acc: 0.9178]
Epoch: 3800 	Train:[Loss: 11.1316, Acc: 0.8765] 	Val:[Loss: 7.0409, Acc: 0.9204]
Epoch: 3900 	Train:[Loss: 10.6099, Acc: 0.8823] 	Val:[Loss: 6.8160, Acc: 0.9230]
Epoch: 4000 	Train:[Loss: 10.1102, Acc: 0.8879] 	Val:[Loss: 6.6084, Acc: 0.9253]
Epoch: 4100 	Train:[Loss: 9.6437, Acc: 0.8930] 	Val:[Loss: 6.4228, Acc: 0.9274]
Epoch: 4200 	Train:[Loss: 9.2185, Acc: 0.8977] 	Val:[Loss: 6.2622, Acc: 0.9292]
Epoch: 4300 	Train:[Loss: 8.8388, Acc: 0.9020] 	Val:[Loss: 6.1284, Acc: 0.9307]
Epoch: 4400 	Train:[Loss: 8.5047, Acc: 0.9057] 	Val:[Loss: 6.0217, Acc: 0.9319]
Epoch: 4500 	Train:[Loss: 8.2135, Acc: 0.9089] 	Val:[Loss: 5.9412, Acc: 0.9329]
Epoch: 4600 	Train:[Loss: 7.9613, Acc: 0.9117] 	Val:[Loss: 5.8853, Acc: 0.9335]
Epoch: 4700 	Train:[Loss: 7.7430, Acc: 0.9141] 	Val:[Loss: 5.8513, Acc: 0.9339]
Epoch: 4800 	Train:[Loss: 7.5538, Acc: 0.9162] 	Val:[Loss: 5.8361, Acc: 0.9340]
Epoch: 4900 	Train:[Loss: 7.3889, Acc: 0.9180] 	Val:[Loss: 5.8365, Acc: 0.9340]
Epoch: 4999 	Train:[Loss: 7.2455, Acc: 0.9196] 	Val:[Loss: 5.8491, Acc: 0.9339]
