(1143, 6)
(1143, 11)
Data split into training (915 samples), validation (114 samples), and testing (114 samples) sets.
Number of classes: 6
Feature data normalized using z-score normalization.
Layer: [in:11] [out:16] [activation:Sigmoid]
Layer: [in:16] [out:64] [activation:Sigmoid]
Layer: [in:64] [out:128] [activation:Sigmoid]
Layer: [in:128] [out:256] [activation:Sigmoid]
Layer: [in:256] [out:6] [activation:Linear]
                                                                                                                                                                                                            

Epoch: 0 	Train:[Loss: 12.0507, Acc: 0.5885] 	Val:[Loss: 5.7565, Acc: 0.8333]
Epoch: 128 	Train:[Loss: 11.9478, Acc: 0.6541] 	Val:[Loss: 6.4761, Acc: 0.8125]
Epoch: 256 	Train:[Loss: 13.7918, Acc: 0.6007] 	Val:[Loss: 5.7565, Acc: 0.8333]
Epoch: 384 	Train:[Loss: 11.7380, Acc: 0.6602] 	Val:[Loss: 6.4761, Acc: 0.8125]
Epoch: 512 	Train:[Loss: 12.2627, Acc: 0.6450] 	Val:[Loss: 11.5131, Acc: 0.6667]
Epoch: 640 	Train:[Loss: 8.3350, Acc: 0.7587] 	Val:[Loss: 12.1128, Acc: 0.6493]
Epoch: 768 	Train:[Loss: 8.3500, Acc: 0.7582] 	Val:[Loss: 17.6296, Acc: 0.4896]
Epoch: 896 	Train:[Loss: 6.2512, Acc: 0.8190] 	Val:[Loss: 11.2733, Acc: 0.6736]
Epoch: 1024 	Train:[Loss: 6.4161, Acc: 0.8142] 	Val:[Loss: 17.3897, Acc: 0.4965]
Epoch: 1152 	Train:[Loss: 6.6260, Acc: 0.8082] 	Val:[Loss: 11.2733, Acc: 0.6736]
Epoch: 1280 	Train:[Loss: 6.4311, Acc: 0.8138] 	Val:[Loss: 7.6754, Acc: 0.7778]
Epoch: 1408 	Train:[Loss: 6.6260, Acc: 0.8082] 	Val:[Loss: 5.7565, Acc: 0.8333]
Epoch: 1536 	Train:[Loss: 6.4311, Acc: 0.8138] 	Val:[Loss: 5.7565, Acc: 0.8333]
Epoch: 1664 	Train:[Loss: 6.5061, Acc: 0.8116] 	Val:[Loss: 6.4761, Acc: 0.8125]
Epoch: 1792 	Train:[Loss: 6.4311, Acc: 0.8138] 	Val:[Loss: 5.7565, Acc: 0.8333]
Epoch: 1920 	Train:[Loss: 6.5960, Acc: 0.8090] 	Val:[Loss: 6.4761, Acc: 0.8125]
Epoch: 2048 	Train:[Loss: 6.7909, Acc: 0.8034] 	Val:[Loss: 5.7565, Acc: 0.8333]
Epoch: 2176 	Train:[Loss: 6.5960, Acc: 0.8090] 	Val:[Loss: 6.4761, Acc: 0.8125]
Epoch: 2304 	Train:[Loss: 7.1657, Acc: 0.7925] 	Val:[Loss: 5.7565, Acc: 0.8333]
Epoch: 2432 	Train:[Loss: 6.9708, Acc: 0.7982] 	Val:[Loss: 5.7565, Acc: 0.8333]
Epoch: 2560 	Train:[Loss: 6.4911, Acc: 0.8121] 	Val:[Loss: 7.6754, Acc: 0.7778]
Epoch: 2688 	Train:[Loss: 6.2512, Acc: 0.8190] 	Val:[Loss: 6.9558, Acc: 0.7986]
Epoch: 2816 	Train:[Loss: 6.4311, Acc: 0.8138] 	Val:[Loss: 7.6754, Acc: 0.7778]
Epoch: 2944 	Train:[Loss: 6.6260, Acc: 0.8082] 	Val:[Loss: 6.9558, Acc: 0.7986]
Epoch: 3072 	Train:[Loss: 7.8553, Acc: 0.7726] 	Val:[Loss: 7.6754, Acc: 0.7778]
Epoch: 3200 	Train:[Loss: 8.0502, Acc: 0.7669] 	Val:[Loss: 5.7565, Acc: 0.8333]
Epoch: 3328 	Train:[Loss: 7.8553, Acc: 0.7726] 	Val:[Loss: 5.7565, Acc: 0.8333]
Epoch: 3456 	Train:[Loss: 7.9302, Acc: 0.7704] 	Val:[Loss: 6.4761, Acc: 0.8125]
Epoch: 3584 	Train:[Loss: 7.9302, Acc: 0.7704] 	Val:[Loss: 5.7565, Acc: 0.8333]
Epoch: 3712 	Train:[Loss: 8.0951, Acc: 0.7656] 	Val:[Loss: 6.4761, Acc: 0.8125]
Epoch: 3840 	Train:[Loss: 8.2900, Acc: 0.7600] 	Val:[Loss: 5.7565, Acc: 0.8333]
Epoch: 3968 	Train:[Loss: 8.4699, Acc: 0.7548] 	Val:[Loss: 6.4761, Acc: 0.8125]
Epoch: 4096 	Train:[Loss: 8.5299, Acc: 0.7530] 	Val:[Loss: 5.7565, Acc: 0.8333]
Epoch: 4224 	Train:[Loss: 7.7803, Acc: 0.7747] 	Val:[Loss: 6.9558, Acc: 0.7986]
Epoch: 4352 	Train:[Loss: 7.8553, Acc: 0.7726] 	Val:[Loss: 7.6754, Acc: 0.7778]
Epoch: 4480 	Train:[Loss: 6.6110, Acc: 0.8086] 	Val:[Loss: 11.2733, Acc: 0.6736]
Epoch: 4608 	Train:[Loss: 6.4311, Acc: 0.8138] 	Val:[Loss: 11.9929, Acc: 0.6528]
Epoch: 4736 	Train:[Loss: 6.6260, Acc: 0.8082] 	Val:[Loss: 11.2733, Acc: 0.6736]
Epoch: 4864 	Train:[Loss: 6.4311, Acc: 0.8138] 	Val:[Loss: 10.7936, Acc: 0.6875]
Epoch: 4992 	Train:[Loss: 6.5061, Acc: 0.8116] 	Val:[Loss: 6.4761, Acc: 0.8125]
