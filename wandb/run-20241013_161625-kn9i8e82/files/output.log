(1143, 6)
(1143, 11)
Data split into training (915 samples), validation (114 samples), and testing (114 samples) sets.
Number of classes: 6
Feature data normalized using z-score normalization.
Layer: [in:11] [out:8] [activation:Linear]
Layer: [in:8] [out:16] [activation:Linear]
Layer: [in:16] [out:16] [activation:Linear]
Layer: [in:16] [out:8] [activation:Linear]
Layer: [in:8] [out:6] [activation:Linear]
                                                                                                                                                                                                            

Epoch: 0 	Train:[Loss: 9.0296, Acc: 0.7967] 	Val:[Loss: 3.8686, Acc: 0.8646]
Epoch: 100 	Train:[Loss: 1.2522, Acc: 0.8646] 	Val:[Loss: 1.8645, Acc: 0.8594]
Epoch: 200 	Train:[Loss: 1.1451, Acc: 0.8651] 	Val:[Loss: 1.5347, Acc: 0.8576]
Epoch: 300 	Train:[Loss: 1.1745, Acc: 0.8651] 	Val:[Loss: 1.5219, Acc: 0.8594]
Epoch: 400 	Train:[Loss: 1.2043, Acc: 0.8653] 	Val:[Loss: 1.5182, Acc: 0.8594]
Epoch: 500 	Train:[Loss: 1.2035, Acc: 0.8659] 	Val:[Loss: 1.5165, Acc: 0.8576]
Epoch: 600 	Train:[Loss: 1.2034, Acc: 0.8663] 	Val:[Loss: 1.5154, Acc: 0.8576]
Epoch: 700 	Train:[Loss: 1.1424, Acc: 0.8664] 	Val:[Loss: 1.5145, Acc: 0.8576]
Epoch: 800 	Train:[Loss: 1.1402, Acc: 0.8664] 	Val:[Loss: 1.5138, Acc: 0.8594]
Epoch: 900 	Train:[Loss: 1.1691, Acc: 0.8666] 	Val:[Loss: 1.5132, Acc: 0.8594]
Epoch: 1000 	Train:[Loss: 1.1685, Acc: 0.8670] 	Val:[Loss: 1.5128, Acc: 0.8594]
Epoch: 1100 	Train:[Loss: 1.1681, Acc: 0.8672] 	Val:[Loss: 1.5124, Acc: 0.8594]
Epoch: 1200 	Train:[Loss: 1.1388, Acc: 0.8672] 	Val:[Loss: 1.5120, Acc: 0.8594]
Epoch: 1300 	Train:[Loss: 1.1379, Acc: 0.8674] 	Val:[Loss: 1.5117, Acc: 0.8594]
Epoch: 1400 	Train:[Loss: 1.1374, Acc: 0.8674] 	Val:[Loss: 1.5113, Acc: 0.8594]
Epoch: 1500 	Train:[Loss: 1.1371, Acc: 0.8672] 	Val:[Loss: 1.5111, Acc: 0.8594]
Epoch: 1600 	Train:[Loss: 1.1370, Acc: 0.8672] 	Val:[Loss: 1.5108, Acc: 0.8594]
Epoch: 1700 	Train:[Loss: 1.1369, Acc: 0.8676] 	Val:[Loss: 1.5105, Acc: 0.8594]
Epoch: 1800 	Train:[Loss: 1.1368, Acc: 0.8674] 	Val:[Loss: 1.5103, Acc: 0.8594]
Epoch: 1900 	Train:[Loss: 1.1368, Acc: 0.8674] 	Val:[Loss: 1.5101, Acc: 0.8594]
Epoch: 2000 	Train:[Loss: 1.1368, Acc: 0.8674] 	Val:[Loss: 1.5099, Acc: 0.8594]
Epoch: 2100 	Train:[Loss: 1.1369, Acc: 0.8674] 	Val:[Loss: 1.5097, Acc: 0.8594]
Epoch: 2200 	Train:[Loss: 1.1370, Acc: 0.8672] 	Val:[Loss: 1.5095, Acc: 0.8594]
Epoch: 2300 	Train:[Loss: 1.1371, Acc: 0.8672] 	Val:[Loss: 1.5093, Acc: 0.8594]
Epoch: 2400 	Train:[Loss: 1.1373, Acc: 0.8672] 	Val:[Loss: 1.5092, Acc: 0.8594]
Epoch: 2500 	Train:[Loss: 1.1376, Acc: 0.8670] 	Val:[Loss: 1.5090, Acc: 0.8594]
Epoch: 2600 	Train:[Loss: 1.1380, Acc: 0.8672] 	Val:[Loss: 1.5089, Acc: 0.8594]
Epoch: 2700 	Train:[Loss: 1.1390, Acc: 0.8672] 	Val:[Loss: 1.5088, Acc: 0.8594]
Epoch: 2800 	Train:[Loss: 1.1661, Acc: 0.8672] 	Val:[Loss: 1.5086, Acc: 0.8594]
Epoch: 2900 	Train:[Loss: 1.1660, Acc: 0.8672] 	Val:[Loss: 1.5085, Acc: 0.8594]
Epoch: 3000 	Train:[Loss: 1.1659, Acc: 0.8672] 	Val:[Loss: 1.5084, Acc: 0.8594]
Epoch: 3100 	Train:[Loss: 1.1658, Acc: 0.8674] 	Val:[Loss: 1.5083, Acc: 0.8594]
Epoch: 3200 	Train:[Loss: 1.1657, Acc: 0.8676] 	Val:[Loss: 1.5082, Acc: 0.8594]
Epoch: 3300 	Train:[Loss: 1.1656, Acc: 0.8674] 	Val:[Loss: 1.5081, Acc: 0.8594]
Epoch: 3400 	Train:[Loss: 1.1655, Acc: 0.8674] 	Val:[Loss: 1.5080, Acc: 0.8594]
Epoch: 3500 	Train:[Loss: 1.1654, Acc: 0.8674] 	Val:[Loss: 1.5079, Acc: 0.8594]
Epoch: 3600 	Train:[Loss: 1.1654, Acc: 0.8676] 	Val:[Loss: 1.5078, Acc: 0.8594]
Epoch: 3700 	Train:[Loss: 1.1653, Acc: 0.8676] 	Val:[Loss: 1.5078, Acc: 0.8594]
Epoch: 3800 	Train:[Loss: 1.1652, Acc: 0.8676] 	Val:[Loss: 1.5077, Acc: 0.8594]
Epoch: 3900 	Train:[Loss: 1.1652, Acc: 0.8676] 	Val:[Loss: 1.5076, Acc: 0.8594]
Epoch: 3999 	Train:[Loss: 1.1651, Acc: 0.8674] 	Val:[Loss: 1.5075, Acc: 0.8594]
